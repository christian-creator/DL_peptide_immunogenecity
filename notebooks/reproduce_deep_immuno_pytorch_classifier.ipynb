{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "3DtMe3EbDK9N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc,precision_recall_curve,roc_curve,confusion_matrix\n",
        "import os,sys\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import seaborn as sns\n",
        "np.random.seed(10)\n",
        "random.seed(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81oG3LWzEh2k"
      },
      "source": [
        "## His functions used to read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "w2TYnX5yEbcL"
      },
      "outputs": [],
      "source": [
        "def aaindex(peptide,after_pca):\n",
        "\n",
        "    amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "    matrix = np.transpose(after_pca)   # [12,21]\n",
        "    encoded = np.empty([len(peptide), 12])  # (seq_len,12)\n",
        "    for i in range(len(peptide)):\n",
        "        query = peptide[i]\n",
        "        if query == 'X': query = '-'\n",
        "        query = query.upper()\n",
        "        encoded[i, :] = matrix[:, amino.index(query)]\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def rescue_unknown_hla(hla, dic_inventory):\n",
        "    type_ = hla[4]\n",
        "    first2 = hla[6:8]\n",
        "    last2 = hla[8:]\n",
        "    big_category = dic_inventory[type_]\n",
        "    if not big_category.get(first2) == None:\n",
        "        small_category = big_category.get(first2)\n",
        "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
        "    else:\n",
        "        small_category = list(big_category.keys())\n",
        "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])\n",
        "\n",
        "def hla_df_to_dic(hla):\n",
        "    dic = {}\n",
        "    for i in range(hla.shape[0]):\n",
        "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
        "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
        "        dic[col1] = col2\n",
        "    return dic\n",
        "\n",
        "def peptide_data_aaindex(peptide,after_pca):   # return numpy array [10,12,1]\n",
        "    length = len(peptide)\n",
        "    if length == 10:\n",
        "        encode = aaindex(peptide,after_pca)\n",
        "    elif length == 9:\n",
        "        peptide = peptide[:5] + '-' + peptide[5:]\n",
        "        encode = aaindex(peptide,after_pca)\n",
        "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
        "    return encode\n",
        "\n",
        "\n",
        "def hla_data_aaindex(hla_dic,hla_type,after_pca):    # return numpy array [34,12,1]\n",
        "    try:\n",
        "        seq = hla_dic[hla_type]\n",
        "    except KeyError:\n",
        "        hla_type = rescue_unknown_hla(hla_type,dic_inventory)\n",
        "        seq = hla_dic[hla_type]\n",
        "    encode = aaindex(seq,after_pca)\n",
        "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
        "    return encode\n",
        "\n",
        "def dict_inventory(inventory):\n",
        "    dicA, dicB, dicC = {}, {}, {}\n",
        "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
        "\n",
        "    for hla in inventory:\n",
        "        type_ = hla[4]  # A,B,C\n",
        "        first2 = hla[6:8]  # 01\n",
        "        last2 = hla[8:]  # 01\n",
        "        try:\n",
        "            dic[type_][first2].append(last2)\n",
        "        except KeyError:\n",
        "            dic[type_][first2] = []\n",
        "            dic[type_][first2].append(last2)\n",
        "\n",
        "    return dic\n",
        "\n",
        "def construct_aaindex(ori,hla_dic,after_pca):\n",
        "    series = []\n",
        "    for i in range(ori.shape[0]):\n",
        "        peptide = ori['peptide'].iloc[i]\n",
        "        hla_type = ori['HLA'].iloc[i]\n",
        "        immuno = np.array(ori['immunogenicity'].iloc[i]).reshape(1,-1)   # [1,1]\n",
        "\n",
        "        encode_pep = peptide_data_aaindex(peptide,after_pca)    # [10,12]\n",
        "\n",
        "        encode_hla = hla_data_aaindex(hla_dic,hla_type,after_pca)   # [46,12]\n",
        "        series.append((encode_pep, encode_hla, immuno))\n",
        "    return series\n",
        "\n",
        "def pull_peptide_aaindex(dataset):\n",
        "    result = np.empty([len(dataset),10,12,1])\n",
        "    for i in range(len(dataset)):\n",
        "        result[i,:,:,:] = dataset[i][0]\n",
        "    return result\n",
        "\n",
        "def pull_hla_aaindex(dataset):\n",
        "    result = np.empty([len(dataset),46,12,1])\n",
        "    for i in range(len(dataset)):\n",
        "        result[i,:,:,:] = dataset[i][1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def pull_label_aaindex(dataset):\n",
        "    col = [item[2] for item in dataset]\n",
        "    result = [0 if item == 'Negative' else 1 for item in col]\n",
        "    result = np.expand_dims(np.array(result),axis=1)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_training_and_validataion_dataset(path_to_partitions,train_splits):\n",
        "    import random\n",
        "    training_partions = random.sample(range(10),train_splits)\n",
        "    # training_partions = [9, 0, 6, 3, 4, 8, 1, 7]\n",
        "    validation_partions = [i for i in range(10) if i not in training_partions]\n",
        "\n",
        "    # path_to_partitions = \"../data/partitions\"\n",
        "    partitions = []\n",
        "    for file in os.listdir(path_to_partitions):\n",
        "        path_to_file = os.path.join(path_to_partitions,file)\n",
        "        data = pd.read_csv(path_to_file,sep=\"\\t\",names=[\"peptide\",\"label\",\"HLA\"])\n",
        "        partitions.append(data)\n",
        "    training_df = pd.concat([partitions[i] for i in training_partions])\n",
        "    validation_df = pd.concat([partitions[i] for i in validation_partions])\n",
        "    return training_df, validation_df,training_partions,validation_partions\n",
        "\n",
        "def retrieve_information_from_df(data_split,entire_df):\n",
        "    potential = []\n",
        "    immunogenicity = []\n",
        "    tested = []\n",
        "    responded = []\n",
        "    for i,row in data_split.iterrows():\n",
        "        peptide, HLA = row[\"peptide\"], row['HLA']\n",
        "        original_entry = entire_df[(entire_df['peptide']==peptide) & (entire_df['HLA'] == HLA)]\n",
        "        assert len(original_entry) == 1\n",
        "        potential.append(float(original_entry['potential']))\n",
        "        immunogenicity.append(original_entry['immunogenicity'].values[0])\n",
        "        tested.append(int(original_entry['test']))\n",
        "        responded.append(int(original_entry['respond']))\n",
        "     \n",
        "    data_split['potential'] = potential\n",
        "    data_split['immunogenicity'] = immunogenicity\n",
        "    data_split['test'] = tested\n",
        "    data_split['respond'] = responded\n",
        "\n",
        "    return data_split  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypa7oEJ5FTBd"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The strategy used by the paper to load the data (Incorrect because 100% identity between validation and validation data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "TwmfzUJpEX_C"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "after_pca = np.loadtxt('../DeepImmuno//reproduce/data/after_pca.txt')\n",
        "ori = pd.read_csv('../DeepImmuno//reproduce/data/remove0123_sample100.csv')\n",
        "ori = ori.sample(frac=1, replace=False).set_index(pd.Index(np.arange(ori.shape[0])))\n",
        "hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "dataset = construct_aaindex(ori, hla_dic, after_pca)\n",
        "input1 = pull_peptide_aaindex(dataset)\n",
        "input2 = pull_hla_aaindex(dataset)\n",
        "label = pull_label_aaindex(dataset)\n",
        "\n",
        "input1 = input1.astype('float32')\n",
        "input2 = input2.astype('float32')\n",
        "label = label.astype('float32')\n",
        "\n",
        "array = np.arange(len(dataset))\n",
        "train_index = np.random.choice(array,int(len(dataset)*0.9),replace=False)\n",
        "valid_index = [item for item in array if item not in train_index]\n",
        "\n",
        "input1 = input1.astype('float32')\n",
        "input2 = input2.astype('float32')\n",
        "label = label.astype('float32')\n",
        "\n",
        "# Reshaped data\n",
        "input1 = pull_peptide_aaindex(dataset).reshape(-1,1,10,12)\n",
        "input2 = pull_hla_aaindex(dataset).reshape(-1,1,46,12)\n",
        "label = pull_label_aaindex(dataset)\n",
        "\n",
        "\n",
        "peptide_train, peptide_val = input1[train_index], input1[valid_index]\n",
        "HLA_train, HLA_val = input2[train_index], input2[valid_index]\n",
        "label_train, label_val = label[train_index], label[valid_index]\n",
        "\n",
        "\n",
        "peptide_train = peptide_train.reshape(-1,1,10,12).astype('float32')\n",
        "HLA_train = HLA_train.reshape(-1,1,46,12).astype('float32')\n",
        "label_train = label_train.astype('float32')\n",
        "\n",
        "peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
        "HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
        "label_val = label_val.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correct partioning of the data (The data is partioned such that the validation and )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Loading the dataset\n",
        "# after_pca = np.loadtxt('../DeepImmuno/reproduce/data/after_pca.txt')\n",
        "# hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "# hla_dic = hla_df_to_dic(hla)\n",
        "# inventory = list(hla_dic.keys())\n",
        "# dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "# entire_df = pd.read_csv('../data/deep_immuno_2.csv')\n",
        "# # Allocating the partitions of the trainign and validation data\n",
        "# training_df, validation_df,training_partions,validation_partions = load_training_and_validataion_dataset(path_to_partitions=\"../data/deepimmuno_parts\",train_splits=8)\n",
        "\n",
        "# # Creating the training dataframe (With correct information such as tested and positive subjects aswell as label)\n",
        "# training_df_entire = retrieve_information_from_df(training_df,entire_df)\n",
        "# # Shuffling the dataframe\n",
        "# training_df_entire = training_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "# # Creating the validation dataframe (With correct information such as tested and positive subjects aswell as label)\n",
        "# validation_df_entire = retrieve_information_from_df(validation_df,entire_df)\n",
        "# # Shuffling the dataframe\n",
        "# validation_df_entire = validation_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "# training_dataset_encoded = construct_aaindex(training_df_entire, hla_dic, after_pca)\n",
        "# peptide_train = pull_peptide_aaindex(training_dataset_encoded)\n",
        "# HLA_train = pull_hla_aaindex(training_dataset_encoded)\n",
        "# label_train = pull_label_aaindex(training_dataset_encoded)\n",
        "\n",
        "\n",
        "# val_dataset_encoded = construct_aaindex(validation_df_entire, hla_dic, after_pca)\n",
        "# peptide_val = pull_peptide_aaindex(val_dataset_encoded)\n",
        "# HLA_val = pull_hla_aaindex(val_dataset_encoded)\n",
        "# label_val = pull_label_aaindex(val_dataset_encoded)\n",
        "\n",
        "# peptide_train = peptide_train.reshape(-1,1,10,12).astype('float32')\n",
        "# HLA_train = HLA_train.reshape(-1,1,46,12).astype('float32')\n",
        "# label_train = label_train.astype('float32')\n",
        "\n",
        "# peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
        "# HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
        "# label_val = label_val.astype('float32')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHFPYHuPKX42"
      },
      "source": [
        "## Definning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "QTz5JsT9KXVt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQ7TqSzPeg7",
        "outputId": "8a7b6150-31f4-4d09-b8fb-fa011d6ff0e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_conv_dim(dim_size,kernel_size,padding,stride):\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride + 1)\n",
        "compute_conv_dim(10,2,0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnYzGji8Ju9t",
        "outputId": "ad092cd1-f4fc-4a7a-a7f1-0d13231ec3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1_peptide): Conv2d(1, 16, kernel_size=(2, 12), stride=(1, 1))\n",
            "  (BatchNorm_conv1_peptides): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (conv2_peptide): Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1))\n",
            "  (BatchNorm_conv2_peptides): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool1_peptide): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1_HLA): Conv2d(1, 16, kernel_size=(15, 12), stride=(1, 1))\n",
            "  (BatchNorm_conv1_HLA): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool1_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2_HLA): Conv2d(16, 32, kernel_size=(9, 1), stride=(1, 1))\n",
            "  (BatchNorm_conv2_HLA): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool2_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (L_in): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (drop_out): Dropout(p=0.2, inplace=False)\n",
            "  (L_out): Linear(in_features=128, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# hyperameters of the model\n",
        "peptide_input_channels = peptide_train.shape[1]\n",
        "peptide_input_height = peptide_train.shape[2]\n",
        "peptide_input_width = peptide_train.shape[3]\n",
        "\n",
        "hla_input_channels = HLA_train.shape[1]\n",
        "hla_input_height = HLA_train.shape[2]\n",
        "hla_input_width = HLA_train.shape[3]\n",
        "\n",
        "# define network\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convelution of peptide\n",
        "        self.conv1_peptide = Conv2d(in_channels=peptide_input_channels,\n",
        "                            out_channels=16,\n",
        "                            kernel_size=(2,12),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        \n",
        "        self.BatchNorm_conv1_peptides = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.conv2_peptide = Conv2d(in_channels=16,\n",
        "                            out_channels=32,\n",
        "                            kernel_size=(2,1),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool1_peptide = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "\n",
        "        # Convelution of HLA\n",
        "        self.conv1_HLA = Conv2d(in_channels=peptide_input_channels,\n",
        "                            out_channels=16,\n",
        "                            kernel_size=(15,12),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv1_HLA = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool1_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "        \n",
        "        self.conv2_HLA = Conv2d(in_channels=16,\n",
        "                            out_channels=32,\n",
        "                            kernel_size=(9,1),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv2_HLA = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool2_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "\n",
        "        # Denselayer\n",
        "        self.L_in = Linear(in_features=256,\n",
        "                            out_features=128)\n",
        "        \n",
        "        self.drop_out = nn.Dropout(p=0.2)\n",
        "        self.L_out = Linear(in_features=128,\n",
        "                            out_features=2,\n",
        "                            bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, peptide, HLA): # x.size() = [batch, channel, height, width]\n",
        "\n",
        "        # Encoding the peptide\n",
        "        peptide = self.conv1_peptide(peptide)\n",
        "        # peptide = self.BatchNorm_conv1_peptides(peptide)\n",
        "        peptide = relu(peptide)\n",
        "        peptide = self.conv2_peptide(peptide)\n",
        "        peptide = self.BatchNorm_conv2_peptides(peptide)\n",
        "        peptide = relu(peptide)\n",
        "        peptide = self.maxpool1_peptide(peptide)\n",
        "        peptide = torch.flatten(peptide,start_dim=1)\n",
        "\n",
        "        # Encoding the HLA\n",
        "        HLA = self.conv1_HLA(HLA)\n",
        "        # HLA = self.BatchNorm_conv1_HLA(HLA)\n",
        "        HLA = relu(HLA)\n",
        "        HLA = self.maxpool1_HLA(HLA)\n",
        "        HLA = self.conv2_HLA(HLA)\n",
        "        HLA = self.BatchNorm_conv2_HLA(HLA)\n",
        "        HLA = relu(HLA)\n",
        "        HLA = self.maxpool2_HLA(HLA)\n",
        "        HLA = torch.flatten(HLA,start_dim=1)\n",
        "\n",
        "        # Combining the output\n",
        "        combined_input = torch.cat((peptide, HLA), 1)\n",
        "\n",
        "        x = self.L_in(combined_input)\n",
        "        x = self.drop_out(x)\n",
        "        x = relu(x)\n",
        "        x = self.L_out(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        return softmax(x, dim=1)\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "5_FHTyCAP25T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5000, 0.5000],\n",
              "        [0.5065, 0.4935],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.5370, 0.4630],\n",
              "        [0.5784, 0.4216],\n",
              "        [0.5823, 0.4177],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.5197, 0.4803],\n",
              "        [0.5123, 0.4877]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peptide_random = np.random.normal(0,1, (10, 1, 10, 12)).astype('float32')\n",
        "peptide_random = Variable(torch.from_numpy(peptide_random))\n",
        "HLA_random = np.random.normal(0,1, (10, 1, 46, 12)).astype('float32')\n",
        "HLA_random = Variable(torch.from_numpy(HLA_random))\n",
        "output = net(peptide_random,HLA_random)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8IcnuJMs34"
      },
      "source": [
        "## Creating testing and validataion datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjBK0OsZM0E2"
      },
      "source": [
        "## Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "zTgk8hKTMzRs"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "peptide_train_loader = list(DataLoader(peptide_train,batch_size=batch_size))\n",
        "HLA_train_loader = list(DataLoader(HLA_train,batch_size=batch_size))\n",
        "label_train_loader = list(DataLoader(label_train,batch_size=batch_size))\n",
        "\n",
        "peptide_val_loader = list(DataLoader(peptide_val,batch_size=batch_size))\n",
        "HLA_val_loader = list(DataLoader(HLA_val,batch_size=batch_size))\n",
        "label_val_loader = list(DataLoader(label_val,batch_size=batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 : Train Loss 0.637018 , Train acc 0.703704, Valid acc 0.719376\n",
            "Epoch 11 : Train Loss 0.520846 , Train acc 0.786944, Valid acc 0.798441\n",
            "Epoch 21 : Train Loss 0.508845 , Train acc 0.801561, Valid acc 0.791759\n",
            "Epoch 31 : Train Loss 0.498391 , Train acc 0.812585, Valid acc 0.796214\n",
            "Epoch 41 : Train Loss 0.490469 , Train acc 0.827450, Valid acc 0.798441\n",
            "Epoch 51 : Train Loss 0.481270 , Train acc 0.836864, Valid acc 0.800668\n",
            "Epoch 61 : Train Loss 0.472946 , Train acc 0.848384, Valid acc 0.797327\n",
            "Epoch 71 : Train Loss 0.465818 , Train acc 0.858169, Valid acc 0.795100\n",
            "Epoch 81 : Train Loss 0.459890 , Train acc 0.864858, Valid acc 0.797327\n",
            "Epoch 91 : Train Loss 0.453656 , Train acc 0.872786, Valid acc 0.797327\n"
          ]
        }
      ],
      "source": [
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "losses = []\n",
        "all_val_targets_pr_epoch = []\n",
        "all_val_predictions_pr_epoch = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    net.train()\n",
        "    current_loss = 0\n",
        "    for train_batch_index in range(len((peptide_train_loader))):\n",
        "        train_peptides = peptide_train_loader[train_batch_index]\n",
        "        train_HLA = HLA_train_loader[train_batch_index]\n",
        "        train_labels = label_train_loader[train_batch_index].long().reshape(-1)\n",
        "    \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(train_peptides,train_HLA)\n",
        "        loss = criterion(outputs, train_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "\n",
        "    # print(epoch, current_loss/batch_size)\n",
        "    losses.append(current_loss/len((peptide_train_loader)))\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        all_train_targets = []\n",
        "        all_predicted_train_labels = []\n",
        "        for i in range(len((peptide_train_loader))):\n",
        "            train_peptides = peptide_train_loader[i]\n",
        "            train_HLA = HLA_train_loader[i]\n",
        "            train_labels = label_train_loader[i].long().reshape(-1)\n",
        "            outputs = net(train_peptides,train_HLA)\n",
        "            _,predicted_labels =  torch.max(outputs, 1)\n",
        "\n",
        "            all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
        "            all_train_targets += train_labels.numpy().tolist()\n",
        "        \n",
        "        all_val_targets = []\n",
        "        all_predicted_val_labels = []\n",
        "        for j in range(len((peptide_val_loader))):\n",
        "            val_peptides = peptide_val_loader[j]\n",
        "            val_HLA = HLA_val_loader[j]\n",
        "            val_labels = label_val_loader[j].long().reshape(-1)\n",
        "            outputs = net(val_peptides,val_HLA)\n",
        "            _,predicted_labels =  torch.max(outputs, 1)\n",
        "\n",
        "            all_predicted_val_labels += predicted_labels.numpy().tolist()\n",
        "            all_val_targets += val_labels.numpy().tolist()\n",
        "\n",
        "    # Calculating the accuracies\n",
        "    train_accuracies.append(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
        "    val_accuracies.append(accuracy_score(all_val_targets,all_predicted_val_labels))\n",
        "    # Saving the predicitons for further validation\n",
        "    all_val_targets_pr_epoch.append(all_val_targets)\n",
        "    all_val_predictions_pr_epoch.append(all_predicted_val_labels)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (epoch+1, losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
        "    \n",
        "    # if epoch % 10 == 0:\n",
        "    #     print(\"Epoch %2i : Train Loss %f\"  % (epoch+1, losses[-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "s_chlCRnXKGh",
        "outputId": "142a89f2-ba3f-4830-eb75-d782c84fb621"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Text(0.5, 0, 'epochs'), Text(0, 0.5, 'Acc'))"
            ]
          },
          "execution_count": 232,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "epoch = np.arange(len(train_accuracies))\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_accuracies, 'r', epoch, val_accuracies, 'b')\n",
        "plt.legend(['Train Accucary','Validation Accuracy'])\n",
        "plt.xlabel('epochs'), plt.ylabel('Acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "G4kBdrpadEVM"
      },
      "outputs": [],
      "source": [
        "val_predictions = np.array(all_val_predictions_pr_epoch)\n",
        "val_labels = np.array(all_val_targets_pr_epoch)\n",
        "# val_predictions = val_predictions.reshape(val_predictions.shape[0],val_predictions.shape[1])\n",
        "# val_labels = val_labels.reshape(val_labels.shape[0],val_labels.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "EPOCH: 13 Recall: 0.8571428571428571 accuracy: 0.8028953229398663 f1-score: 0.8052805280528051\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score,recall_score\n",
        "best_epoch_model = np.argmax(val_accuracies)\n",
        "\n",
        "print(len(val_predictions))\n",
        "\n",
        "# for epoch in range(len(val_predictions)):\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "accuracy = accuracy_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "recall = recall_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "f1 = f1_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "\n",
        "print(f\"EPOCH: {best_epoch_model} Recall: {recall} accuracy: {accuracy} f1-score: {f1}\")\n",
        "plt.title(\"Confusion matrix of best epoch for validation performance\")\n",
        "conf_mat = confusion_matrix(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking at the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "conv1_peptide.weight \t torch.Size([16, 1, 2, 12])\n",
            "conv1_peptide.bias \t torch.Size([16])\n",
            "BatchNorm_conv1_peptides.weight \t torch.Size([16])\n",
            "BatchNorm_conv1_peptides.bias \t torch.Size([16])\n",
            "conv2_peptide.weight \t torch.Size([32, 16, 2, 1])\n",
            "conv2_peptide.bias \t torch.Size([32])\n",
            "BatchNorm_conv2_peptides.weight \t torch.Size([32])\n",
            "BatchNorm_conv2_peptides.bias \t torch.Size([32])\n",
            "conv1_HLA.weight \t torch.Size([16, 1, 15, 12])\n",
            "conv1_HLA.bias \t torch.Size([16])\n",
            "BatchNorm_conv1_HLA.weight \t torch.Size([16])\n",
            "BatchNorm_conv1_HLA.bias \t torch.Size([16])\n",
            "conv2_HLA.weight \t torch.Size([32, 16, 9, 1])\n",
            "conv2_HLA.bias \t torch.Size([32])\n",
            "BatchNorm_conv2_HLA.weight \t torch.Size([32])\n",
            "BatchNorm_conv2_HLA.bias \t torch.Size([32])\n",
            "L_in.weight \t torch.Size([128, 256])\n",
            "L_in.bias \t torch.Size([128])\n",
            "L_out.weight \t torch.Size([2, 128])\n",
            "Optimizer's state_dict:\n",
            "state \t {0: {'step': 8100, 'exp_avg': tensor([[[[ 5.6700e-03,  5.0649e-04,  7.7974e-03,  3.6598e-03, -8.3838e-03,\n",
            "            1.7291e-03,  3.6936e-03,  1.7423e-03,  3.3842e-03, -4.0513e-03,\n",
            "           -6.7993e-04, -1.0154e-03],\n",
            "          [ 7.0568e-03,  1.4131e-02,  1.4569e-02,  9.0848e-03,  1.3637e-02,\n",
            "            4.8732e-03,  6.3659e-03, -2.2105e-03,  7.7109e-03,  9.7800e-04,\n",
            "           -1.6575e-03, -8.2526e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.0993e-03, -4.5499e-03, -8.2868e-03, -1.5886e-03, -1.2221e-03,\n",
            "           -4.1631e-03, -4.4695e-03, -5.9575e-03, -1.3204e-03,  4.4956e-03,\n",
            "            8.8846e-04,  6.8495e-04],\n",
            "          [-3.6018e-03, -6.8444e-03, -3.3728e-03,  6.2965e-03, -9.5109e-04,\n",
            "            5.4114e-03, -7.2191e-03, -1.3585e-04,  5.5943e-04,  2.2730e-03,\n",
            "           -1.8159e-03,  1.7598e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.4635e-02,  1.1220e-02,  1.7639e-02,  5.3859e-03, -2.0483e-03,\n",
            "           -7.8737e-03, -4.4715e-03, -1.9005e-03, -4.8411e-03, -5.6689e-03,\n",
            "           -4.0187e-03, -4.9321e-04],\n",
            "          [-1.9915e-03, -8.0510e-03, -8.6541e-03,  1.5409e-03,  7.6630e-03,\n",
            "           -8.0629e-03, -9.3792e-03, -1.4513e-02, -5.7516e-03,  6.3275e-04,\n",
            "           -2.2114e-03, -7.2421e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 7.5427e-03, -5.3018e-04,  4.3509e-03, -4.3542e-03, -1.2437e-02,\n",
            "           -3.9059e-03,  4.5071e-03,  3.8336e-03, -3.9033e-03,  2.1528e-03,\n",
            "           -1.9809e-03,  2.4215e-03],\n",
            "          [ 9.1205e-04, -1.5339e-03,  5.2506e-03, -1.5441e-03, -1.2578e-02,\n",
            "           -1.4360e-03,  2.1618e-03,  1.6135e-03, -3.8089e-03,  1.5879e-03,\n",
            "           -1.4446e-03,  3.6653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9767e-04,  3.1535e-03, -3.8982e-03, -2.0796e-03, -4.4941e-04,\n",
            "           -1.3850e-03, -1.6169e-03, -4.1860e-03, -3.3674e-03, -1.3929e-03,\n",
            "           -7.2836e-04, -2.2754e-04],\n",
            "          [-1.6383e-03,  4.3029e-03,  1.0739e-04, -8.0277e-03, -1.1342e-02,\n",
            "            6.7560e-03,  5.7122e-04, -2.8428e-03, -9.0418e-04, -1.2492e-03,\n",
            "           -1.4310e-04,  2.3120e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.2623e-03, -7.6146e-03, -1.9782e-02, -2.9472e-03, -9.0275e-03,\n",
            "            8.9616e-04, -1.4986e-03, -7.5766e-04,  4.4274e-03,  1.2390e-03,\n",
            "           -3.4954e-03,  4.1208e-03],\n",
            "          [ 1.1515e-02,  7.3631e-03, -4.8308e-03, -7.3915e-03, -6.6356e-03,\n",
            "           -4.8591e-03,  5.3562e-04,  4.4407e-04,  2.9547e-03,  2.2109e-03,\n",
            "            1.9067e-03,  3.5494e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0598e-02,  5.2645e-05,  6.1746e-03,  2.8950e-03,  5.5839e-03,\n",
            "            1.7436e-03,  3.3868e-03,  1.4065e-03,  1.0423e-03, -3.1311e-03,\n",
            "           -1.5507e-03, -2.7672e-04],\n",
            "          [-2.2298e-02,  1.2158e-02,  7.4348e-03, -1.2728e-03, -1.3479e-03,\n",
            "           -6.2706e-03, -3.2092e-03,  5.7542e-03,  2.6944e-04, -2.4261e-03,\n",
            "            2.8220e-03, -2.5253e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9644e-02,  1.9311e-03, -5.8978e-03, -5.3448e-03,  4.8512e-03,\n",
            "           -1.0758e-02,  8.2626e-03,  4.6386e-03, -8.5931e-04, -2.6998e-03,\n",
            "           -2.7009e-04, -5.2980e-03],\n",
            "          [ 1.9248e-03, -1.5192e-03,  4.0254e-03,  3.7301e-03, -5.6302e-04,\n",
            "            3.9704e-03,  3.0203e-03,  1.6670e-03, -1.6363e-03, -3.5966e-04,\n",
            "            9.7809e-05, -1.2646e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6745e-02,  6.9107e-03,  6.1572e-03, -4.3624e-03, -8.3589e-03,\n",
            "           -2.5512e-03, -1.4547e-03, -3.7465e-03, -5.0979e-03,  1.6027e-04,\n",
            "           -1.2283e-03,  1.9270e-03],\n",
            "          [ 1.7920e-02, -5.9658e-04, -1.7653e-03,  3.5928e-03, -1.0989e-02,\n",
            "            3.0777e-03,  7.3871e-04, -3.1536e-03,  1.5582e-06, -4.2909e-03,\n",
            "           -4.1817e-03,  3.8773e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6815e-03,  1.9248e-03,  2.6267e-03, -8.4905e-04,  3.0615e-03,\n",
            "           -5.9149e-04,  3.3705e-03,  1.5123e-05,  1.8638e-03,  3.9543e-03,\n",
            "            1.0835e-03,  2.9722e-04],\n",
            "          [ 1.1505e-02, -8.0667e-03, -9.8305e-03,  2.8215e-03,  3.7158e-04,\n",
            "            4.5624e-03,  2.8503e-03,  2.2312e-03,  1.9767e-03, -2.2664e-04,\n",
            "            2.3834e-03, -2.5413e-04]]],\n",
            "\n",
            "\n",
            "        [[[-7.0811e-03, -4.8317e-03, -8.3252e-03, -3.7646e-04, -3.2065e-03,\n",
            "            1.2698e-04,  5.1009e-03,  2.1795e-03, -2.4441e-03, -3.3665e-03,\n",
            "            2.3069e-03, -1.1218e-03],\n",
            "          [-7.9655e-03, -4.6318e-03,  5.0661e-03,  1.3728e-03,  3.3230e-03,\n",
            "           -6.4417e-03, -9.3550e-04,  2.6415e-03,  6.4577e-03, -1.6910e-03,\n",
            "            1.2384e-03, -1.0540e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.7196e-04, -7.7413e-04,  4.7906e-04,  9.6966e-03,  2.5541e-03,\n",
            "            1.1131e-03,  6.7520e-03,  2.8237e-03,  5.2309e-03,  4.5382e-03,\n",
            "           -3.2797e-03, -2.8778e-04],\n",
            "          [ 1.6833e-03, -1.0136e-03,  8.9653e-03, -1.4815e-03,  8.8063e-03,\n",
            "            4.7778e-03,  7.3423e-03,  1.9926e-03,  2.4701e-03, -2.0357e-04,\n",
            "            2.6407e-03,  3.6458e-04]]],\n",
            "\n",
            "\n",
            "        [[[-2.5660e-02, -8.9299e-03,  9.9845e-03,  8.1224e-03,  5.3008e-03,\n",
            "            9.2241e-03,  3.0605e-03, -3.2993e-03,  6.4222e-03, -1.5041e-03,\n",
            "           -2.1166e-03, -1.6552e-03],\n",
            "          [-1.3368e-02, -7.5288e-04,  1.2942e-02,  1.5170e-04,  1.6981e-03,\n",
            "           -5.3542e-03, -6.5004e-03,  7.5089e-04,  2.5582e-03, -3.3620e-03,\n",
            "            2.2966e-03, -1.6035e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.3567e-03, -4.4457e-03, -6.4737e-03,  5.3499e-03, -2.7570e-03,\n",
            "            3.0498e-03, -5.8234e-04, -3.9693e-03,  5.8559e-03, -1.2565e-03,\n",
            "           -2.8012e-03, -1.4720e-03],\n",
            "          [-3.5763e-03, -1.6793e-02,  3.0492e-03,  4.9246e-03,  5.5235e-03,\n",
            "           -7.0515e-05, -2.2797e-05,  1.2610e-03,  3.9189e-03, -6.8561e-04,\n",
            "            1.7581e-03,  1.0545e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4699e-03,  4.5877e-03,  1.5926e-03, -2.9711e-03,  8.1348e-03,\n",
            "            5.6410e-03, -2.2001e-03,  3.8192e-04,  3.0315e-04,  5.2393e-03,\n",
            "           -7.4108e-04, -2.4657e-03],\n",
            "          [-4.7224e-03, -7.4676e-03,  7.4496e-03, -1.0535e-03,  8.5711e-04,\n",
            "            1.7713e-03,  4.8223e-03,  5.7420e-03, -2.2025e-03,  1.3252e-03,\n",
            "           -4.9100e-03, -5.0313e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2961e-02, -7.0081e-03,  3.7739e-03, -6.2767e-03,  8.3229e-03,\n",
            "           -6.8078e-04,  6.2257e-03, -3.0148e-03, -3.1807e-03, -1.1557e-03,\n",
            "            6.2210e-03, -3.1839e-03],\n",
            "          [ 1.3015e-02,  5.7909e-03,  8.5093e-03,  2.0328e-03, -3.2379e-03,\n",
            "            1.3868e-03,  3.3990e-03,  4.3601e-03, -3.3072e-03,  2.7943e-03,\n",
            "           -2.0754e-03,  1.8001e-04]]]]), 'exp_avg_sq': tensor([[[[1.7662e-03, 1.8829e-03, 1.7575e-03, 1.1111e-03, 1.1915e-03,\n",
            "           4.6588e-04, 2.8793e-04, 2.7187e-04, 3.3310e-04, 1.6748e-04,\n",
            "           3.1954e-04, 1.9346e-04],\n",
            "          [7.7058e-03, 4.0487e-03, 3.3319e-03, 1.2585e-03, 9.6283e-04,\n",
            "           3.8341e-04, 5.2403e-04, 2.3842e-04, 4.2774e-04, 2.0664e-04,\n",
            "           2.0781e-04, 1.6177e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.7580e-04, 2.5826e-03, 5.6737e-04, 3.8822e-04, 8.9536e-04,\n",
            "           4.1910e-04, 1.4711e-04, 1.9735e-04, 1.9388e-04, 1.2352e-04,\n",
            "           1.4480e-04, 7.9912e-05],\n",
            "          [2.0189e-03, 1.2536e-03, 8.0669e-04, 5.7857e-04, 5.8477e-04,\n",
            "           3.7352e-04, 2.4594e-04, 2.1295e-04, 2.5354e-04, 1.0909e-04,\n",
            "           1.1749e-04, 1.2257e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.4295e-03, 1.1245e-03, 2.4873e-03, 1.2415e-03, 1.0340e-03,\n",
            "           5.7367e-04, 3.4251e-04, 1.8043e-04, 2.6465e-04, 2.6801e-04,\n",
            "           1.3588e-04, 3.7539e-05],\n",
            "          [1.3579e-03, 2.0439e-03, 1.1453e-03, 7.7191e-04, 7.2495e-04,\n",
            "           4.1735e-04, 4.3754e-04, 4.3514e-04, 3.1721e-04, 2.6679e-04,\n",
            "           2.1388e-04, 1.2584e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.3320e-04, 1.5354e-03, 3.1565e-04, 2.1454e-04, 9.0909e-04,\n",
            "           1.6863e-04, 1.7961e-04, 1.3052e-04, 1.7722e-04, 5.9035e-05,\n",
            "           1.1672e-04, 8.1660e-05],\n",
            "          [9.1482e-04, 1.3695e-03, 1.5879e-03, 8.6692e-04, 7.1586e-04,\n",
            "           5.9915e-04, 2.1652e-04, 1.8199e-04, 1.7253e-04, 1.5110e-04,\n",
            "           1.2664e-04, 7.8488e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.5725e-04, 1.0654e-03, 8.8581e-04, 5.7030e-04, 2.5608e-04,\n",
            "           5.6756e-04, 2.8798e-04, 2.0595e-04, 1.5124e-04, 7.7954e-05,\n",
            "           1.0985e-04, 9.2142e-05],\n",
            "          [7.1436e-04, 6.8952e-04, 7.8136e-04, 3.5555e-04, 5.0694e-04,\n",
            "           4.7289e-04, 1.2440e-04, 1.1899e-04, 1.1752e-04, 1.2957e-04,\n",
            "           8.4780e-05, 5.1202e-05]]],\n",
            "\n",
            "\n",
            "        [[[1.4397e-03, 1.1887e-03, 1.7461e-03, 7.0481e-04, 1.3558e-03,\n",
            "           6.6976e-04, 2.2686e-04, 1.5737e-04, 3.5188e-04, 2.5771e-04,\n",
            "           2.0399e-04, 1.6369e-04],\n",
            "          [5.1237e-03, 1.8665e-03, 1.4479e-03, 8.1742e-04, 9.8374e-04,\n",
            "           8.5132e-04, 1.9978e-04, 1.5103e-04, 3.7960e-04, 1.1519e-04,\n",
            "           2.1467e-04, 1.1991e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.0148e-03, 6.4516e-04, 1.2339e-03, 2.8756e-04, 3.8764e-04,\n",
            "           2.2032e-04, 2.8579e-04, 1.4410e-04, 1.4610e-04, 1.4418e-04,\n",
            "           9.7352e-05, 3.8268e-05],\n",
            "          [3.3383e-03, 8.5697e-04, 1.1368e-03, 3.7490e-04, 4.5007e-04,\n",
            "           8.1400e-04, 2.5019e-04, 2.0239e-04, 1.8789e-04, 1.3745e-04,\n",
            "           1.0486e-04, 7.5352e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.1510e-03, 6.9424e-04, 5.5257e-04, 3.1040e-04, 4.7933e-04,\n",
            "           5.2067e-04, 1.9239e-04, 2.1330e-04, 1.9552e-04, 9.7083e-05,\n",
            "           1.3561e-04, 8.7327e-05],\n",
            "          [5.3461e-04, 7.1893e-04, 7.3333e-04, 4.0068e-04, 6.2138e-04,\n",
            "           6.1360e-04, 1.2986e-04, 1.3257e-04, 1.7963e-04, 1.0988e-04,\n",
            "           1.1142e-04, 5.3547e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.4885e-03, 5.1383e-04, 1.1969e-03, 7.6444e-04, 4.7982e-04,\n",
            "           5.6038e-04, 1.4126e-04, 2.0031e-04, 2.1846e-04, 1.1624e-04,\n",
            "           1.0056e-04, 6.6251e-05],\n",
            "          [2.7942e-03, 5.4639e-04, 8.8886e-04, 4.9912e-04, 4.2349e-04,\n",
            "           4.9237e-04, 2.0297e-04, 1.5065e-04, 1.7175e-04, 1.7343e-04,\n",
            "           1.1844e-04, 5.6610e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.3128e-03, 7.1795e-04, 1.1237e-03, 7.9352e-04, 6.5409e-04,\n",
            "           2.4286e-04, 1.1549e-04, 1.6759e-04, 2.0077e-04, 1.4060e-04,\n",
            "           9.2041e-05, 7.2785e-05],\n",
            "          [4.6188e-03, 4.5111e-04, 1.5910e-03, 5.8389e-04, 3.6291e-04,\n",
            "           8.4252e-04, 2.0008e-04, 2.0227e-04, 1.7820e-04, 1.4783e-04,\n",
            "           7.8520e-05, 6.9836e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.9415e-04, 1.7818e-03, 1.0763e-03, 7.9478e-04, 7.9634e-04,\n",
            "           2.3576e-04, 2.9206e-04, 2.5846e-04, 2.6717e-04, 1.7957e-04,\n",
            "           1.5413e-04, 1.1490e-04],\n",
            "          [1.4992e-03, 1.0837e-03, 4.2234e-04, 2.8936e-04, 5.7381e-04,\n",
            "           8.8115e-04, 2.2843e-04, 2.0190e-04, 2.5875e-04, 8.5915e-05,\n",
            "           1.4449e-04, 1.1182e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.8902e-04, 1.4849e-03, 5.9997e-04, 3.8822e-04, 7.5176e-04,\n",
            "           4.6988e-04, 2.3701e-04, 1.9255e-04, 2.7416e-04, 1.5199e-04,\n",
            "           2.1301e-04, 1.2113e-04],\n",
            "          [7.6205e-04, 7.3458e-04, 1.0899e-03, 8.3158e-04, 8.9348e-04,\n",
            "           6.8092e-04, 2.0336e-04, 2.5698e-04, 2.2460e-04, 1.9862e-04,\n",
            "           1.3429e-04, 6.5751e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.0185e-03, 5.5566e-04, 1.4508e-03, 3.1798e-04, 6.2596e-04,\n",
            "           6.2336e-04, 1.0101e-04, 9.9429e-05, 2.9543e-04, 1.4401e-04,\n",
            "           1.2903e-04, 8.2626e-05],\n",
            "          [1.8133e-03, 7.9248e-04, 1.2565e-03, 7.3512e-04, 5.0582e-04,\n",
            "           1.8939e-04, 2.1007e-04, 1.3670e-04, 1.2897e-04, 1.9695e-04,\n",
            "           1.3945e-04, 5.2872e-05]]],\n",
            "\n",
            "\n",
            "        [[[7.6054e-04, 1.9280e-03, 9.5544e-04, 5.9761e-04, 7.1133e-04,\n",
            "           2.3218e-04, 2.1200e-04, 2.1068e-04, 2.3131e-04, 8.9632e-05,\n",
            "           1.9762e-04, 9.3348e-05],\n",
            "          [2.8855e-03, 1.5578e-03, 8.9801e-04, 4.8123e-04, 5.8882e-04,\n",
            "           1.0336e-04, 2.1992e-04, 1.7277e-04, 1.2601e-04, 1.0235e-04,\n",
            "           1.1521e-04, 5.2492e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.1055e-03, 8.0407e-04, 9.3349e-04, 4.1118e-04, 8.6398e-04,\n",
            "           2.1440e-04, 1.8133e-04, 1.5133e-04, 1.8404e-04, 1.1902e-04,\n",
            "           1.5271e-04, 7.7924e-05],\n",
            "          [1.7293e-03, 1.1885e-03, 1.0691e-03, 8.6495e-04, 5.6286e-04,\n",
            "           1.3500e-04, 2.4271e-04, 2.3475e-04, 2.0531e-04, 1.0478e-04,\n",
            "           1.4549e-04, 9.1983e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.2495e-03, 1.5480e-03, 8.0143e-04, 5.1253e-04, 7.3898e-04,\n",
            "           1.6937e-04, 1.3540e-04, 1.4560e-04, 1.2531e-04, 1.0585e-04,\n",
            "           1.2368e-04, 4.1274e-05],\n",
            "          [2.5772e-03, 7.9937e-04, 1.2810e-03, 7.9058e-04, 6.7082e-04,\n",
            "           1.8499e-04, 2.0705e-04, 2.4414e-04, 2.5553e-04, 1.4456e-04,\n",
            "           1.5036e-04, 8.6861e-05]]]])}, 1: {'step': 8100, 'exp_avg': tensor([ 7.3661e-05,  6.7321e-04, -9.0732e-04, -6.4390e-04, -3.5986e-04,\n",
            "         1.4049e-04, -3.1418e-04,  1.5226e-04, -1.1359e-04,  6.6075e-04,\n",
            "        -1.6878e-04, -1.1054e-04,  5.6808e-04,  5.9078e-05,  4.1521e-05,\n",
            "         1.1493e-06]), 'exp_avg_sq': tensor([9.5003e-06, 8.9789e-06, 8.4271e-06, 5.9715e-06, 4.3786e-06, 9.9352e-06,\n",
            "        4.6439e-06, 4.2301e-06, 5.4228e-06, 7.3018e-06, 4.2551e-06, 6.0582e-06,\n",
            "        5.0303e-06, 4.1750e-06, 6.0763e-06, 5.9480e-06])}, 4: {'step': 8100, 'exp_avg': tensor([[[[ 1.5056e-03],\n",
            "          [ 4.2566e-03]],\n",
            "\n",
            "         [[ 1.7075e-03],\n",
            "          [ 4.5824e-03]],\n",
            "\n",
            "         [[ 2.2916e-03],\n",
            "          [-2.3931e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9490e-03],\n",
            "          [-7.4913e-04]],\n",
            "\n",
            "         [[-4.1290e-03],\n",
            "          [ 5.6194e-04]],\n",
            "\n",
            "         [[ 1.4070e-03],\n",
            "          [-6.3318e-04]]],\n",
            "\n",
            "\n",
            "        [[[-4.2355e-03],\n",
            "          [-1.1688e-03]],\n",
            "\n",
            "         [[-5.3576e-04],\n",
            "          [-1.6070e-03]],\n",
            "\n",
            "         [[ 9.2387e-04],\n",
            "          [ 3.1559e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4351e-03],\n",
            "          [-1.0103e-03]],\n",
            "\n",
            "         [[ 2.0871e-03],\n",
            "          [ 5.1843e-03]],\n",
            "\n",
            "         [[-2.6960e-04],\n",
            "          [-1.3920e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.0123e-04],\n",
            "          [-2.7421e-03]],\n",
            "\n",
            "         [[-1.6193e-03],\n",
            "          [-1.2192e-03]],\n",
            "\n",
            "         [[ 1.6981e-03],\n",
            "          [ 3.9446e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2186e-04],\n",
            "          [ 1.8807e-04]],\n",
            "\n",
            "         [[ 8.4420e-04],\n",
            "          [-2.6199e-04]],\n",
            "\n",
            "         [[ 4.1728e-04],\n",
            "          [-5.8501e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.7068e-04],\n",
            "          [-1.2221e-03]],\n",
            "\n",
            "         [[-7.0638e-05],\n",
            "          [ 3.4510e-05]],\n",
            "\n",
            "         [[ 1.4956e-03],\n",
            "          [ 3.3000e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2320e-03],\n",
            "          [-4.1212e-03]],\n",
            "\n",
            "         [[-3.4216e-03],\n",
            "          [-3.6033e-03]],\n",
            "\n",
            "         [[ 1.4943e-03],\n",
            "          [-1.4667e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.3084e-03],\n",
            "          [-1.7484e-03]],\n",
            "\n",
            "         [[ 1.1199e-03],\n",
            "          [-1.6654e-03]],\n",
            "\n",
            "         [[-2.5173e-03],\n",
            "          [-2.2734e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.3429e-03],\n",
            "          [-1.9482e-03]],\n",
            "\n",
            "         [[ 7.9294e-04],\n",
            "          [ 3.9398e-03]],\n",
            "\n",
            "         [[-1.1844e-03],\n",
            "          [-3.0628e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3829e-03],\n",
            "          [-4.5038e-04]],\n",
            "\n",
            "         [[ 3.9811e-03],\n",
            "          [ 1.2167e-03]],\n",
            "\n",
            "         [[-4.2502e-03],\n",
            "          [-4.7939e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4869e-04],\n",
            "          [ 3.2590e-03]],\n",
            "\n",
            "         [[ 3.8920e-03],\n",
            "          [-6.4319e-04]],\n",
            "\n",
            "         [[-2.2292e-03],\n",
            "          [-1.3597e-03]]]]), 'exp_avg_sq': tensor([[[[8.4840e-05],\n",
            "          [6.9654e-05]],\n",
            "\n",
            "         [[1.0465e-04],\n",
            "          [1.1918e-04]],\n",
            "\n",
            "         [[2.6338e-05],\n",
            "          [5.8632e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.2557e-05],\n",
            "          [5.4929e-05]],\n",
            "\n",
            "         [[1.0518e-04],\n",
            "          [4.8864e-05]],\n",
            "\n",
            "         [[6.0888e-05],\n",
            "          [7.5983e-05]]],\n",
            "\n",
            "\n",
            "        [[[5.2832e-05],\n",
            "          [1.9289e-04]],\n",
            "\n",
            "         [[1.0230e-04],\n",
            "          [1.6821e-04]],\n",
            "\n",
            "         [[1.2843e-04],\n",
            "          [8.7438e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.3795e-05],\n",
            "          [7.7764e-05]],\n",
            "\n",
            "         [[9.2687e-05],\n",
            "          [1.4143e-04]],\n",
            "\n",
            "         [[7.3175e-05],\n",
            "          [1.0898e-04]]],\n",
            "\n",
            "\n",
            "        [[[2.2995e-05],\n",
            "          [3.5124e-05]],\n",
            "\n",
            "         [[3.4429e-05],\n",
            "          [1.7640e-05]],\n",
            "\n",
            "         [[3.5981e-05],\n",
            "          [3.1822e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.2792e-05],\n",
            "          [1.3272e-05]],\n",
            "\n",
            "         [[3.1623e-05],\n",
            "          [2.6147e-05]],\n",
            "\n",
            "         [[1.7963e-05],\n",
            "          [1.3974e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[7.3535e-05],\n",
            "          [1.6408e-04]],\n",
            "\n",
            "         [[1.1124e-04],\n",
            "          [1.1191e-04]],\n",
            "\n",
            "         [[2.0598e-04],\n",
            "          [8.6475e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[3.8551e-05],\n",
            "          [8.7016e-05]],\n",
            "\n",
            "         [[1.1459e-04],\n",
            "          [3.6129e-04]],\n",
            "\n",
            "         [[8.0145e-05],\n",
            "          [1.4643e-04]]],\n",
            "\n",
            "\n",
            "        [[[1.0008e-04],\n",
            "          [2.7431e-05]],\n",
            "\n",
            "         [[7.4501e-05],\n",
            "          [4.8335e-05]],\n",
            "\n",
            "         [[2.7811e-05],\n",
            "          [1.1231e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[5.4357e-05],\n",
            "          [2.2907e-05]],\n",
            "\n",
            "         [[7.9084e-05],\n",
            "          [1.0711e-04]],\n",
            "\n",
            "         [[6.0524e-05],\n",
            "          [4.1642e-05]]],\n",
            "\n",
            "\n",
            "        [[[7.2179e-05],\n",
            "          [1.2426e-04]],\n",
            "\n",
            "         [[1.5293e-04],\n",
            "          [1.2358e-04]],\n",
            "\n",
            "         [[3.4709e-04],\n",
            "          [3.9061e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.2505e-05],\n",
            "          [1.7204e-04]],\n",
            "\n",
            "         [[3.8153e-04],\n",
            "          [1.0455e-04]],\n",
            "\n",
            "         [[1.9373e-04],\n",
            "          [9.0004e-05]]]])}, 5: {'step': 8100, 'exp_avg': tensor([-2.6412e-10,  1.0789e-10,  9.2020e-11, -2.4736e-11, -3.5665e-11,\n",
            "        -9.5871e-12, -6.4779e-11, -3.2436e-11, -5.6961e-11, -2.5265e-10,\n",
            "         1.8988e-10,  4.5592e-10,  3.7254e-10,  7.0206e-11,  3.4323e-10,\n",
            "        -4.2053e-12, -1.3948e-10, -1.3663e-10, -1.3992e-10, -1.4765e-10,\n",
            "         2.6462e-12,  4.4000e-10, -3.6995e-11, -5.8513e-14, -4.3331e-10,\n",
            "         2.3212e-10, -5.9220e-11, -4.0272e-11, -6.8259e-10, -3.5059e-11,\n",
            "         7.7650e-11, -6.3729e-11]), 'exp_avg_sq': tensor([7.2351e-19, 5.2779e-19, 1.7032e-19, 1.8730e-19, 1.1573e-19, 6.2602e-19,\n",
            "        5.8623e-20, 1.2803e-19, 4.3482e-20, 3.2457e-18, 2.5442e-19, 3.1797e-18,\n",
            "        8.5709e-19, 2.0113e-19, 5.6154e-19, 7.7817e-19, 1.4762e-19, 6.9415e-19,\n",
            "        6.9749e-19, 3.0722e-19, 2.0582e-19, 4.0803e-18, 5.4942e-19, 2.6790e-18,\n",
            "        3.7204e-18, 2.2545e-18, 2.4566e-19, 3.3781e-19, 4.9899e-18, 1.0827e-18,\n",
            "        7.4849e-19, 1.0877e-18])}, 6: {'step': 8100, 'exp_avg': tensor([-1.8936e-03,  1.5655e-03,  9.5310e-04,  5.6611e-04,  3.8996e-04,\n",
            "         2.1640e-03, -3.0354e-04,  4.8895e-04, -4.6363e-04, -2.9637e-03,\n",
            "        -1.4715e-03, -6.1124e-03, -2.1493e-03,  6.4254e-04,  1.5721e-03,\n",
            "         2.0255e-03, -5.2563e-04, -1.5114e-03, -3.0514e-03, -6.4148e-04,\n",
            "        -8.3187e-04,  4.6529e-03, -2.3928e-03,  4.9878e-04, -6.8221e-03,\n",
            "         9.9785e-05, -1.3862e-03,  1.6542e-03,  5.0566e-03, -5.4222e-03,\n",
            "         3.9272e-03,  9.3374e-04]), 'exp_avg_sq': tensor([4.1573e-05, 2.0421e-05, 7.7825e-06, 1.6413e-05, 1.2646e-05, 5.5912e-05,\n",
            "        8.3829e-06, 6.2456e-06, 2.8639e-06, 7.2542e-05, 1.5075e-05, 1.0659e-04,\n",
            "        6.1085e-05, 7.8066e-06, 1.1891e-05, 5.0786e-05, 9.2045e-06, 2.4156e-05,\n",
            "        4.7356e-05, 1.5373e-05, 1.0938e-05, 1.0406e-04, 2.5394e-05, 8.1802e-05,\n",
            "        1.8937e-04, 4.7086e-05, 3.0585e-05, 1.2061e-05, 9.7243e-05, 6.2377e-05,\n",
            "        4.0126e-05, 4.5503e-05])}, 7: {'step': 8100, 'exp_avg': tensor([-3.4043e-03,  2.1459e-03,  1.4581e-03,  1.9398e-03,  1.2778e-03,\n",
            "         3.1748e-03,  1.1415e-03,  7.9678e-04,  1.9845e-04, -3.5453e-03,\n",
            "        -3.8656e-04, -6.4625e-03, -1.2065e-03,  1.2851e-03,  1.5438e-03,\n",
            "         4.9997e-03,  6.5924e-05, -2.2681e-03, -2.1919e-03, -5.2105e-04,\n",
            "        -1.5483e-04,  6.6795e-03, -2.9707e-03,  2.2153e-03, -6.9697e-03,\n",
            "         2.2958e-03, -1.2090e-03,  2.6941e-03,  7.2056e-03, -5.5653e-03,\n",
            "         4.0680e-03,  3.3452e-03]), 'exp_avg_sq': tensor([5.1951e-05, 1.5785e-05, 7.5587e-06, 1.2443e-05, 7.0650e-06, 5.3181e-05,\n",
            "        6.2203e-06, 6.6516e-06, 3.0312e-06, 1.0012e-04, 7.6299e-06, 1.1228e-04,\n",
            "        2.0272e-05, 9.4925e-06, 1.3176e-05, 5.6543e-05, 5.7243e-06, 2.0580e-05,\n",
            "        4.1493e-05, 5.5422e-06, 7.2407e-06, 8.8276e-05, 1.9857e-05, 6.0870e-05,\n",
            "        1.8599e-04, 3.8582e-05, 1.6774e-05, 1.5733e-05, 1.5787e-04, 4.9919e-05,\n",
            "        4.8358e-05, 4.8807e-05])}, 8: {'step': 8100, 'exp_avg': tensor([[[[ 8.9538e-04,  6.5182e-04,  1.0616e-03,  ..., -9.2285e-05,\n",
            "           -1.6458e-04, -1.7160e-03],\n",
            "          [ 1.6063e-03, -2.7350e-03,  2.6373e-03,  ...,  7.8245e-04,\n",
            "            5.2547e-04, -1.0227e-04],\n",
            "          [-1.0460e-03, -4.0834e-03, -1.0865e-03,  ...,  5.8211e-04,\n",
            "           -6.3087e-04,  1.5256e-04],\n",
            "          ...,\n",
            "          [ 9.8473e-04, -3.3943e-06,  2.3131e-03,  ...,  1.2788e-03,\n",
            "           -3.5422e-04,  4.6598e-04],\n",
            "          [ 4.3210e-04,  1.0314e-03,  3.8036e-03,  ...,  8.3062e-04,\n",
            "           -1.2962e-04, -1.0129e-03],\n",
            "          [-3.3587e-04,  6.4338e-04, -1.0233e-04,  ...,  4.3434e-05,\n",
            "            1.2158e-04, -2.6663e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 9.0406e-04,  9.3316e-04,  4.8632e-04,  ...,  5.4924e-04,\n",
            "            8.4873e-04, -4.2211e-04],\n",
            "          [-9.9737e-05, -8.1662e-04, -1.8228e-03,  ..., -8.1484e-05,\n",
            "            4.4311e-04,  2.4829e-04],\n",
            "          [ 1.8635e-03,  3.4545e-04,  5.0446e-04,  ..., -4.7573e-04,\n",
            "           -2.7759e-04,  3.8690e-04],\n",
            "          ...,\n",
            "          [ 1.2343e-03,  4.3102e-04,  1.2404e-03,  ..., -3.2800e-04,\n",
            "           -7.2052e-04,  1.4849e-04],\n",
            "          [ 1.0740e-03,  2.3392e-03,  5.2951e-05,  ...,  4.2034e-04,\n",
            "           -6.8953e-04,  1.2630e-04],\n",
            "          [ 1.4531e-03, -2.1368e-03,  5.2157e-03,  ...,  9.6890e-05,\n",
            "            1.7148e-03, -2.7599e-04]]],\n",
            "\n",
            "\n",
            "        [[[-3.4187e-04, -6.4263e-05, -9.2280e-04,  ...,  2.8139e-04,\n",
            "           -2.4218e-04,  8.0395e-04],\n",
            "          [-1.0222e-03,  3.7895e-03, -2.7767e-03,  ..., -2.0228e-04,\n",
            "           -1.1648e-03,  1.4024e-03],\n",
            "          [ 3.7821e-04,  2.1096e-03,  7.1350e-04,  ...,  3.9832e-04,\n",
            "           -6.4206e-04,  1.6499e-03],\n",
            "          ...,\n",
            "          [-1.2070e-03,  4.2944e-04, -1.5790e-03,  ...,  5.0108e-04,\n",
            "            1.4694e-04, -7.7547e-05],\n",
            "          [-1.4277e-03, -3.6799e-03, -8.3572e-05,  ...,  6.8640e-05,\n",
            "           -3.2308e-04, -7.4488e-05],\n",
            "          [-1.8868e-03, -4.1752e-03, -4.0839e-04,  ...,  2.2434e-04,\n",
            "            3.5516e-04,  2.0173e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.5417e-05,  1.5683e-04,  3.0237e-05,  ...,  5.8822e-05,\n",
            "            1.7874e-04,  4.2749e-05],\n",
            "          [-6.3820e-04, -8.8946e-04, -4.8034e-04,  ..., -2.3638e-04,\n",
            "            1.2446e-04, -2.2951e-04],\n",
            "          [-2.6448e-04, -5.4808e-04,  1.1237e-05,  ...,  1.4642e-04,\n",
            "            1.4201e-05,  4.1649e-04],\n",
            "          ...,\n",
            "          [-2.4292e-04, -3.8368e-04, -1.5161e-04,  ..., -1.2652e-04,\n",
            "           -3.4619e-04,  3.0332e-04],\n",
            "          [-6.5636e-04, -7.2491e-04, -8.3545e-04,  ...,  2.5489e-04,\n",
            "           -4.9881e-04, -5.2950e-06],\n",
            "          [-7.5971e-04, -2.0333e-04, -9.4029e-04,  ..., -7.9237e-05,\n",
            "           -9.3897e-04,  8.0101e-05]]],\n",
            "\n",
            "\n",
            "        [[[-8.0924e-04,  2.2864e-04, -1.2447e-03,  ..., -3.1787e-04,\n",
            "           -3.2733e-04,  7.3297e-04],\n",
            "          [ 7.3022e-05,  1.5516e-04, -4.4735e-04,  ..., -1.1860e-05,\n",
            "            9.9284e-05,  7.5190e-04],\n",
            "          [ 1.0641e-03,  1.9580e-03,  1.1527e-03,  ..., -3.4068e-04,\n",
            "           -6.0162e-04,  3.8094e-04],\n",
            "          ...,\n",
            "          [ 6.7935e-04, -9.3461e-04,  1.1412e-03,  ..., -1.0415e-03,\n",
            "           -4.2676e-04,  2.1193e-04],\n",
            "          [ 1.5156e-04,  9.5118e-04, -2.6664e-04,  ...,  7.4156e-04,\n",
            "            1.7855e-04, -6.1526e-04],\n",
            "          [-2.5067e-04, -5.1064e-04, -5.0832e-04,  ...,  1.4993e-04,\n",
            "           -3.2824e-04,  1.3466e-05]]],\n",
            "\n",
            "\n",
            "        [[[-3.6551e-04, -3.8398e-03, -1.0980e-03,  ..., -2.4594e-04,\n",
            "           -2.1488e-03,  5.9543e-04],\n",
            "          [ 4.4112e-04,  1.8200e-03, -1.1356e-03,  ...,  3.7047e-06,\n",
            "           -6.1760e-04,  4.4197e-05],\n",
            "          [ 1.8299e-03,  3.2430e-03,  1.0897e-03,  ..., -3.1743e-04,\n",
            "            5.9695e-05, -8.7016e-05],\n",
            "          ...,\n",
            "          [ 2.0568e-03,  5.1938e-03,  7.0898e-04,  ..., -9.2397e-05,\n",
            "            6.8042e-04,  1.5326e-04],\n",
            "          [ 1.6090e-03, -2.9014e-03,  2.2523e-03,  ..., -3.3646e-04,\n",
            "            2.1453e-04,  1.7534e-04],\n",
            "          [-3.8104e-04,  9.8626e-04, -7.7150e-04,  ..., -1.0705e-04,\n",
            "            1.6678e-04, -1.6262e-04]]]]), 'exp_avg_sq': tensor([[[[8.9514e-06, 2.6181e-05, 1.3122e-05,  ..., 4.6461e-06,\n",
            "           9.7105e-06, 1.2448e-05],\n",
            "          [1.7430e-05, 9.9309e-05, 4.7937e-05,  ..., 5.6565e-06,\n",
            "           1.0400e-05, 4.7820e-06],\n",
            "          [1.5488e-05, 7.6979e-05, 2.2627e-05,  ..., 4.8194e-06,\n",
            "           9.9633e-06, 1.4350e-05],\n",
            "          ...,\n",
            "          [1.7857e-05, 1.5901e-04, 4.7043e-05,  ..., 9.2057e-06,\n",
            "           3.2220e-06, 4.8314e-06],\n",
            "          [2.3559e-05, 1.0456e-04, 6.4258e-05,  ..., 7.6275e-06,\n",
            "           5.0639e-06, 8.2134e-06],\n",
            "          [1.2016e-05, 4.2860e-05, 1.8639e-05,  ..., 1.5605e-05,\n",
            "           5.9723e-06, 6.4979e-06]]],\n",
            "\n",
            "\n",
            "        [[[1.0896e-05, 2.0766e-05, 2.4928e-05,  ..., 5.1298e-06,\n",
            "           7.2011e-06, 5.1484e-06],\n",
            "          [1.3212e-05, 5.0650e-05, 2.5154e-05,  ..., 4.0239e-06,\n",
            "           9.0652e-06, 6.0224e-06],\n",
            "          [7.7649e-06, 6.0144e-05, 3.2658e-05,  ..., 8.7253e-06,\n",
            "           7.3368e-06, 3.5170e-06],\n",
            "          ...,\n",
            "          [1.0610e-05, 9.8625e-05, 3.7252e-05,  ..., 5.4225e-06,\n",
            "           6.8672e-06, 6.4567e-06],\n",
            "          [7.3715e-06, 2.0885e-05, 1.0510e-05,  ..., 7.3090e-06,\n",
            "           5.6579e-06, 5.1494e-06],\n",
            "          [2.0621e-05, 1.3624e-04, 6.8484e-05,  ..., 5.4395e-06,\n",
            "           8.9299e-06, 4.4464e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.6585e-06, 1.5223e-04, 3.0383e-05,  ..., 3.2227e-06,\n",
            "           7.7672e-06, 8.1742e-06],\n",
            "          [1.3754e-05, 7.2496e-05, 4.1775e-05,  ..., 4.6119e-06,\n",
            "           1.2934e-05, 1.1666e-05],\n",
            "          [1.0089e-05, 2.7372e-05, 2.4429e-05,  ..., 7.5589e-06,\n",
            "           1.1776e-05, 1.1233e-05],\n",
            "          ...,\n",
            "          [2.7020e-05, 3.0687e-05, 4.9072e-05,  ..., 1.0108e-05,\n",
            "           5.6774e-06, 6.3155e-06],\n",
            "          [2.1067e-05, 7.8699e-05, 3.0979e-05,  ..., 7.4078e-06,\n",
            "           7.3167e-06, 8.5106e-06],\n",
            "          [4.9219e-05, 1.3023e-04, 7.1411e-05,  ..., 8.3846e-06,\n",
            "           2.1823e-05, 6.9158e-06]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[3.7340e-06, 9.9591e-06, 4.9916e-06,  ..., 1.3254e-06,\n",
            "           1.4075e-06, 1.3336e-06],\n",
            "          [3.8054e-06, 6.2356e-06, 1.3144e-05,  ..., 1.4127e-06,\n",
            "           2.8213e-06, 2.2555e-06],\n",
            "          [4.2680e-06, 2.4094e-05, 8.9918e-06,  ..., 1.1788e-06,\n",
            "           2.2036e-06, 2.3013e-06],\n",
            "          ...,\n",
            "          [1.0617e-05, 4.3611e-05, 2.0283e-05,  ..., 3.5161e-06,\n",
            "           2.7492e-06, 2.1546e-06],\n",
            "          [4.0704e-06, 7.2490e-06, 5.9703e-06,  ..., 2.3373e-06,\n",
            "           1.0100e-06, 1.0761e-06],\n",
            "          [3.0972e-06, 2.0812e-05, 6.7701e-06,  ..., 1.0818e-06,\n",
            "           2.4021e-06, 2.7368e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.1903e-06, 4.1060e-05, 2.2791e-05,  ..., 3.2646e-06,\n",
            "           5.4089e-06, 4.8495e-06],\n",
            "          [6.9855e-06, 1.3931e-05, 1.8848e-05,  ..., 2.4269e-06,\n",
            "           2.8603e-06, 4.5939e-06],\n",
            "          [1.1954e-05, 4.0525e-05, 1.9083e-05,  ..., 6.5616e-06,\n",
            "           5.0465e-06, 4.2234e-06],\n",
            "          ...,\n",
            "          [2.1777e-05, 8.4084e-05, 5.5408e-05,  ..., 8.0151e-06,\n",
            "           4.4740e-06, 4.0197e-06],\n",
            "          [1.2434e-05, 2.0821e-05, 2.0376e-05,  ..., 5.6585e-06,\n",
            "           2.9772e-06, 4.0726e-06],\n",
            "          [1.0060e-05, 1.9546e-05, 1.7385e-05,  ..., 2.4228e-06,\n",
            "           4.4158e-06, 4.6292e-06]]],\n",
            "\n",
            "\n",
            "        [[[1.1433e-05, 7.1974e-05, 1.7793e-05,  ..., 4.0719e-06,\n",
            "           8.9254e-06, 4.9746e-06],\n",
            "          [7.4039e-06, 2.2327e-05, 1.3628e-05,  ..., 2.9936e-06,\n",
            "           6.1604e-06, 6.8948e-06],\n",
            "          [9.2197e-06, 4.4741e-05, 2.1785e-05,  ..., 4.0221e-06,\n",
            "           5.2751e-06, 4.8203e-06],\n",
            "          ...,\n",
            "          [1.0118e-05, 1.4069e-04, 4.5375e-05,  ..., 4.6942e-06,\n",
            "           6.6353e-06, 5.6445e-06],\n",
            "          [9.2437e-06, 1.5568e-04, 3.9428e-05,  ..., 8.8580e-06,\n",
            "           4.3043e-06, 4.7663e-06],\n",
            "          [1.0464e-05, 2.0555e-05, 1.9573e-05,  ..., 9.9760e-06,\n",
            "           3.5440e-06, 3.2181e-06]]]])}, 9: {'step': 8100, 'exp_avg': tensor([-3.5472e-05, -3.9919e-06,  4.8319e-05, -7.8228e-05,  3.8669e-06,\n",
            "        -1.5824e-05, -7.4206e-05,  9.9378e-05, -9.1940e-05, -2.1466e-04,\n",
            "         9.4662e-07, -1.8380e-05,  4.8411e-05,  1.2732e-04,  6.8456e-05,\n",
            "         4.1808e-05]), 'exp_avg_sq': tensor([4.8360e-08, 4.4067e-08, 8.6008e-08, 2.1023e-07, 1.8090e-07, 3.1408e-07,\n",
            "        9.4342e-08, 1.1375e-07, 2.3297e-07, 3.2058e-07, 1.6303e-07, 3.4847e-08,\n",
            "        1.7774e-07, 1.0935e-07, 3.0807e-07, 6.2492e-08])}, 12: {'step': 8100, 'exp_avg': tensor([[[[-5.4819e-06],\n",
            "          [ 1.5077e-04],\n",
            "          [-9.6951e-04],\n",
            "          ...,\n",
            "          [ 4.5117e-04],\n",
            "          [ 3.5953e-04],\n",
            "          [-3.0983e-04]],\n",
            "\n",
            "         [[ 1.7811e-04],\n",
            "          [-3.1471e-04],\n",
            "          [-4.9041e-04],\n",
            "          ...,\n",
            "          [ 2.9137e-04],\n",
            "          [-3.3155e-04],\n",
            "          [-6.6466e-05]],\n",
            "\n",
            "         [[-2.7872e-04],\n",
            "          [-2.7352e-04],\n",
            "          [ 2.4304e-05],\n",
            "          ...,\n",
            "          [-1.0261e-04],\n",
            "          [-3.9405e-04],\n",
            "          [ 2.7600e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2741e-05],\n",
            "          [ 3.7062e-04],\n",
            "          [ 3.9161e-04],\n",
            "          ...,\n",
            "          [ 3.0546e-04],\n",
            "          [ 3.6787e-05],\n",
            "          [ 6.5303e-05]],\n",
            "\n",
            "         [[ 5.1320e-04],\n",
            "          [-8.4866e-05],\n",
            "          [ 1.6590e-05],\n",
            "          ...,\n",
            "          [-1.3791e-04],\n",
            "          [ 1.8534e-04],\n",
            "          [ 5.5338e-04]],\n",
            "\n",
            "         [[ 5.9166e-04],\n",
            "          [ 3.0262e-04],\n",
            "          [-5.6939e-04],\n",
            "          ...,\n",
            "          [-9.2490e-04],\n",
            "          [-6.6926e-05],\n",
            "          [ 3.2965e-04]]],\n",
            "\n",
            "\n",
            "        [[[-3.8924e-04],\n",
            "          [ 1.2230e-03],\n",
            "          [-2.0020e-04],\n",
            "          ...,\n",
            "          [-2.8780e-04],\n",
            "          [-6.0154e-04],\n",
            "          [-2.3987e-05]],\n",
            "\n",
            "         [[ 3.4886e-04],\n",
            "          [-5.3449e-04],\n",
            "          [-5.2376e-04],\n",
            "          ...,\n",
            "          [-2.7274e-04],\n",
            "          [ 2.0429e-04],\n",
            "          [ 7.3326e-04]],\n",
            "\n",
            "         [[-1.0965e-03],\n",
            "          [-3.7163e-05],\n",
            "          [ 9.2625e-05],\n",
            "          ...,\n",
            "          [-8.6951e-05],\n",
            "          [-8.0484e-04],\n",
            "          [ 5.0035e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.3365e-05],\n",
            "          [ 5.6469e-04],\n",
            "          [ 5.3569e-04],\n",
            "          ...,\n",
            "          [ 3.1241e-04],\n",
            "          [-1.4016e-05],\n",
            "          [-6.5251e-04]],\n",
            "\n",
            "         [[ 4.2586e-04],\n",
            "          [ 1.9988e-04],\n",
            "          [-2.6233e-05],\n",
            "          ...,\n",
            "          [-7.1282e-05],\n",
            "          [-6.8596e-05],\n",
            "          [-1.1661e-03]],\n",
            "\n",
            "         [[ 7.4846e-06],\n",
            "          [ 4.2781e-04],\n",
            "          [ 4.4368e-04],\n",
            "          ...,\n",
            "          [-3.8304e-04],\n",
            "          [ 8.2943e-04],\n",
            "          [ 4.7598e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4424e-04],\n",
            "          [-5.0360e-04],\n",
            "          [ 8.4099e-04],\n",
            "          ...,\n",
            "          [ 5.6444e-04],\n",
            "          [-5.8803e-04],\n",
            "          [ 1.8504e-03]],\n",
            "\n",
            "         [[-9.4659e-05],\n",
            "          [-2.0611e-04],\n",
            "          [-1.6612e-04],\n",
            "          ...,\n",
            "          [ 2.2289e-04],\n",
            "          [-6.5530e-04],\n",
            "          [-3.4046e-04]],\n",
            "\n",
            "         [[ 1.2034e-03],\n",
            "          [-3.3251e-04],\n",
            "          [ 6.0455e-04],\n",
            "          ...,\n",
            "          [-6.3371e-04],\n",
            "          [ 6.1700e-07],\n",
            "          [ 2.8879e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.4496e-04],\n",
            "          [-3.6428e-04],\n",
            "          [-2.5795e-04],\n",
            "          ...,\n",
            "          [ 5.9749e-04],\n",
            "          [ 1.6281e-04],\n",
            "          [-5.3728e-04]],\n",
            "\n",
            "         [[-3.8555e-05],\n",
            "          [ 1.1380e-04],\n",
            "          [ 1.2125e-03],\n",
            "          ...,\n",
            "          [ 2.2825e-04],\n",
            "          [ 7.7381e-04],\n",
            "          [-1.0541e-03]],\n",
            "\n",
            "         [[ 3.9070e-04],\n",
            "          [ 2.1295e-04],\n",
            "          [-4.2879e-05],\n",
            "          ...,\n",
            "          [ 9.3227e-04],\n",
            "          [ 6.3686e-05],\n",
            "          [-3.3971e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.4009e-04],\n",
            "          [-8.4599e-04],\n",
            "          [-3.3726e-05],\n",
            "          ...,\n",
            "          [ 1.7543e-04],\n",
            "          [-6.6760e-04],\n",
            "          [-3.7251e-05]],\n",
            "\n",
            "         [[ 1.7911e-04],\n",
            "          [-7.4115e-04],\n",
            "          [ 6.6424e-04],\n",
            "          ...,\n",
            "          [-5.2400e-04],\n",
            "          [ 2.8183e-05],\n",
            "          [ 4.4821e-04]],\n",
            "\n",
            "         [[-3.3975e-04],\n",
            "          [ 4.2915e-05],\n",
            "          [-2.3976e-04],\n",
            "          ...,\n",
            "          [-2.7436e-04],\n",
            "          [ 5.5472e-04],\n",
            "          [ 6.6844e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.6062e-06],\n",
            "          [ 8.0495e-04],\n",
            "          [ 1.1265e-05],\n",
            "          ...,\n",
            "          [ 4.8325e-05],\n",
            "          [ 8.2351e-05],\n",
            "          [ 1.7395e-04]],\n",
            "\n",
            "         [[ 3.5251e-04],\n",
            "          [ 5.2138e-05],\n",
            "          [-4.8099e-04],\n",
            "          ...,\n",
            "          [-2.7913e-05],\n",
            "          [-7.4951e-04],\n",
            "          [-2.6217e-04]],\n",
            "\n",
            "         [[-2.7609e-04],\n",
            "          [-1.6377e-04],\n",
            "          [-3.8417e-04],\n",
            "          ...,\n",
            "          [-5.5368e-04],\n",
            "          [-7.7618e-04],\n",
            "          [ 5.4002e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2109e-04],\n",
            "          [ 4.1466e-04],\n",
            "          [-1.4635e-03],\n",
            "          ...,\n",
            "          [ 2.8668e-04],\n",
            "          [ 1.7176e-03],\n",
            "          [-1.1607e-03]],\n",
            "\n",
            "         [[-9.5557e-04],\n",
            "          [-2.0250e-04],\n",
            "          [-9.3569e-04],\n",
            "          ...,\n",
            "          [-1.8885e-04],\n",
            "          [ 4.0963e-04],\n",
            "          [-2.7853e-04]],\n",
            "\n",
            "         [[-7.0272e-04],\n",
            "          [ 9.8901e-04],\n",
            "          [-7.7036e-04],\n",
            "          ...,\n",
            "          [ 1.5496e-04],\n",
            "          [-5.0214e-04],\n",
            "          [-4.4909e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.6925e-04],\n",
            "          [ 8.9230e-04],\n",
            "          [ 1.5222e-04],\n",
            "          ...,\n",
            "          [-1.2139e-04],\n",
            "          [ 1.7315e-04],\n",
            "          [-6.4583e-04]],\n",
            "\n",
            "         [[ 5.6907e-04],\n",
            "          [-1.2944e-04],\n",
            "          [-4.8975e-04],\n",
            "          ...,\n",
            "          [ 3.2427e-04],\n",
            "          [-4.0539e-04],\n",
            "          [ 2.3316e-04]],\n",
            "\n",
            "         [[ 8.5147e-04],\n",
            "          [ 2.5219e-04],\n",
            "          [-1.1186e-03],\n",
            "          ...,\n",
            "          [-6.5602e-04],\n",
            "          [ 1.7470e-03],\n",
            "          [ 5.3386e-05]]],\n",
            "\n",
            "\n",
            "        [[[-6.9431e-04],\n",
            "          [ 7.7861e-04],\n",
            "          [ 2.9709e-04],\n",
            "          ...,\n",
            "          [-8.2856e-04],\n",
            "          [ 9.2735e-04],\n",
            "          [ 6.0560e-04]],\n",
            "\n",
            "         [[-4.0976e-05],\n",
            "          [ 4.3979e-04],\n",
            "          [ 3.0791e-05],\n",
            "          ...,\n",
            "          [-3.3949e-04],\n",
            "          [-4.0518e-05],\n",
            "          [-7.8793e-05]],\n",
            "\n",
            "         [[-1.9311e-05],\n",
            "          [-1.7680e-04],\n",
            "          [-2.5333e-04],\n",
            "          ...,\n",
            "          [-8.5041e-04],\n",
            "          [ 7.5565e-04],\n",
            "          [-1.8034e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5718e-04],\n",
            "          [-5.1634e-04],\n",
            "          [-4.3258e-05],\n",
            "          ...,\n",
            "          [ 1.7651e-04],\n",
            "          [ 1.8765e-04],\n",
            "          [ 1.5530e-04]],\n",
            "\n",
            "         [[ 2.1152e-04],\n",
            "          [ 5.5683e-04],\n",
            "          [ 1.6315e-04],\n",
            "          ...,\n",
            "          [ 6.8687e-04],\n",
            "          [-2.1526e-04],\n",
            "          [ 2.0857e-04]],\n",
            "\n",
            "         [[-5.6194e-04],\n",
            "          [ 7.6163e-04],\n",
            "          [ 7.2578e-04],\n",
            "          ...,\n",
            "          [-4.3170e-04],\n",
            "          [-7.2728e-04],\n",
            "          [ 4.9535e-04]]]]), 'exp_avg_sq': tensor([[[[3.2123e-06],\n",
            "          [2.7450e-06],\n",
            "          [5.8564e-06],\n",
            "          ...,\n",
            "          [4.2303e-06],\n",
            "          [2.7422e-06],\n",
            "          [6.5835e-06]],\n",
            "\n",
            "         [[1.9566e-06],\n",
            "          [2.2903e-06],\n",
            "          [1.8654e-06],\n",
            "          ...,\n",
            "          [3.6384e-06],\n",
            "          [3.0870e-06],\n",
            "          [1.1974e-06]],\n",
            "\n",
            "         [[2.4485e-06],\n",
            "          [3.6370e-06],\n",
            "          [2.2721e-06],\n",
            "          ...,\n",
            "          [3.3807e-06],\n",
            "          [6.0475e-06],\n",
            "          [2.4047e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[8.6586e-07],\n",
            "          [1.4753e-06],\n",
            "          [1.0661e-06],\n",
            "          ...,\n",
            "          [1.3031e-06],\n",
            "          [1.4763e-06],\n",
            "          [1.5371e-06]],\n",
            "\n",
            "         [[1.1106e-06],\n",
            "          [1.4623e-06],\n",
            "          [8.8370e-07],\n",
            "          ...,\n",
            "          [1.7234e-06],\n",
            "          [2.2201e-06],\n",
            "          [5.4494e-06]],\n",
            "\n",
            "         [[1.6747e-06],\n",
            "          [2.0973e-06],\n",
            "          [2.1973e-06],\n",
            "          ...,\n",
            "          [3.6785e-06],\n",
            "          [3.0626e-06],\n",
            "          [7.4793e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.7755e-07],\n",
            "          [7.5882e-06],\n",
            "          [1.7335e-06],\n",
            "          ...,\n",
            "          [1.1058e-06],\n",
            "          [3.4736e-06],\n",
            "          [1.2628e-06]],\n",
            "\n",
            "         [[7.7460e-07],\n",
            "          [4.5431e-06],\n",
            "          [3.0960e-06],\n",
            "          ...,\n",
            "          [2.5620e-06],\n",
            "          [8.3327e-07],\n",
            "          [2.9689e-06]],\n",
            "\n",
            "         [[5.5274e-06],\n",
            "          [1.1556e-06],\n",
            "          [1.6862e-06],\n",
            "          ...,\n",
            "          [3.0225e-06],\n",
            "          [7.0212e-06],\n",
            "          [2.5778e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.6404e-06],\n",
            "          [1.4949e-06],\n",
            "          [2.4103e-06],\n",
            "          ...,\n",
            "          [1.6141e-06],\n",
            "          [1.5437e-06],\n",
            "          [1.8051e-06]],\n",
            "\n",
            "         [[9.4046e-07],\n",
            "          [3.3375e-07],\n",
            "          [5.4191e-07],\n",
            "          ...,\n",
            "          [6.6488e-07],\n",
            "          [2.1216e-06],\n",
            "          [5.6812e-06]],\n",
            "\n",
            "         [[4.1871e-07],\n",
            "          [1.1518e-06],\n",
            "          [2.2340e-06],\n",
            "          ...,\n",
            "          [2.1534e-06],\n",
            "          [5.3505e-06],\n",
            "          [2.4057e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.7237e-06],\n",
            "          [5.1477e-06],\n",
            "          [6.8760e-06],\n",
            "          ...,\n",
            "          [6.3617e-06],\n",
            "          [9.2253e-06],\n",
            "          [1.3071e-05]],\n",
            "\n",
            "         [[4.2554e-06],\n",
            "          [3.3677e-06],\n",
            "          [4.2443e-06],\n",
            "          ...,\n",
            "          [4.8353e-06],\n",
            "          [9.0654e-06],\n",
            "          [3.4363e-06]],\n",
            "\n",
            "         [[8.6938e-06],\n",
            "          [7.9315e-06],\n",
            "          [6.6245e-06],\n",
            "          ...,\n",
            "          [8.0432e-06],\n",
            "          [7.8980e-06],\n",
            "          [6.1812e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[6.6566e-06],\n",
            "          [2.4287e-06],\n",
            "          [2.5142e-06],\n",
            "          ...,\n",
            "          [1.7393e-05],\n",
            "          [2.1561e-06],\n",
            "          [4.3683e-06]],\n",
            "\n",
            "         [[1.5338e-06],\n",
            "          [2.5674e-06],\n",
            "          [8.6712e-06],\n",
            "          ...,\n",
            "          [3.1345e-06],\n",
            "          [6.2503e-06],\n",
            "          [5.3464e-06]],\n",
            "\n",
            "         [[4.2497e-06],\n",
            "          [3.9348e-06],\n",
            "          [3.2960e-06],\n",
            "          ...,\n",
            "          [9.2169e-06],\n",
            "          [8.0406e-06],\n",
            "          [1.0411e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[4.1198e-06],\n",
            "          [6.6711e-06],\n",
            "          [2.1400e-06],\n",
            "          ...,\n",
            "          [4.5769e-06],\n",
            "          [5.1406e-06],\n",
            "          [1.8987e-06]],\n",
            "\n",
            "         [[2.8416e-06],\n",
            "          [6.2454e-06],\n",
            "          [4.9703e-06],\n",
            "          ...,\n",
            "          [7.6557e-06],\n",
            "          [1.1057e-06],\n",
            "          [3.5567e-06]],\n",
            "\n",
            "         [[4.1928e-06],\n",
            "          [1.3505e-06],\n",
            "          [4.2498e-06],\n",
            "          ...,\n",
            "          [1.6303e-05],\n",
            "          [3.3001e-06],\n",
            "          [4.9202e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1005e-06],\n",
            "          [3.0460e-06],\n",
            "          [3.1098e-06],\n",
            "          ...,\n",
            "          [3.8747e-06],\n",
            "          [7.0625e-07],\n",
            "          [2.6259e-06]],\n",
            "\n",
            "         [[1.8645e-06],\n",
            "          [8.3968e-07],\n",
            "          [1.1395e-06],\n",
            "          ...,\n",
            "          [1.1798e-06],\n",
            "          [3.7829e-06],\n",
            "          [2.5971e-06]],\n",
            "\n",
            "         [[1.4270e-06],\n",
            "          [2.2330e-06],\n",
            "          [1.6842e-06],\n",
            "          ...,\n",
            "          [3.4985e-06],\n",
            "          [8.8660e-06],\n",
            "          [3.8602e-06]]],\n",
            "\n",
            "\n",
            "        [[[2.6080e-06],\n",
            "          [4.1796e-06],\n",
            "          [1.0995e-05],\n",
            "          ...,\n",
            "          [5.4587e-06],\n",
            "          [8.6775e-06],\n",
            "          [1.7161e-05]],\n",
            "\n",
            "         [[5.6615e-06],\n",
            "          [3.7075e-06],\n",
            "          [6.0222e-06],\n",
            "          ...,\n",
            "          [6.5382e-06],\n",
            "          [3.8460e-06],\n",
            "          [3.3992e-06]],\n",
            "\n",
            "         [[5.6605e-06],\n",
            "          [7.6464e-06],\n",
            "          [3.2051e-06],\n",
            "          ...,\n",
            "          [5.3449e-06],\n",
            "          [5.3462e-06],\n",
            "          [7.5302e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[2.7089e-06],\n",
            "          [3.2214e-06],\n",
            "          [1.9572e-06],\n",
            "          ...,\n",
            "          [2.0395e-06],\n",
            "          [2.0094e-06],\n",
            "          [3.9898e-06]],\n",
            "\n",
            "         [[2.4471e-06],\n",
            "          [2.8965e-06],\n",
            "          [2.3261e-06],\n",
            "          ...,\n",
            "          [1.8316e-06],\n",
            "          [3.3141e-06],\n",
            "          [3.9143e-06]],\n",
            "\n",
            "         [[4.3079e-06],\n",
            "          [2.5431e-06],\n",
            "          [6.6379e-06],\n",
            "          ...,\n",
            "          [3.9061e-06],\n",
            "          [8.1914e-06],\n",
            "          [6.3552e-06]]],\n",
            "\n",
            "\n",
            "        [[[5.0007e-06],\n",
            "          [2.9641e-06],\n",
            "          [3.5847e-06],\n",
            "          ...,\n",
            "          [3.5562e-06],\n",
            "          [5.3491e-06],\n",
            "          [5.4579e-06]],\n",
            "\n",
            "         [[2.6079e-06],\n",
            "          [2.2458e-06],\n",
            "          [1.8805e-06],\n",
            "          ...,\n",
            "          [1.2936e-06],\n",
            "          [1.5294e-06],\n",
            "          [2.1665e-06]],\n",
            "\n",
            "         [[2.5647e-06],\n",
            "          [3.7707e-06],\n",
            "          [2.0648e-06],\n",
            "          ...,\n",
            "          [2.0270e-06],\n",
            "          [5.1172e-06],\n",
            "          [6.5231e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.2610e-06],\n",
            "          [1.5841e-06],\n",
            "          [1.5510e-06],\n",
            "          ...,\n",
            "          [2.4083e-06],\n",
            "          [1.1567e-06],\n",
            "          [6.4278e-07]],\n",
            "\n",
            "         [[9.0264e-07],\n",
            "          [1.7480e-06],\n",
            "          [6.5717e-07],\n",
            "          ...,\n",
            "          [2.7449e-06],\n",
            "          [3.6347e-06],\n",
            "          [3.9592e-06]],\n",
            "\n",
            "         [[3.0472e-06],\n",
            "          [1.9351e-06],\n",
            "          [4.3509e-06],\n",
            "          ...,\n",
            "          [2.8559e-06],\n",
            "          [5.4718e-06],\n",
            "          [2.6461e-06]]]])}, 13: {'step': 8100, 'exp_avg': tensor([-2.9755e-11,  1.0616e-11,  5.6853e-11, -2.9304e-11, -9.6308e-11,\n",
            "         2.6700e-11, -1.5475e-10, -2.9917e-11,  2.7840e-11, -5.6429e-11,\n",
            "         1.5356e-11,  2.7952e-11, -6.7596e-11, -3.7354e-11,  2.8381e-11,\n",
            "         2.0129e-10, -3.9265e-13,  4.9659e-11,  3.8814e-11,  1.4366e-12,\n",
            "         5.5876e-11, -8.3454e-12,  2.5442e-11,  7.8127e-12, -2.5962e-11,\n",
            "        -1.0790e-12,  5.2754e-11,  2.8973e-11,  1.2904e-11, -5.8411e-11,\n",
            "        -2.9050e-11,  2.6298e-11]), 'exp_avg_sq': tensor([2.1708e-20, 7.0271e-20, 5.4154e-20, 2.0215e-20, 6.1007e-20, 9.6900e-21,\n",
            "        6.0713e-20, 1.7602e-20, 1.1565e-20, 2.4557e-20, 6.1481e-21, 1.2347e-20,\n",
            "        1.4177e-19, 1.5109e-20, 6.8975e-21, 5.6963e-19, 1.9431e-20, 1.9452e-20,\n",
            "        6.4852e-20, 1.3140e-19, 1.5765e-19, 1.5605e-20, 2.4271e-20, 2.3197e-20,\n",
            "        1.1281e-20, 1.1943e-19, 2.9574e-20, 1.6171e-20, 2.7185e-20, 6.6863e-20,\n",
            "        2.3416e-20, 3.1777e-20])}, 14: {'step': 8100, 'exp_avg': tensor([-6.7221e-05, -1.9659e-03,  4.3067e-04, -7.9770e-04, -6.6724e-05,\n",
            "         4.8924e-07, -7.7328e-04,  1.9368e-04,  9.4431e-05,  4.6645e-04,\n",
            "         3.9259e-05, -1.1548e-04,  1.0629e-03, -3.1677e-04,  6.9094e-06,\n",
            "         5.8289e-04,  7.5167e-05, -3.8752e-04, -5.7731e-06, -7.8681e-05,\n",
            "        -1.6890e-03, -1.1281e-04, -1.2384e-04,  2.3835e-04, -4.2526e-04,\n",
            "         7.2648e-04,  4.9529e-04, -2.4786e-04,  1.2740e-04, -1.4077e-03,\n",
            "        -2.1292e-05, -1.9059e-05]), 'exp_avg_sq': tensor([1.0023e-06, 1.2828e-05, 1.3723e-06, 4.6621e-06, 4.8678e-06, 7.6857e-07,\n",
            "        5.8384e-06, 3.5449e-06, 3.2402e-07, 7.6184e-07, 3.3023e-07, 5.4046e-07,\n",
            "        4.3199e-06, 5.3649e-07, 3.2610e-07, 1.3727e-05, 1.2946e-06, 1.0763e-06,\n",
            "        1.0715e-06, 2.3302e-06, 1.8646e-05, 1.1348e-06, 2.5762e-06, 9.9206e-07,\n",
            "        8.1631e-07, 3.7102e-06, 1.9937e-06, 6.4505e-07, 7.9662e-07, 9.1385e-06,\n",
            "        6.9070e-07, 1.6271e-06])}, 15: {'step': 8100, 'exp_avg': tensor([-2.2682e-05, -1.3953e-03,  4.2320e-04, -3.1788e-04,  1.4818e-04,\n",
            "        -8.9054e-05, -1.0737e-03,  2.5154e-04,  2.2450e-04,  4.0076e-04,\n",
            "         1.7470e-04, -1.5386e-04,  8.3557e-04, -1.0485e-04,  1.7056e-04,\n",
            "         7.3569e-04,  4.4053e-04, -6.9064e-05,  2.7781e-04,  1.0613e-04,\n",
            "        -1.3178e-03, -2.1543e-04, -8.3154e-05,  4.6812e-04, -3.3468e-04,\n",
            "         7.9652e-04,  4.2591e-04, -3.2435e-04, -5.7951e-06, -1.1207e-03,\n",
            "         3.6924e-05,  1.1718e-04]), 'exp_avg_sq': tensor([9.3474e-07, 6.6418e-06, 1.7549e-06, 1.2768e-06, 9.1421e-07, 6.6116e-07,\n",
            "        5.4174e-06, 8.4195e-07, 7.0300e-07, 8.4524e-07, 3.0914e-07, 8.3216e-07,\n",
            "        2.4884e-06, 7.5677e-07, 3.6831e-07, 7.4859e-06, 8.0618e-07, 9.7354e-07,\n",
            "        2.9902e-06, 1.4770e-06, 1.2817e-05, 1.7602e-06, 1.5981e-06, 8.0383e-07,\n",
            "        6.8786e-07, 5.4276e-06, 1.2887e-06, 1.0183e-06, 1.1649e-06, 6.6560e-06,\n",
            "        4.9433e-07, 1.5571e-06])}, 16: {'step': 8100, 'exp_avg': tensor([[ 1.3093e-04, -7.4676e-04, -1.2488e-03,  ..., -2.5546e-04,\n",
            "         -1.0503e-04, -2.4033e-04],\n",
            "        [-1.0651e-04,  5.1511e-04,  5.3354e-04,  ...,  6.1711e-05,\n",
            "          1.9304e-04,  7.1226e-05],\n",
            "        [-1.3785e-04, -1.0225e-03, -1.8771e-03,  ..., -2.9358e-04,\n",
            "         -6.3273e-04, -2.7118e-04],\n",
            "        ...,\n",
            "        [ 3.8345e-04,  2.3662e-06,  6.6228e-05,  ..., -1.8946e-06,\n",
            "          4.5187e-12, -1.6235e-07],\n",
            "        [ 2.9805e-05, -4.4260e-05, -9.9853e-05,  ...,  1.1830e-04,\n",
            "         -2.9680e-04,  2.5570e-04],\n",
            "        [-4.1777e-04, -1.5370e-04, -5.5178e-05,  ..., -1.6225e-04,\n",
            "         -7.1201e-06, -4.0276e-04]]), 'exp_avg_sq': tensor([[2.7326e-06, 3.2547e-06, 2.2837e-06,  ..., 4.5791e-07, 2.4969e-07,\n",
            "         1.7255e-06],\n",
            "        [7.9287e-07, 8.4903e-07, 8.0003e-07,  ..., 1.5618e-07, 2.5759e-07,\n",
            "         4.5322e-07],\n",
            "        [5.5563e-06, 7.7062e-06, 5.3184e-06,  ..., 9.2246e-07, 1.4328e-06,\n",
            "         1.8001e-06],\n",
            "        ...,\n",
            "        [5.7567e-07, 3.9496e-07, 2.5649e-07,  ..., 1.1341e-08, 1.7525e-10,\n",
            "         1.7730e-08],\n",
            "        [2.4015e-06, 2.7439e-06, 2.1505e-06,  ..., 5.6164e-07, 1.1423e-06,\n",
            "         1.8565e-06],\n",
            "        [2.8211e-06, 1.7736e-06, 2.0173e-06,  ..., 5.2122e-07, 1.9425e-07,\n",
            "         1.3854e-06]])}, 17: {'step': 8100, 'exp_avg': tensor([-1.0385e-03,  5.9058e-04, -1.2449e-03,  7.9391e-04, -1.8822e-04,\n",
            "        -9.6873e-04,  8.5146e-04, -5.6909e-04, -9.6366e-04, -3.8340e-04,\n",
            "        -1.0602e-03,  2.2224e-04,  9.5568e-04, -2.1381e-04,  5.1112e-05,\n",
            "        -8.6201e-04, -1.1500e-03, -1.0971e-03,  4.1176e-04,  4.9359e-06,\n",
            "        -9.8189e-05,  9.0425e-04,  1.0382e-03, -5.3765e-04, -5.1987e-04,\n",
            "         6.3761e-04, -2.9972e-04, -5.3947e-04, -5.0801e-04, -3.9737e-05,\n",
            "         6.3455e-04, -1.3861e-04, -1.4172e-03,  5.3897e-05, -2.0060e-04,\n",
            "         6.3135e-04,  5.1727e-04, -8.0025e-04, -9.7672e-04,  4.7123e-04,\n",
            "        -4.7079e-04, -8.9871e-04, -1.2011e-03,  6.1531e-04,  9.0685e-04,\n",
            "        -9.9990e-04,  1.5009e-05, -6.3972e-04,  4.4559e-04,  2.4884e-05,\n",
            "        -4.0781e-04,  5.7689e-04,  1.0099e-03, -1.0911e-03,  1.6302e-04,\n",
            "         5.6411e-04,  8.7330e-04,  5.4903e-04, -9.7333e-04, -3.3704e-04,\n",
            "         8.3308e-04,  9.2035e-05,  3.0969e-04,  1.1642e-04,  4.1872e-04,\n",
            "        -1.2226e-03, -8.5237e-04, -7.7500e-04,  7.1258e-04, -1.4873e-03,\n",
            "        -5.8027e-04,  1.0057e-04, -3.4590e-04,  8.1212e-04,  7.6969e-04,\n",
            "         2.7738e-04, -4.8183e-04, -9.4223e-04, -1.1548e-03, -6.3784e-04,\n",
            "        -9.8104e-04,  3.3830e-04, -6.8493e-04,  3.3873e-04, -2.6141e-04,\n",
            "        -3.2759e-04, -5.9599e-04,  6.0822e-07,  3.9330e-04, -5.0285e-04,\n",
            "        -7.9543e-04, -9.0069e-04, -5.0953e-04, -4.8141e-04,  8.8944e-04,\n",
            "        -2.2231e-04, -5.2838e-04, -2.1481e-04, -1.1865e-04, -6.5829e-04,\n",
            "        -1.3045e-03,  6.3730e-04, -2.3718e-04,  2.4496e-04,  5.1636e-04,\n",
            "         8.4147e-04,  9.8981e-04,  9.9363e-05,  4.5797e-04,  5.6258e-04,\n",
            "        -3.3737e-04,  1.0458e-03,  5.9135e-04, -6.1956e-04,  7.0424e-04,\n",
            "         8.0617e-04,  5.7152e-04,  2.9440e-04, -6.3746e-04, -5.7061e-04,\n",
            "         1.7331e-05, -8.7777e-04, -1.0061e-03,  6.1675e-04,  1.1205e-04,\n",
            "         2.1818e-04, -4.6200e-04,  3.5239e-04]), 'exp_avg_sq': tensor([3.2541e-06, 1.1474e-06, 6.6700e-06, 5.0523e-06, 8.8998e-08, 5.4796e-06,\n",
            "        4.3032e-06, 5.6292e-07, 5.2913e-06, 1.1324e-06, 3.4827e-06, 1.8449e-07,\n",
            "        3.7671e-06, 6.9242e-07, 1.4696e-06, 3.3252e-06, 2.2145e-06, 4.8383e-06,\n",
            "        2.4914e-06, 2.9188e-10, 6.3204e-07, 2.3671e-06, 3.3016e-06, 3.1707e-06,\n",
            "        2.7733e-06, 2.5276e-06, 6.2907e-07, 2.2254e-06, 5.5025e-06, 1.2211e-06,\n",
            "        3.1417e-06, 2.2604e-06, 6.7986e-06, 3.1722e-06, 2.1773e-06, 1.6821e-06,\n",
            "        5.7360e-06, 4.5623e-06, 2.2058e-06, 2.0033e-06, 5.0031e-06, 4.8074e-06,\n",
            "        3.4338e-06, 3.7425e-06, 4.4128e-06, 5.6214e-06, 2.5740e-09, 2.2884e-06,\n",
            "        4.9690e-07, 2.5650e-09, 3.3317e-06, 2.5095e-06, 3.3033e-06, 3.7236e-06,\n",
            "        2.2893e-06, 5.9710e-06, 1.8160e-06, 2.2231e-06, 2.2595e-06, 2.2875e-06,\n",
            "        3.3657e-06, 2.6045e-08, 6.7677e-06, 3.6499e-08, 2.6745e-06, 5.4704e-06,\n",
            "        2.2991e-06, 4.1017e-06, 3.5921e-06, 4.7968e-06, 2.3606e-06, 1.4455e-07,\n",
            "        1.1846e-06, 2.1563e-06, 2.0480e-06, 3.2672e-07, 1.6892e-06, 3.3526e-06,\n",
            "        4.8215e-06, 2.3013e-06, 3.6803e-06, 9.4193e-07, 3.3408e-06, 1.0498e-06,\n",
            "        1.6107e-06, 1.6855e-06, 2.2665e-06, 4.8066e-10, 1.3504e-06, 3.3915e-06,\n",
            "        2.3951e-06, 1.7706e-06, 3.8043e-06, 2.4844e-06, 2.3730e-06, 3.9315e-06,\n",
            "        2.2744e-06, 1.7066e-07, 6.2128e-06, 3.3923e-06, 5.3623e-06, 2.0539e-06,\n",
            "        3.2325e-07, 3.9114e-07, 1.2058e-06, 3.3823e-06, 3.6924e-06, 4.3828e-08,\n",
            "        1.2354e-06, 3.1653e-06, 4.4877e-06, 4.1031e-06, 3.1131e-06, 3.5886e-06,\n",
            "        1.6619e-06, 2.8877e-06, 3.1893e-06, 5.0526e-07, 2.2316e-06, 1.6141e-06,\n",
            "        3.8095e-09, 4.1335e-06, 2.1960e-06, 5.8297e-06, 5.6879e-08, 7.9505e-07,\n",
            "        2.4450e-06, 3.5984e-06])}, 18: {'step': 8100, 'exp_avg': tensor([[-2.6823e-03, -2.4121e-03, -1.4114e-03, -1.0691e-03, -2.5303e-03,\n",
            "         -1.3390e-03,  4.6507e-05, -1.6703e-03, -1.6330e-03, -1.5865e-03,\n",
            "         -2.7369e-03, -8.0121e-03, -4.9646e-04, -9.6986e-04,  9.9521e-05,\n",
            "         -1.6795e-03, -1.6850e-03, -9.7342e-04,  6.7509e-04, -1.5616e-02,\n",
            "         -3.3137e-04,  1.6717e-04, -1.7348e-03, -1.1321e-03, -1.7117e-03,\n",
            "         -3.7760e-04, -2.4827e-03, -1.1998e-03, -5.4646e-04, -5.0429e-04,\n",
            "         -9.4860e-04, -5.5326e-04, -2.0236e-03,  7.2669e-04, -9.6741e-04,\n",
            "         -1.8463e-03, -1.6113e-04, -1.6901e-03, -1.4938e-03, -8.0686e-04,\n",
            "         -1.5790e-03, -1.0700e-03, -2.0010e-03, -7.7624e-04, -1.3426e-03,\n",
            "         -1.1836e-03, -1.5189e-02, -1.3662e-03, -5.2331e-03, -2.9648e-02,\n",
            "         -7.3007e-04, -5.4134e-04, -2.8670e-04, -1.1007e-03,  3.0347e-04,\n",
            "         -1.3223e-03, -1.2206e-03, -3.5954e-04, -1.7661e-03, -6.8607e-04,\n",
            "         -5.4232e-04, -1.7566e-02,  7.7508e-05, -1.2322e-02,  3.6332e-04,\n",
            "         -2.0541e-03, -2.4850e-03, -8.9597e-04, -1.3583e-04, -2.3248e-03,\n",
            "         -9.6214e-04, -6.0536e-03, -1.5173e-03, -7.9898e-04, -2.4928e-03,\n",
            "         -5.0733e-03, -1.4773e-03, -2.1652e-03, -1.7377e-03, -1.2200e-03,\n",
            "         -1.4677e-03, -1.6851e-03, -1.3853e-03, -1.6179e-03, -4.7724e-04,\n",
            "         -1.4639e-03, -1.5981e-03, -2.5231e-02, -1.9356e-03, -7.6004e-04,\n",
            "         -3.0523e-03, -2.7970e-03, -5.3427e-04, -1.0677e-03, -4.6136e-04,\n",
            "         -5.7257e-04, -1.4846e-03, -3.0343e-03,  5.5803e-04, -1.9778e-03,\n",
            "         -1.2911e-03, -8.1967e-04, -2.5150e-03, -1.7415e-03, -2.1235e-03,\n",
            "         -1.6461e-05, -1.7538e-03, -1.1165e-02, -2.2228e-03, -6.8320e-04,\n",
            "         -9.0914e-05, -1.0013e-03, -4.8394e-04, -1.4673e-03, -1.4719e-03,\n",
            "         -1.8257e-03, -1.9570e-04, -3.4091e-03, -1.3842e-03, -1.0558e-03,\n",
            "         -1.1741e-02, -6.3933e-04, -2.0566e-03, -6.1255e-04, -9.0009e-03,\n",
            "          8.7064e-05, -7.0846e-04,  1.6472e-04],\n",
            "        [ 2.6971e-03,  2.3151e-03,  1.4058e-03,  1.0525e-03,  2.5234e-03,\n",
            "          1.3043e-03, -5.6491e-05,  1.6397e-03,  1.6120e-03,  1.5915e-03,\n",
            "          2.7291e-03,  7.9146e-03,  4.9348e-04,  9.6556e-04, -1.0913e-04,\n",
            "          1.6806e-03,  1.6797e-03,  9.7040e-04, -6.9866e-04,  1.5364e-02,\n",
            "          3.2911e-04, -1.9176e-04,  1.6866e-03,  1.1192e-03,  1.6969e-03,\n",
            "          2.7403e-04,  2.4704e-03,  1.1957e-03,  5.0156e-04,  5.0140e-04,\n",
            "          8.7128e-04,  5.1987e-04,  2.0186e-03, -7.6329e-04,  9.6512e-04,\n",
            "          1.7974e-03,  1.6108e-04,  1.6921e-03,  1.4904e-03,  7.5749e-04,\n",
            "          1.5765e-03,  1.0422e-03,  2.0074e-03,  6.8952e-04,  1.2611e-03,\n",
            "          1.1545e-03,  1.4816e-02,  1.3578e-03,  5.1327e-03,  2.9351e-02,\n",
            "          6.9965e-04,  5.0841e-04,  1.9932e-04,  1.0988e-03, -3.9605e-04,\n",
            "          1.2883e-03,  1.2217e-03,  2.9097e-04,  1.7414e-03,  6.8403e-04,\n",
            "          4.8692e-04,  1.7236e-02, -1.4146e-04,  1.2050e-02, -3.7468e-04,\n",
            "          2.0583e-03,  2.4790e-03,  8.9210e-04,  1.2387e-04,  2.3332e-03,\n",
            "          9.5758e-04,  5.8430e-03,  1.4830e-03,  7.9892e-04,  2.4211e-03,\n",
            "          4.9182e-03,  1.4662e-03,  2.0928e-03,  1.7492e-03,  1.2204e-03,\n",
            "          1.4611e-03,  1.6292e-03,  1.3796e-03,  1.5437e-03,  4.7600e-04,\n",
            "          1.4452e-03,  1.5927e-03,  2.4904e-02,  1.8284e-03,  7.6529e-04,\n",
            "          3.0252e-03,  2.7590e-03,  5.1088e-04,  1.0398e-03,  4.1492e-04,\n",
            "          5.4713e-04,  1.4748e-03,  2.9823e-03, -5.7714e-04,  1.9521e-03,\n",
            "          1.2547e-03,  7.4721e-04,  2.4930e-03,  1.6170e-03,  2.0522e-03,\n",
            "          1.1194e-07,  1.7158e-03,  1.1065e-02,  2.1460e-03,  6.3838e-04,\n",
            "          6.9150e-05,  9.6235e-04,  4.2638e-04,  1.4408e-03,  1.3824e-03,\n",
            "          1.7369e-03,  1.3058e-04,  3.3549e-03,  1.3709e-03,  1.0504e-03,\n",
            "          1.1546e-02,  6.4174e-04,  2.0322e-03,  5.3462e-04,  8.7852e-03,\n",
            "         -9.2258e-05,  7.0181e-04, -2.4492e-04]]), 'exp_avg_sq': tensor([[1.5948e-05, 3.3769e-05, 9.1775e-06, 8.5041e-06, 2.7240e-05, 9.9103e-06,\n",
            "         5.3198e-06, 1.9614e-05, 8.0078e-06, 8.0528e-06, 1.4881e-05, 3.0284e-04,\n",
            "         8.9341e-06, 7.3738e-06, 9.0455e-06, 9.3923e-06, 9.2126e-06, 9.4271e-06,\n",
            "         1.0920e-05, 1.0145e-03, 5.8808e-06, 5.6300e-06, 1.5314e-05, 6.9732e-06,\n",
            "         8.1901e-06, 1.4434e-05, 2.2695e-05, 7.8387e-06, 1.6880e-05, 5.7512e-06,\n",
            "         2.4821e-05, 9.7843e-06, 9.7923e-06, 5.7209e-06, 1.0507e-05, 3.5543e-05,\n",
            "         6.5783e-06, 8.6845e-06, 7.5027e-06, 2.7447e-05, 5.1457e-06, 8.3742e-06,\n",
            "         1.1879e-05, 1.9494e-05, 1.5830e-05, 1.1779e-05, 2.7039e-03, 5.3292e-06,\n",
            "         8.9612e-05, 2.1266e-03, 1.5577e-05, 1.3181e-05, 1.2043e-05, 6.0594e-06,\n",
            "         2.9467e-05, 2.5703e-05, 9.6849e-06, 1.2563e-05, 7.0011e-06, 5.2366e-06,\n",
            "         1.4054e-05, 1.1003e-03, 1.3593e-05, 5.6246e-04, 1.0476e-05, 8.7140e-06,\n",
            "         1.3574e-05, 1.2492e-05, 6.8776e-06, 1.1251e-05, 9.1345e-06, 3.9939e-04,\n",
            "         1.3319e-05, 1.0034e-05, 3.8066e-05, 1.5709e-04, 1.3093e-05, 1.1036e-05,\n",
            "         1.8960e-05, 5.6157e-06, 1.9572e-05, 2.8164e-05, 6.9423e-06, 5.9544e-05,\n",
            "         1.0137e-05, 1.4021e-05, 7.4428e-06, 2.0214e-03, 3.2252e-05, 1.0696e-05,\n",
            "         2.2672e-05, 2.2589e-05, 6.5162e-06, 9.5391e-06, 1.4165e-05, 1.2085e-05,\n",
            "         1.5150e-05, 5.7448e-05, 8.2884e-06, 1.3357e-05, 1.8331e-05, 1.6833e-05,\n",
            "         4.4488e-05, 8.9294e-05, 5.3082e-05, 4.0231e-06, 2.4562e-05, 6.4222e-04,\n",
            "         4.3254e-05, 8.1840e-06, 1.3680e-05, 1.3969e-05, 2.0465e-05, 1.0711e-05,\n",
            "         3.0911e-05, 2.3564e-05, 1.5621e-05, 7.9889e-05, 1.2350e-05, 9.1510e-06,\n",
            "         1.7979e-03, 1.2059e-05, 1.3603e-05, 1.5056e-05, 4.7486e-04, 2.4269e-06,\n",
            "         9.8628e-06, 1.7256e-05],\n",
            "        [1.5943e-05, 3.3878e-05, 9.1275e-06, 8.5537e-06, 2.7219e-05, 9.8564e-06,\n",
            "         5.3414e-06, 1.9577e-05, 7.9694e-06, 8.0439e-06, 1.4858e-05, 3.0334e-04,\n",
            "         8.9586e-06, 7.3660e-06, 9.0423e-06, 9.3684e-06, 9.1793e-06, 9.4290e-06,\n",
            "         1.0986e-05, 1.0156e-03, 5.8818e-06, 5.6700e-06, 1.5346e-05, 6.9496e-06,\n",
            "         8.1639e-06, 1.4531e-05, 2.2643e-05, 7.8114e-06, 1.6849e-05, 5.7461e-06,\n",
            "         2.4912e-05, 9.7858e-06, 9.7517e-06, 5.7422e-06, 1.0495e-05, 3.5623e-05,\n",
            "         6.5939e-06, 8.6429e-06, 7.4775e-06, 2.7575e-05, 5.1384e-06, 8.3640e-06,\n",
            "         1.1870e-05, 1.9542e-05, 1.5886e-05, 1.1737e-05, 2.7060e-03, 5.3244e-06,\n",
            "         8.9821e-05, 2.1277e-03, 1.5550e-05, 1.3226e-05, 1.2123e-05, 6.0421e-06,\n",
            "         2.9632e-05, 2.5788e-05, 9.6985e-06, 1.2644e-05, 6.9800e-06, 5.2129e-06,\n",
            "         1.4120e-05, 1.1015e-03, 1.3685e-05, 5.6317e-04, 1.0489e-05, 8.6882e-06,\n",
            "         1.3550e-05, 1.2473e-05, 6.9035e-06, 1.1213e-05, 9.1273e-06, 3.9980e-04,\n",
            "         1.3293e-05, 1.0051e-05, 3.8284e-05, 1.5734e-04, 1.3092e-05, 1.0994e-05,\n",
            "         1.8949e-05, 5.6052e-06, 1.9567e-05, 2.8226e-05, 6.9292e-06, 5.9651e-05,\n",
            "         1.0141e-05, 1.4013e-05, 7.4102e-06, 2.0221e-03, 3.2349e-05, 1.0655e-05,\n",
            "         2.2641e-05, 2.2535e-05, 6.4840e-06, 9.5147e-06, 1.4216e-05, 1.2033e-05,\n",
            "         1.5135e-05, 5.7461e-05, 8.2618e-06, 1.3339e-05, 1.8306e-05, 1.6900e-05,\n",
            "         4.4541e-05, 8.9452e-05, 5.3264e-05, 4.0445e-06, 2.4589e-05, 6.4271e-04,\n",
            "         4.3406e-05, 8.2187e-06, 1.3662e-05, 1.4034e-05, 2.0564e-05, 1.0689e-05,\n",
            "         3.1058e-05, 2.3637e-05, 1.5697e-05, 8.0124e-05, 1.2321e-05, 9.1366e-06,\n",
            "         1.7984e-03, 1.2015e-05, 1.3551e-05, 1.5167e-05, 4.7520e-04, 2.4271e-06,\n",
            "         9.8537e-06, 1.7353e-05]])}}\n",
            "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]}]\n"
          ]
        }
      ],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in net.state_dict():\n",
        "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating data independence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def calculate_identity(query,database):\n",
        "#     if len(query) == len(database):\n",
        "#         return np.sum([1 for i in range(len(query)) if query[i] == database[i]])/len(query)\n",
        "    \n",
        "#     elif len(query) > len(database):\n",
        "#         max_score = 0\n",
        "#         diff_len = len(query) - len(database)\n",
        "#         for i in range(diff_len + 1):\n",
        "#             query_shortened = query[i:i+len(database)]\n",
        "#             score = np.sum([1 for i in range(len(query_shortened)) if query_shortened[i] == database[i]])/len(query_shortened)\n",
        "#             if score > max_score:\n",
        "#                 max_score = score\n",
        "\n",
        "#         return max_score\n",
        "    \n",
        "#     elif len(query) < len(database):\n",
        "#         max_score = 0\n",
        "#         diff_len = len(database) - len(query)\n",
        "#         for i in range(diff_len + 1):\n",
        "#             database_shortened = database[i:i+len(query)]\n",
        "#             score = np.sum([1 for i in range(len(query)) if query[i] == database_shortened[i]])/len(database_shortened)\n",
        "#             if score > max_score:\n",
        "#                 max_score = score\n",
        "\n",
        "#         return max_score\n",
        "        \n",
        "# identity = [[calculate_identity(pep1,pep2) for pep1 in training_df[\"peptide\"]] for pep2 in validation_df[\"peptide\"]]\n",
        "# peptides = [[(pep1,pep2) for pep1 in training_df[\"peptide\"]] for pep2 in validation_df[\"peptide\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identity = np.array(identity)\n",
        "# peptides = np.array(peptides)\n",
        "# max_identity = np.max(identity)\n",
        "# max_row_index,max_column_index = np.where(identity == np.amax(identity))\n",
        "# print(peptides[max_row_index[0],max_column_index[0]])\n",
        "# print(calculate_identity(\"FLPSDFFPSV\",\"FLPPDFFPSV\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_peptides_train = []\n",
        "# all_HLA_train = []\n",
        "\n",
        "# for i in range(len((peptide_train_loader))):\n",
        "#     train_peptides = peptide_train_loader[i].reshape(len(peptide_train_loader[i]),-1).tolist()\n",
        "#     all_peptides_train += train_peptides\n",
        "#     train_HLA = HLA_train_loader[i].reshape(len(HLA_train_loader[i]),-1).tolist()\n",
        "#     all_HLA_train += train_HLA\n",
        "\n",
        "\n",
        "# all_peptides_val = []\n",
        "# all_HLA_val = []\n",
        "# for j in range(len((peptide_val_loader))):\n",
        "#     val_peptides = peptide_val_loader[j].reshape(len(peptide_val_loader[j]),-1).tolist()\n",
        "#     all_peptides_val += val_peptides\n",
        "#     val_HLA = HLA_val_loader[j].reshape(len(HLA_val_loader[j]),-1).tolist()\n",
        "#     all_HLA_val += val_HLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(all_peptides_train[0] + all_HLA_train[0])\n",
        "# a = np.array(all_peptides_train[0] + all_HLA_train[0])\n",
        "# b =  np.array(all_peptides_train[1] + all_HLA_train[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix_HLA_pept_dist = [[0 for _ in range(len(all_HLA_val))] for _ in range(len(all_peptides_train))]\n",
        "# matrix_HLA_pept_dist = np.array(matrix_HLA_pept_dist)\n",
        "\n",
        "# for i in range(len(all_peptides_train)):\n",
        "#     if i % 500 == 0:\n",
        "#         print(i)\n",
        "#     for j in range(len(all_peptides_val)):\n",
        "#         train_obs = np.array(all_peptides_train[i] + all_HLA_train[i])\n",
        "#         val_obs = np.array(all_peptides_val[j] + all_HLA_val[j])\n",
        "#         dist = np.linalg.norm(train_obs-val_obs)\n",
        "#         matrix_HLA_pept_dist[i,j] = dist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.min(matrix_HLA_pept_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix_pept_dist = [[0 for _ in range(len(all_HLA_val))] for _ in range(len(all_peptides_train))]\n",
        "# matrix_pept_dist = np.array(matrix_pept_dist)\n",
        "\n",
        "# for i in range(len(all_peptides_train)):\n",
        "#     if i % 500 == 0:\n",
        "#         print(i)\n",
        "#     for j in range(len(all_peptides_val)):\n",
        "#         train_obs = np.array(all_peptides_train[i])\n",
        "#         val_obs = np.array(all_peptides_val[j])\n",
        "#         dist = np.linalg.norm(train_obs-val_obs)\n",
        "#         matrix_pept_dist[i,j] = dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.min(matrix_pept_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reproduce_deep_immuno_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
