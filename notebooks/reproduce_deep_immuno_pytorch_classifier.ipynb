{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3DtMe3EbDK9N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc,precision_recall_curve,roc_curve,confusion_matrix\n",
        "import os,sys\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import seaborn as sns\n",
        "np.random.seed(10)\n",
        "random.seed(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81oG3LWzEh2k"
      },
      "source": [
        "## His functions used to read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w2TYnX5yEbcL"
      },
      "outputs": [],
      "source": [
        "def aaindex(peptide,after_pca):\n",
        "\n",
        "    amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
        "    matrix = np.transpose(after_pca)   # [12,21]\n",
        "    encoded = np.empty([len(peptide), 12])  # (seq_len,12)\n",
        "    for i in range(len(peptide)):\n",
        "        query = peptide[i]\n",
        "        if query == 'X': query = '-'\n",
        "        query = query.upper()\n",
        "        encoded[i, :] = matrix[:, amino.index(query)]\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def rescue_unknown_hla(hla, dic_inventory):\n",
        "    type_ = hla[4]\n",
        "    first2 = hla[6:8]\n",
        "    last2 = hla[8:]\n",
        "    big_category = dic_inventory[type_]\n",
        "    if not big_category.get(first2) == None:\n",
        "        small_category = big_category.get(first2)\n",
        "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
        "    else:\n",
        "        small_category = list(big_category.keys())\n",
        "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
        "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
        "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])\n",
        "\n",
        "def hla_df_to_dic(hla):\n",
        "    dic = {}\n",
        "    for i in range(hla.shape[0]):\n",
        "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
        "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
        "        dic[col1] = col2\n",
        "    return dic\n",
        "\n",
        "def peptide_data_aaindex(peptide,after_pca):   # return numpy array [10,12,1]\n",
        "    length = len(peptide)\n",
        "    if length == 10:\n",
        "        encode = aaindex(peptide,after_pca)\n",
        "    elif length == 9:\n",
        "        peptide = peptide[:5] + '-' + peptide[5:]\n",
        "        encode = aaindex(peptide,after_pca)\n",
        "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
        "    return encode\n",
        "\n",
        "\n",
        "def hla_data_aaindex(hla_dic,hla_type,after_pca):    # return numpy array [34,12,1]\n",
        "    try:\n",
        "        seq = hla_dic[hla_type]\n",
        "    except KeyError:\n",
        "        hla_type = rescue_unknown_hla(hla_type,dic_inventory)\n",
        "        seq = hla_dic[hla_type]\n",
        "    encode = aaindex(seq,after_pca)\n",
        "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
        "    return encode\n",
        "\n",
        "def dict_inventory(inventory):\n",
        "    dicA, dicB, dicC = {}, {}, {}\n",
        "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
        "\n",
        "    for hla in inventory:\n",
        "        type_ = hla[4]  # A,B,C\n",
        "        first2 = hla[6:8]  # 01\n",
        "        last2 = hla[8:]  # 01\n",
        "        try:\n",
        "            dic[type_][first2].append(last2)\n",
        "        except KeyError:\n",
        "            dic[type_][first2] = []\n",
        "            dic[type_][first2].append(last2)\n",
        "\n",
        "    return dic\n",
        "\n",
        "def construct_aaindex(ori,hla_dic,after_pca):\n",
        "    series = []\n",
        "    for i in range(ori.shape[0]):\n",
        "        peptide = ori['peptide'].iloc[i]\n",
        "        hla_type = ori['HLA'].iloc[i]\n",
        "        immuno = np.array(ori['immunogenicity'].iloc[i]).reshape(1,-1)   # [1,1]\n",
        "\n",
        "        encode_pep = peptide_data_aaindex(peptide,after_pca)    # [10,12]\n",
        "\n",
        "        encode_hla = hla_data_aaindex(hla_dic,hla_type,after_pca)   # [46,12]\n",
        "        series.append((encode_pep, encode_hla, immuno))\n",
        "    return series\n",
        "\n",
        "def pull_peptide_aaindex(dataset):\n",
        "    result = np.empty([len(dataset),10,12,1])\n",
        "    for i in range(len(dataset)):\n",
        "        result[i,:,:,:] = dataset[i][0]\n",
        "    return result\n",
        "\n",
        "def pull_hla_aaindex(dataset):\n",
        "    result = np.empty([len(dataset),46,12,1])\n",
        "    for i in range(len(dataset)):\n",
        "        result[i,:,:,:] = dataset[i][1]\n",
        "    return result\n",
        "\n",
        "\n",
        "def pull_label_aaindex(dataset):\n",
        "    col = [item[2] for item in dataset]\n",
        "    result = [0 if item == 'Negative' else 1 for item in col]\n",
        "    result = np.expand_dims(np.array(result),axis=1)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_training_and_validataion_dataset(path_to_partitions,train_splits):\n",
        "    import random\n",
        "    # TRAINING\n",
        "    # training_partions = random.sample(range(10),train_splits)\n",
        "    training_partions = [9, 0, 6, 3, 4, 8, 1, 7]\n",
        "\n",
        "    #VALIDATION\n",
        "    validation_partions = [i for i in range(10) if i not in training_partions]\n",
        "\n",
        "    # path_to_partitions = \"../data/partitions\"\n",
        "    partitions = []\n",
        "    for file in os.listdir(path_to_partitions):\n",
        "        path_to_file = os.path.join(path_to_partitions,file)\n",
        "        data = pd.read_csv(path_to_file,sep=\"\\t\",names=[\"peptide\",\"label\",\"HLA\"])\n",
        "        partitions.append(data)\n",
        "    training_df = pd.concat([partitions[i] for i in training_partions])\n",
        "    validation_df = pd.concat([partitions[i] for i in validation_partions])\n",
        "    return training_df, validation_df,training_partions,validation_partions\n",
        "\n",
        "def retrieve_information_from_df(data_split,entire_df):\n",
        "    potential = []\n",
        "    immunogenicity = []\n",
        "    tested = []\n",
        "    responded = []\n",
        "    for i,row in data_split.iterrows():\n",
        "        peptide, HLA = row[\"peptide\"], row['HLA']\n",
        "        original_entry = entire_df[(entire_df['peptide']==peptide) & (entire_df['HLA'] == HLA)]\n",
        "        assert len(original_entry) == 1\n",
        "        potential.append(float(original_entry['potential']))\n",
        "        immunogenicity.append(original_entry['immunogenicity'].values[0])\n",
        "        tested.append(int(original_entry['test']))\n",
        "        responded.append(int(original_entry['respond']))\n",
        "     \n",
        "    data_split['potential'] = potential\n",
        "    data_split['immunogenicity'] = immunogenicity\n",
        "    data_split['test'] = tested\n",
        "    data_split['respond'] = responded\n",
        "\n",
        "    return data_split  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypa7oEJ5FTBd"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The strategy used by the paper to load the data (Incorrect because 100% identity between validation and validation data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TwmfzUJpEX_C"
      },
      "outputs": [],
      "source": [
        "# # Loading the dataset\n",
        "# after_pca = np.loadtxt('../DeepImmuno//reproduce/data/after_pca.txt')\n",
        "# ori = pd.read_csv('../DeepImmuno//reproduce/data/remove0123_sample100.csv')\n",
        "# ori = ori.sample(frac=1, replace=False).set_index(pd.Index(np.arange(ori.shape[0])))\n",
        "# hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "# hla_dic = hla_df_to_dic(hla)\n",
        "# inventory = list(hla_dic.keys())\n",
        "# dic_inventory = dict_inventory(inventory)\n",
        "# dataset = construct_aaindex(ori, hla_dic, after_pca)\n",
        "# input1 = pull_peptide_aaindex(dataset)\n",
        "# input2 = pull_hla_aaindex(dataset)\n",
        "# label = pull_label_aaindex(dataset)\n",
        "\n",
        "# input1 = input1.astype('float32')\n",
        "# input2 = input2.astype('float32')\n",
        "# label = label.astype('float32')\n",
        "\n",
        "# array = np.arange(len(dataset))\n",
        "# train_index = np.random.choice(array,int(len(dataset)*0.9),replace=False)\n",
        "# valid_index = [item for item in array if item not in train_index]\n",
        "\n",
        "# input1 = input1.astype('float32')\n",
        "# input2 = input2.astype('float32')\n",
        "# label = label.astype('float32')\n",
        "\n",
        "# # Reshaped data\n",
        "# input1 = pull_peptide_aaindex(dataset).reshape(-1,1,10,12)\n",
        "# input2 = pull_hla_aaindex(dataset).reshape(-1,1,46,12)\n",
        "# label = pull_label_aaindex(dataset)\n",
        "\n",
        "\n",
        "# peptide_train, peptide_val = input1[train_index], input1[valid_index]\n",
        "# HLA_train, HLA_val = input2[train_index], input2[valid_index]\n",
        "# label_train, label_val = label[train_index], label[valid_index]\n",
        "\n",
        "\n",
        "# peptide_train = peptide_train.reshape(-1,1,10,12).astype('float32')\n",
        "# HLA_train = HLA_train.reshape(-1,1,46,12).astype('float32')\n",
        "# label_train = label_train.astype('float32')\n",
        "\n",
        "# peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
        "# HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
        "# label_val = label_val.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Correct partioning of the data (The data is partioned such that the validation and )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "after_pca = np.loadtxt('../DeepImmuno/reproduce/data/after_pca.txt')\n",
        "hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
        "hla_dic = hla_df_to_dic(hla)\n",
        "inventory = list(hla_dic.keys())\n",
        "dic_inventory = dict_inventory(inventory)\n",
        "\n",
        "entire_df = pd.read_csv('../data/deep_immuno_2.csv')\n",
        "# Allocating the partitions of the trainign and validation data\n",
        "training_df, validation_df,training_partions,validation_partions = load_training_and_validataion_dataset(path_to_partitions=\"../data/deepimmuno_parts\",train_splits=8)\n",
        "\n",
        "# Creating the training dataframe (With correct information such as tested and positive subjects aswell as label)\n",
        "training_df_entire = retrieve_information_from_df(training_df,entire_df)\n",
        "# Shuffling the dataframe\n",
        "training_df_entire = training_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "# Creating the validation dataframe (With correct information such as tested and positive subjects aswell as label)\n",
        "validation_df_entire = retrieve_information_from_df(validation_df,entire_df)\n",
        "# Shuffling the dataframe\n",
        "validation_df_entire = validation_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "training_dataset_encoded = construct_aaindex(training_df_entire, hla_dic, after_pca)\n",
        "peptide_train = pull_peptide_aaindex(training_dataset_encoded)\n",
        "HLA_train = pull_hla_aaindex(training_dataset_encoded)\n",
        "label_train = pull_label_aaindex(training_dataset_encoded)\n",
        "\n",
        "\n",
        "val_dataset_encoded = construct_aaindex(validation_df_entire, hla_dic, after_pca)\n",
        "peptide_val = pull_peptide_aaindex(val_dataset_encoded)\n",
        "HLA_val = pull_hla_aaindex(val_dataset_encoded)\n",
        "label_val = pull_label_aaindex(val_dataset_encoded)\n",
        "\n",
        "peptide_train = peptide_train.reshape(-1,1,10,12).astype('float32')\n",
        "HLA_train = HLA_train.reshape(-1,1,46,12).astype('float32')\n",
        "label_train = label_train.astype('float32')\n",
        "\n",
        "peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
        "HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
        "label_val = label_val.astype('float32')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHFPYHuPKX42"
      },
      "source": [
        "## Definning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QTz5JsT9KXVt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQ7TqSzPeg7",
        "outputId": "8a7b6150-31f4-4d09-b8fb-fa011d6ff0e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_conv_dim(dim_size,kernel_size,padding,stride):\n",
        "    return int((dim_size - kernel_size + 2 * padding) / stride + 1)\n",
        "compute_conv_dim(10,2,0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnYzGji8Ju9t",
        "outputId": "ad092cd1-f4fc-4a7a-a7f1-0d13231ec3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1_peptide): Conv2d(1, 16, kernel_size=(2, 12), stride=(1, 1))\n",
            "  (BatchNorm_conv1_peptides): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (conv2_peptide): Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1))\n",
            "  (BatchNorm_conv2_peptides): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool1_peptide): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv1_HLA): Conv2d(1, 16, kernel_size=(15, 12), stride=(1, 1))\n",
            "  (BatchNorm_conv1_HLA): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool1_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2_HLA): Conv2d(16, 32, kernel_size=(9, 1), stride=(1, 1))\n",
            "  (BatchNorm_conv2_HLA): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "  (maxpool2_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
            "  (L_in): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (drop_out): Dropout(p=0.2, inplace=False)\n",
            "  (L_out): Linear(in_features=128, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# hyperameters of the model\n",
        "peptide_input_channels = peptide_train.shape[1]\n",
        "peptide_input_height = peptide_train.shape[2]\n",
        "peptide_input_width = peptide_train.shape[3]\n",
        "\n",
        "hla_input_channels = HLA_train.shape[1]\n",
        "hla_input_height = HLA_train.shape[2]\n",
        "hla_input_width = HLA_train.shape[3]\n",
        "\n",
        "# define network\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Convelution of peptide\n",
        "        self.conv1_peptide = Conv2d(in_channels=peptide_input_channels,\n",
        "                            out_channels=16,\n",
        "                            kernel_size=(2,12),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        \n",
        "        self.BatchNorm_conv1_peptides = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.conv2_peptide = Conv2d(in_channels=16,\n",
        "                            out_channels=32,\n",
        "                            kernel_size=(2,1),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool1_peptide = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "\n",
        "        # Convelution of HLA\n",
        "        self.conv1_HLA = Conv2d(in_channels=peptide_input_channels,\n",
        "                            out_channels=16,\n",
        "                            kernel_size=(15,12),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv1_HLA = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool1_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "        \n",
        "        self.conv2_HLA = Conv2d(in_channels=16,\n",
        "                            out_channels=32,\n",
        "                            kernel_size=(9,1),\n",
        "                            stride=1,\n",
        "                            padding=0)\n",
        "        self.BatchNorm_conv2_HLA = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
        "        self.maxpool2_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
        "\n",
        "        # Denselayer\n",
        "        self.L_in = Linear(in_features=256,\n",
        "                            out_features=128)\n",
        "        \n",
        "        self.drop_out = nn.Dropout(p=0.2)\n",
        "        self.L_out = Linear(in_features=128,\n",
        "                            out_features=2,\n",
        "                            bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, peptide, HLA): # x.size() = [batch, channel, height, width]\n",
        "\n",
        "        # Encoding the peptide\n",
        "        peptide = self.conv1_peptide(peptide)\n",
        "        # peptide = self.BatchNorm_conv1_peptides(peptide)\n",
        "        peptide = relu(peptide)\n",
        "        peptide = self.conv2_peptide(peptide)\n",
        "        peptide = self.BatchNorm_conv2_peptides(peptide)\n",
        "        peptide = relu(peptide)\n",
        "        peptide = self.maxpool1_peptide(peptide)\n",
        "        peptide = torch.flatten(peptide,start_dim=1)\n",
        "\n",
        "        # Encoding the HLA\n",
        "        HLA = self.conv1_HLA(HLA)\n",
        "        # HLA = self.BatchNorm_conv1_HLA(HLA)\n",
        "        HLA = relu(HLA)\n",
        "        HLA = self.maxpool1_HLA(HLA)\n",
        "        HLA = self.conv2_HLA(HLA)\n",
        "        HLA = self.BatchNorm_conv2_HLA(HLA)\n",
        "        HLA = relu(HLA)\n",
        "        HLA = self.maxpool2_HLA(HLA)\n",
        "        HLA = torch.flatten(HLA,start_dim=1)\n",
        "\n",
        "        # Combining the output\n",
        "        \n",
        "        combined_input = torch.cat((peptide, HLA), 1)\n",
        "        x = self.L_in(combined_input)\n",
        "        x = self.drop_out(x)\n",
        "        x = relu(x)\n",
        "        x = self.L_out(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        return softmax(x, dim=1)\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5_FHTyCAP25T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.4762, 0.5238],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.4883, 0.5117],\n",
              "        [0.4965, 0.5035],\n",
              "        [0.5195, 0.4805],\n",
              "        [0.5000, 0.5000],\n",
              "        [0.4299, 0.5701],\n",
              "        [0.5418, 0.4582],\n",
              "        [0.4938, 0.5062]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peptide_random = np.random.normal(0,1, (10, 1, 10, 12)).astype('float32')\n",
        "peptide_random = Variable(torch.from_numpy(peptide_random))\n",
        "HLA_random = np.random.normal(0,1, (10, 1, 46, 12)).astype('float32')\n",
        "HLA_random = Variable(torch.from_numpy(HLA_random))\n",
        "output = net(peptide_random,HLA_random)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq8IcnuJMs34"
      },
      "source": [
        "## Creating testing and validataion datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 100\n",
        "\n",
        "peptide_train_loader = list(DataLoader(peptide_train,batch_size=batch_size))\n",
        "HLA_train_loader = list(DataLoader(HLA_train,batch_size=batch_size))\n",
        "label_train_loader = list(DataLoader(label_train,batch_size=batch_size))\n",
        "\n",
        "peptide_val_loader = list(DataLoader(peptide_val,batch_size=batch_size))\n",
        "HLA_val_loader = list(DataLoader(HLA_val,batch_size=batch_size))\n",
        "label_val_loader = list(DataLoader(label_val,batch_size=batch_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjBK0OsZM0E2"
      },
      "source": [
        "## Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zTgk8hKTMzRs"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 : Train Loss 0.636681 , Train acc 0.731033, Valid acc 0.721058\n",
            "Epoch 11 : Train Loss 0.517321 , Train acc 0.789157, Valid acc 0.773980\n",
            "Epoch 21 : Train Loss 0.506667 , Train acc 0.800475, Valid acc 0.767365\n",
            "Epoch 31 : Train Loss 0.498919 , Train acc 0.813609, Valid acc 0.769019\n",
            "Epoch 41 : Train Loss 0.489340 , Train acc 0.822412, Valid acc 0.767916\n",
            "Epoch 51 : Train Loss 0.481958 , Train acc 0.835406, Valid acc 0.765160\n",
            "Epoch 61 : Train Loss 0.474933 , Train acc 0.845885, Valid acc 0.769019\n",
            "Epoch 71 : Train Loss 0.468030 , Train acc 0.852732, Valid acc 0.770121\n",
            "Epoch 81 : Train Loss 0.461844 , Train acc 0.863490, Valid acc 0.772326\n",
            "Epoch 91 : Train Loss 0.457300 , Train acc 0.871035, Valid acc 0.773429\n"
          ]
        }
      ],
      "source": [
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "losses = []\n",
        "all_val_targets_pr_epoch = []\n",
        "all_val_predictions_pr_epoch = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    net.train()\n",
        "    current_loss = 0\n",
        "    for train_batch_index in range(len((peptide_train_loader))):\n",
        "        train_peptides = peptide_train_loader[train_batch_index]\n",
        "        train_HLA = HLA_train_loader[train_batch_index]\n",
        "        train_labels = label_train_loader[train_batch_index].long().reshape(-1)\n",
        "    \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(train_peptides,train_HLA)\n",
        "        loss = criterion(outputs, train_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "\n",
        "    # print(epoch, current_loss/batch_size)\n",
        "    losses.append(current_loss/len((peptide_train_loader)))\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        all_train_targets = []\n",
        "        all_predicted_train_labels = []\n",
        "        for i in range(len((peptide_train_loader))):\n",
        "            train_peptides = peptide_train_loader[i]\n",
        "            train_HLA = HLA_train_loader[i]\n",
        "            train_labels = label_train_loader[i].long().reshape(-1)\n",
        "            outputs = net(train_peptides,train_HLA)\n",
        "            _,predicted_labels =  torch.max(outputs, 1)\n",
        "\n",
        "            all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
        "            all_train_targets += train_labels.numpy().tolist()\n",
        "        \n",
        "        all_val_targets = []\n",
        "        all_predicted_val_labels = []\n",
        "        for j in range(len((peptide_val_loader))):\n",
        "            val_peptides = peptide_val_loader[j]\n",
        "            val_HLA = HLA_val_loader[j]\n",
        "            val_labels = label_val_loader[j].long().reshape(-1)\n",
        "            outputs = net(val_peptides,val_HLA)\n",
        "            _,predicted_labels =  torch.max(outputs, 1)\n",
        "\n",
        "            all_predicted_val_labels += predicted_labels.numpy().tolist()\n",
        "            all_val_targets += val_labels.numpy().tolist()\n",
        "\n",
        "    # Calculating the accuracies\n",
        "    train_accuracies.append(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
        "    val_accuracies.append(accuracy_score(all_val_targets,all_predicted_val_labels))\n",
        "    # Saving the predicitons for further validation\n",
        "    all_val_targets_pr_epoch.append(all_val_targets)\n",
        "    all_val_predictions_pr_epoch.append(all_predicted_val_labels)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (epoch+1, losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
        "    \n",
        "    # if epoch % 10 == 0:\n",
        "    #     print(\"Epoch %2i : Train Loss %f\"  % (epoch+1, losses[-1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "s_chlCRnXKGh",
        "outputId": "142a89f2-ba3f-4830-eb75-d782c84fb621"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Text(0.5, 0, 'epochs'), Text(0, 0.5, 'Acc'))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "epoch = np.arange(len(train_accuracies))\n",
        "plt.figure()\n",
        "plt.plot(epoch, train_accuracies, 'r', epoch, val_accuracies, 'b')\n",
        "plt.legend(['Train Accucary','Validation Accuracy'])\n",
        "plt.xlabel('epochs'), plt.ylabel('Acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4kBdrpadEVM"
      },
      "outputs": [],
      "source": [
        "val_predictions = np.array(all_val_predictions_pr_epoch)\n",
        "val_labels = np.array(all_val_targets_pr_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "EPOCH: 13 Recall: 0.8571428571428571 accuracy: 0.8028953229398663 f1-score: 0.8052805280528051\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAGDCAYAAADNkawvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdfElEQVR4nO3dd5gdZd3/8fd3dxMSSAKBYGgJERLAgEoJVUAeQJoFLDSxgHQFRR9/dDChicgj0pQWQEWqCGIFBBECREIvoYeSBIiRJJCEQNr9+2Nm48myLcuePWHv9+u6cuWcKfd8Z8498zkzc87ZSCkhScpTXa0LkCTVjiEgSRkzBCQpY4aAJGXMEJCkjBkCkpSxbhECEdE7Iv4YEW9FxA0foJ39IuK2zqytViJim4h4tgrttrqtI2JkRFzV2ctdGkXEXRFx0BJMf3hETImIWRGxUjVra6OO7SJiUsXzpyJiu/ZM24FlXRQRJ3V0/q4ShSsiYnpEPFDrerpSQ1cuLCK+CvwAWA+YCTwKnJ5SGvMBm/4KMBBYKaU0v6ONpJR+C/z2A9ZSdRGRgGEppRdamialdA+wbhUW3ynbeklExBDgJaBHVy2zs0VED+BnwBYppcdqXU+llNL6ndFOROwPHJRS2rqi7cM6o+0usDXwGWCNlNLsWhfTlbrsTCAifgD8HDiD4iAyGPgFsHsnNL8m8NyH9QDR2SKimuHutu6YgUAv4KklnbF8l9otztqXRuX+sibwckcCoMr7W/WllKr+D1gemAXs2co0y1CExGvlv58Dy5TjtgMmAf8L/Bt4HTigHDcKmAvMK5dxIDASuKqi7SFAAhrK5/sDEyjORl4C9qsYPqZivq2AccBb5f9bVYy7CzgVuLds5zZgQAvr1lj/0RX17wHsBjwHTAOOr5h+M+B+YEY57QVAz3Lc3eW6zC7Xd++K9o8B3gB+0zisnGftchkbl89XA/4DbNdCvR8r128GxUHrCy1t62bmHQn8Driu3C4PA5+sGL8acCMwtdz2322y3g8CbwNTgJ+Vw18t13lW+W/LZpZbBxwLvAi8CVwPrNjk9T+Eom+9Dvxve/peOX53irPWt8v2d1mSPgCsU75ejetwZzv71+ll23OAoU3aPBb4XZNh5wLnlY8PAJ4u65oAHNq0P1Y8fxnYsXzcG7gSmA6MB/5fk2kbt/HMcvwXK/rMu8CCch1nlMOvBE6rmP9g4AWK/ngLsFrFuAQcBjxfLv9CIFrooyPpeD9rnPeq8jU9tEnto9pZ63fKWl+iE/fx9myLsrbG13c8i+/bza53i8feahz0m3nBdgHmUx6EW5jmFGAs8BFgZeA+4NSKTju/nKZHuWHfAfpXvKiVB/2mz4eUG7UBWK584dctx60KrF8+3p8yBIAVy43/9XK+fcvnK1XspC9S7OC9y+dntrBujfWfXNZ/cPkiXQ30BdYvO+Fa5fSbAFuUyx1SvthHNekgQ5tp/ycUB7TevH9Hb+w0ywK3Ame3UGsPio5/PNAT2L7saOs2t21b2DnnUVw26gH8sOyMPSgO1A+V26EnsBbFAWrnct77ga+Xj/tQXDpZ7PVrZblHUfSfNcptcDFwTZP5rylf/4+X27/xwNda39uM4iD9mbL+1YH1OtAHFlsH2te/Xi37RgPFpbDK9tak2Af6lc/rKQ4mjdvssxThH8Cny2kbDxRN+8bLFdviTOCesr5BwJNNpt2T4kBTR/EGZDawatP9p2L6KylDgKIv/QfYuHyNzgfubtKv/wSsQHGlYCpl4HZyP2ucd49y2t5Na29nrbeX26lxf+vsfbzZbVG+BpOBTcvXdyhFf2h1vWsdAvsBb7QxzYvAbhXPd6Y4PaPcwHOoOAhQpG1jhx/JkoXADODLQO8mNSzqCBQ75wNNxt8P7F+xk55YMe7bwN9aWLfG+uvL533LejavmOYhYI9WDnA3NekgTUNgLtCrybBJTdq5BXgCeJyKd7pNptmG4myirmLYNcDI5rZtCzvn2IrndRQHp22AzYFXm0x/HHBF+fhuirONAU2mWfT6tbLcp4EdKp6vSrGjN1TMv17F+LOA0e3oexcD57SwzCXpA4utQzv71ylt7DNjgG+Ujz8DvNjKtDcD32uub7B4CEyg4sBLcfY0qZV2HwV2b7r/VIy/kv+GwGjgrIpxfcrXaEhFv966Yvz1wLFV6GcjqTigN1d7O2vdvsn+1tn7eLPbguJN3PeaaaPV9W7pX1ddZ3wTGNDGtbPVgFcqnr9SDlvURlr8OvQ7FC/MEknFNb+9KU61Xo+IP0fEeu2op7Gm1Suev7EE9byZUlpQPp5T/j+lYvycxvkjYp2I+FNEvBERb1PcRxnQStsAU1NK77YxzaXABsD5KaX3WphmNWBiSmlhxbCm692WiY0PynYmle2uCawWETMa/1GccQwsJz+Q4l31MxExLiI+twTLXBO4qaLdpylO7wdWTDOx4nFl/2qt7w2iCImWLEkfqNSe/jWR1l1NcQYB8NXyOQARsWtEjI2IaeX22I22+1BjXU230yIR8Y2IeLRiO2/QznYb217UXkppFsWxoaP7VEf72WLzfoBam7bR2ft4S9uipT7ZnvV+n64KgfspToX2aGWa1yhWotHgclhHzKa47NFolcqRKaVbU0qfoXi3+AzFwbGtehprmtzBmpbELynqGpZS6kfxQkYb86TWRkZEH4pr3aOBkRGxYguTvgYManIjcknXe1DFcusoLtG8RrHTvJRSWqHiX9+U0m4AKaXnU0r7UlyW+Qnwu4hYrq11K00Edm3Sdq+UUmXdgyoeV/av1vreRIrLKp2tPf2rrfW+AdguItYAvkgZAhGxDMV14bOBgSmlFYC/0HYfguLddNPtRNnumhT7yhEUl61WoLhc1NhuW/Uuts7la7sSHd+nOtTPOrHW9vTLlnRkH2/UUp9sz3q/T5eEQErpLYrrVBdGxB4RsWxE9CjfrZxVTnYNcGJErBwRA8rpO/p580eBbSNicEQsT3FKBEBEDIyIL5Qv6nsUN4IWNNPGX4B1IuKrEdEQEXsDwymu01VbX4r7FrPKs5TDm4yfQnG9b0mcCzyUUjoI+DNwUQvT/YsiRI8uX6PtgM8D1y7BsjaJiC+VZ35HUWznscADwNsRcUz5fYP6iNggIjYFiIivRcTK5bu6GWVbCyiuhy6k9XW+CDi9PFBR9qPdm0xzUtn31qe4cXpdOby1vjcaOCAidoiIuohYvYUzxyX1gftXSmkqxWWjKyh2/qfLUT0prmNPBeZHxK7ATu1s9nrguIjoX4bLkRXjGgN5KkBEHEBxJtBoCrBGRPRsoe2rKbblhmVQnQH8K6X0cjtra6pD/aydOrvWptrax1tzGfDDiNik/OTY0LLfd2i9u+xjZymln1F8R+BEik40keIdxc3lJKdRfDLkcYrr1g+XwzqyrNspdvDHKa7DVe5YdRSfMnqN4o79pymu5TZt403gc+W0b1Lc9f9cSuk/HalpCf2Q4vR+JsU7r+uajB8J/Ko85durrcbKg+EuFJfAoHgdNo6I/ZpOm1KaC3wB2JXixtgvKK47P7ME9f+B4pLbdIpr319KKc0rT5U/D2xIcRPvPxQdevlyvl2ApyJiFkVo7ZNSejel9A7lJ2XKdd6imWWeS3HP47aImElxMNi8yTT/pLjpfQfFjfHGLwa22PdSSg9QBMY5FDeI/8n738EvsU7sX1cDO1JxKSilNBP4LsUBfTpFX7qlne2NorgM8hLFp51+U9HueOD/KM7sp1DcYL+3Yt47KT5N9kZEvG89Ukp3ACdRnKW8TvFudp921tWcjvazNlWh1qba2sdbq+0Giv3h6nL+myk+Cdeh9Y7y5oHUbUU3+LKZFhcRIyk+HPG1WtfyYecXUCQpY4aAJGXMy0GSlDHPBCQpY4aAJGVsqf31u95bHe91Ki0Vxt1wfK1LkADYYPU+7f1CWbt5JiBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMNdS6AHXcMj0b+PsvDqZnjwYa6uu46R9PctroOzjhwB341hdGMHX6bAB+dPFt3Hr/cwxeZQUeveb7PPfKVAAeeGoi3/3pH2q5CuomLjxrFA+OvYflV1iRn19+PQD33XU71/3qEia/+hJn/uLXDF13+KLpX37xeS4+53TemT2burrgJ7/8DT17LlOr8rNmCHyIvTd3PrscOZrZc+bSUF/HnRcdym1jnwPg/Gvv5efXjHnfPBMmT2OL/S/o6lLVzW238+fZdY+9OO/MHy0aNvijQzl61E+5+JwzFpt2wYL5nPvjE/necacyZO11mPnWDOrrPRTVilv+Q272nLkA9Giop6GhjpRSjStSjtb/5Mb8+43XFhu2xpofbXbaR8eNZchawxiy9joA9F1+hWqXp1ZULQQiYj1gd2B1IAGvAbeklJ6u1jJzVFcX3Hf5d1h7jZW4+PdjGTd+EjttuS6HfWVLvrrrRjz8zGSOPf8vzJj5LgBDVu3P/VcewczZ7zHqktu597GXa7sCys7rk14FglOO/g5vz5jO1tvvzB77fLPWZWWrKjeGI+IY4FoggAeAceXjayLi2FbmOyQiHoyIB+dPeaQapXU7Cxcmttj/Aobu8RNGfGwQw9cayKW//xfD9zybzb95AW+8OZMzj9wNgDfenMk6X/wJW+5/Acec92euHLkXfZf1Oqy61oIF83nmyUc56oTTOP280fxrzD94/OEHal1Wtqr16aADgU1TSmemlK4q/50JbFaOa1ZK6ZKU0oiU0oiGgRtVqbTu6a1Z73L3IxPYafNh/Hv6LBYuTKSUuPwP4xgxfBAAc+ctYNrbcwB45NnXmDB5GsMGD6hl2crQSisPZPgnN6bf8v1ZpldvNt78U0x47plal5WtaoXAQmC1ZoavWo5TJxiwwnIs36cXAL16NrD9iKE8+8pUVlmp76Jpdv/0+oyfMGXR9HV1AcCQ1fozdNBKvDR5WtcXrqxtuOmWvPLi87z37hwWLJjPU489zKAhzd8/UPVV657AUcAdEfE8MLEcNhgYChxRpWVmZ5WV+nLpSV+hvi6oq6vjxjue4K/3Pcvok/fkE8NWJaXEK6/P4MizbgZg6w2HcNJBOzJ/wUIWLFzIkWf9gekz59R2JdQt/OzU43nqsQeZ+dYMDt5rV/be/1D69u3HZef/lLffms4Zx3+PIWuvw8lnXUifvv34/J5f4+jDv0FEsPHmn2KTLbap9SpkK6r1aZKIqKO4/LM6xf2AScC4lNKC9szfe6vj/ZiLlgrjbji+1iVIAGywep/o7Dar9umglNJCYGy12pckfXD+bIQkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZa2hpREScD6SWxqeUvluViiRJXabFEAAe7LIqJEk10WIIpJR+1ZWFSJK6XmtnAgBExMrAMcBwoFfj8JTS9lWsS5LUBdpzY/i3wNPAR4FRwMvAuCrWJEnqIu0JgZVSSqOBeSmlf6aUvgVsUeW6JEldoM3LQcC88v/XI+KzwGvAGtUrSZLUVdoTAqdFxPLA/wLnA/2A71e1KklSl2gzBFJKfyofvgX8T3XLkSR1pfZ8OugKmvnSWHlvQJL0Idaey0F/qnjcC/gixX0BSdKHXHsuB91Y+TwirgH+XrWKJEldpiM/IDcMGNzZhUiSul6k1OJvxBUTRMxk8XsCbwDHNT1D6Gzvzm/5x+ukrtR/0yNqXYIEwJxHLojObrM9l4P6dvZCJUlLhzYvB0XEHe0ZJkn68Gnt7wn0ApYFBkREf6DxNKQfsFoX1CZJqrLWLgcdChxFccB/iP+GwNvAhdUtS5LUFVr7ewLnAudGxJEppfO7sCZJUhdpz0dEF0bECo1PIqJ/RHy7eiVJkrpKe0Lg4JTSjMYnKaXpwMFVq0iS1GXaEwJ1EbHos6kRUQ/0rF5JkqSu0p7fDroVuD4iLqL40thhwF+rWpUkqUu0JwSOAQ4BDqf4hNAjwKrVLEqS1DXavByUUloIjAUmACOAHSj+5rAk6UOutS+LrQPsA+wLvAlcB5BS8g/LSFI30drloGeAe4DPp5ReAIgI/6ykJHUjrV0O+jLFL4b+IyIujYgd+O+3hiVJ3UCLIZBSuimltDewHnAXxR+XHxgRv4yInbqoPklSFbXnxvDslNJvU0qfA9YAHgWOrXZhkqTqW6K/LJZSmpZSujiltH21CpIkdZ2O/HlJSVI3YQhIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJylhDrQtQ53n77bcZdfKJvPDCc0QEo049gylT3uCXF17ASxNe5LfX3sD6G3y81mWqG1qmZwN/H30UPXs20FBfz01/f4TTLvoLAIfv82kO23tb5i9YyN/ueZITzv0DABsMW40LTtyXvsv1YuHCxNZfO4v35s6v5WpkyRDoRs768el8autt+L+fn8e8uXOZ8+679O3bj3POPZ9TR/2o1uWpG3tv7nx2OeQ8Zs+ZS0NDHXde/gNuu3c8vZbpwee2+zib7vVj5s6bz8r9+wBQX1/H5ad9kwNP+jVPPDeZFZdfjnnzF9R4LfJkCHQTs2bN4qGHxnHqGWcC0KNnT3r07Em/fv1qXJlyMXvOXAB6NNTT0FBPSolD9tyGs6+4nbnzinf4U6fPAmDHLdfjyecn88RzkwGY9tbs2hQt7wl0F5MmTqR//xU5+YTj2OvLezDy5BN45513al2WMlJXF4y99lheveNM7hz7DOOefIWha36ET220Nnf/+ofcdtn32GT4YACGDf4IKcEtF36H+64+hh98c8caV5+vLg+BiDiglXGHRMSDEfHg6Esv6cqyPvQWLJjPM0+PZ8999uX6G2+md+/eXH6Z21BdZ+HCxBb7nMnQnU9kxAZrMnztVWmor6N/v2XZ9htnc/w5N3PVWd8CoKG+nq02WosDTriSHb71M76w/SfZbrN1arwGearFmcColkaklC5JKY1IKY048OBDurKmD72BA1dh4MBV+MQnPgnAZ3bahWeeHl/jqpSjt2bN4e4Hn2enrYYzecoMbr7jMQAefOoVFi5MDOjfh8n/nsE9D73AmzNmM+fdefxtzFNstN6gGleep6qEQEQ83sK/J4CB1Vhm7gasvDIDV1mFl1+aAMC/xt7PWmuvXeOqlIsB/fuwfJ/eAPRapgfbb74uz748hT/e9fiid/hDB3+Enj0a+M/0Wdx+33g2GLY6vXv1oL6+jm02GcrTE96o5Spkq1o3hgcCOwPTmwwP4L4qLTN7xx5/Escd80PmzZvHGmsM4pTTfswdf7+dM884lenTpnHEtw9l3XU/xkWXjq51qepmVhnQj0tP+Tr1dXXU1QU33v4wf73nSXo01HPxyP148IbjmTtvAQed/BsAZsycw3lX3cmYq44mpcStY57ib2OeqvFa5ClSSp3faMRo4IqU0phmxl2dUvpqW228O5/OL0zqgP6bHlHrEiQA5jxyQXR2m1U5E0gpHdjKuDYDQJLUNfyIqCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjJmCEhSxgwBScqYISBJGTMEJCljhoAkZcwQkKSMGQKSlDFDQJIyZghIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMmYISFLGDAFJypghIEkZMwQkKWOGgCRlzBCQpIwZApKUMUNAkjIWKaVa16AqiohDUkqX1LoOyb64dPJMoPs7pNYFSCX74lLIEJCkjBkCkpQxQ6D78xqslhb2xaWQN4YlKWOeCUhSxgyBbioidomIZyPihYg4ttb1KF8RcXlE/Dsinqx1LXo/Q6Abioh64EJgV2A4sG9EDK9tVcrYlcAutS5CzTMEuqfNgBdSShNSSnOBa4Hda1yTMpVSuhuYVus61DxDoHtaHZhY8XxSOUySFmMIdE/RzDA/BibpfQyB7mkSMKji+RrAazWqRdJSzBDonsYBwyLioxHRE9gHuKXGNUlaChkC3VBKaT5wBHAr8DRwfUrpqdpWpVxFxDXA/cC6ETEpIg6sdU36L78xLEkZ80xAkjJmCEhSxgwBScqYISBJGTMEJCljhoC6jYhYEBGPRsSTEXFDRCz7Adq6MiK+Uj6+rLUf4IuI7SJiqw4s4+WIGNDRGqXOYAioO5mTUtowpbQBMBc4rHJk+euqSyyldFBKaXwrk2wHLHEISEsDQ0Dd1T3A0PJd+j8i4mrgiYioj4ifRsS4iHg8Ig4FiMIFETE+Iv4MfKSxoYi4KyJGlI93iYiHI+KxiLgjIoZQhM33y7OQbSJi5Yi4sVzGuIj4VDnvShFxW0Q8EhEX0/xvPEldqqHWBUidLSIaKP6Wwt/KQZsBG6SUXoqIQ4C3UkqbRsQywL0RcRuwEbAu8HFgIDAeuLxJuysDlwLblm2tmFKaFhEXAbNSSmeX010NnJNSGhMRgym+uf0x4EfAmJTSKRHxWeCQqm4IqR0MAXUnvSPi0fLxPcBoiss0D6SUXiqH7wR8ovF6P7A8MAzYFrgmpbQAeC0i7mym/S2AuxvbSim19Bv5OwLDIxa90e8XEX3LZXypnPfPETG9Y6spdR5DQN3JnJTShpUDygPx7MpBwJEppVubTLcbbf/cdrRjGigus26ZUprTTC3+TouWKt4TUG5uBQ6PiB4AEbFORCwH3A3sU94zWBX4n2bmvR/4dER8tJx3xXL4TKBvxXS3UfyAH+V0G5YP7wb2K4ftCvTvrJWSOsoQUG4uo7je/3D5h88vpjgjvgl4HngC+CXwz6YzppSmUlzH/31EPAZcV476I/DFxhvDwHeBEeWN5/H891NKo4BtI+JhistSr1ZpHaV281dEJSljnglIUsYMAUnKmCEgSRkzBCQpY4aAJGXMEJCkjBkCkpQxQ0CSMvb/AVPHaXEv5J2IAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score,recall_score\n",
        "best_epoch_model = np.argmax(val_accuracies)\n",
        "\n",
        "print(len(val_predictions))\n",
        "\n",
        "# for epoch in range(len(val_predictions)):\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "accuracy = accuracy_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "recall = recall_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "f1 = f1_score(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "\n",
        "print(f\"EPOCH: {best_epoch_model} Recall: {recall} accuracy: {accuracy} f1-score: {f1}\")\n",
        "plt.title(\"Confusion matrix of best epoch for validation performance\")\n",
        "conf_mat = confusion_matrix(val_labels[best_epoch_model],val_predictions[best_epoch_model])\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking at the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "conv1_peptide.weight \t torch.Size([16, 1, 2, 12])\n",
            "conv1_peptide.bias \t torch.Size([16])\n",
            "BatchNorm_conv1_peptides.weight \t torch.Size([16])\n",
            "BatchNorm_conv1_peptides.bias \t torch.Size([16])\n",
            "conv2_peptide.weight \t torch.Size([32, 16, 2, 1])\n",
            "conv2_peptide.bias \t torch.Size([32])\n",
            "BatchNorm_conv2_peptides.weight \t torch.Size([32])\n",
            "BatchNorm_conv2_peptides.bias \t torch.Size([32])\n",
            "conv1_HLA.weight \t torch.Size([16, 1, 15, 12])\n",
            "conv1_HLA.bias \t torch.Size([16])\n",
            "BatchNorm_conv1_HLA.weight \t torch.Size([16])\n",
            "BatchNorm_conv1_HLA.bias \t torch.Size([16])\n",
            "conv2_HLA.weight \t torch.Size([32, 16, 9, 1])\n",
            "conv2_HLA.bias \t torch.Size([32])\n",
            "BatchNorm_conv2_HLA.weight \t torch.Size([32])\n",
            "BatchNorm_conv2_HLA.bias \t torch.Size([32])\n",
            "L_in.weight \t torch.Size([128, 256])\n",
            "L_in.bias \t torch.Size([128])\n",
            "L_out.weight \t torch.Size([2, 128])\n",
            "Optimizer's state_dict:\n",
            "state \t {0: {'step': 8100, 'exp_avg': tensor([[[[ 5.6700e-03,  5.0649e-04,  7.7974e-03,  3.6598e-03, -8.3838e-03,\n",
            "            1.7291e-03,  3.6936e-03,  1.7423e-03,  3.3842e-03, -4.0513e-03,\n",
            "           -6.7993e-04, -1.0154e-03],\n",
            "          [ 7.0568e-03,  1.4131e-02,  1.4569e-02,  9.0848e-03,  1.3637e-02,\n",
            "            4.8732e-03,  6.3659e-03, -2.2105e-03,  7.7109e-03,  9.7800e-04,\n",
            "           -1.6575e-03, -8.2526e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.0993e-03, -4.5499e-03, -8.2868e-03, -1.5886e-03, -1.2221e-03,\n",
            "           -4.1631e-03, -4.4695e-03, -5.9575e-03, -1.3204e-03,  4.4956e-03,\n",
            "            8.8846e-04,  6.8495e-04],\n",
            "          [-3.6018e-03, -6.8444e-03, -3.3728e-03,  6.2965e-03, -9.5109e-04,\n",
            "            5.4114e-03, -7.2191e-03, -1.3585e-04,  5.5943e-04,  2.2730e-03,\n",
            "           -1.8159e-03,  1.7598e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.4635e-02,  1.1220e-02,  1.7639e-02,  5.3859e-03, -2.0483e-03,\n",
            "           -7.8737e-03, -4.4715e-03, -1.9005e-03, -4.8411e-03, -5.6689e-03,\n",
            "           -4.0187e-03, -4.9321e-04],\n",
            "          [-1.9915e-03, -8.0510e-03, -8.6541e-03,  1.5409e-03,  7.6630e-03,\n",
            "           -8.0629e-03, -9.3792e-03, -1.4513e-02, -5.7516e-03,  6.3275e-04,\n",
            "           -2.2114e-03, -7.2421e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 7.5427e-03, -5.3018e-04,  4.3509e-03, -4.3542e-03, -1.2437e-02,\n",
            "           -3.9059e-03,  4.5071e-03,  3.8336e-03, -3.9033e-03,  2.1528e-03,\n",
            "           -1.9809e-03,  2.4215e-03],\n",
            "          [ 9.1205e-04, -1.5339e-03,  5.2506e-03, -1.5441e-03, -1.2578e-02,\n",
            "           -1.4360e-03,  2.1618e-03,  1.6135e-03, -3.8089e-03,  1.5879e-03,\n",
            "           -1.4446e-03,  3.6653e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9767e-04,  3.1535e-03, -3.8982e-03, -2.0796e-03, -4.4941e-04,\n",
            "           -1.3850e-03, -1.6169e-03, -4.1860e-03, -3.3674e-03, -1.3929e-03,\n",
            "           -7.2836e-04, -2.2754e-04],\n",
            "          [-1.6383e-03,  4.3029e-03,  1.0739e-04, -8.0277e-03, -1.1342e-02,\n",
            "            6.7560e-03,  5.7122e-04, -2.8428e-03, -9.0418e-04, -1.2492e-03,\n",
            "           -1.4310e-04,  2.3120e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.2623e-03, -7.6146e-03, -1.9782e-02, -2.9472e-03, -9.0275e-03,\n",
            "            8.9616e-04, -1.4986e-03, -7.5766e-04,  4.4274e-03,  1.2390e-03,\n",
            "           -3.4954e-03,  4.1208e-03],\n",
            "          [ 1.1515e-02,  7.3631e-03, -4.8308e-03, -7.3915e-03, -6.6356e-03,\n",
            "           -4.8591e-03,  5.3562e-04,  4.4407e-04,  2.9547e-03,  2.2109e-03,\n",
            "            1.9067e-03,  3.5494e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0598e-02,  5.2645e-05,  6.1746e-03,  2.8950e-03,  5.5839e-03,\n",
            "            1.7436e-03,  3.3868e-03,  1.4065e-03,  1.0423e-03, -3.1311e-03,\n",
            "           -1.5507e-03, -2.7672e-04],\n",
            "          [-2.2298e-02,  1.2158e-02,  7.4348e-03, -1.2728e-03, -1.3479e-03,\n",
            "           -6.2706e-03, -3.2092e-03,  5.7542e-03,  2.6944e-04, -2.4261e-03,\n",
            "            2.8220e-03, -2.5253e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9644e-02,  1.9311e-03, -5.8978e-03, -5.3448e-03,  4.8512e-03,\n",
            "           -1.0758e-02,  8.2626e-03,  4.6386e-03, -8.5931e-04, -2.6998e-03,\n",
            "           -2.7009e-04, -5.2980e-03],\n",
            "          [ 1.9248e-03, -1.5192e-03,  4.0254e-03,  3.7301e-03, -5.6302e-04,\n",
            "            3.9704e-03,  3.0203e-03,  1.6670e-03, -1.6363e-03, -3.5966e-04,\n",
            "            9.7809e-05, -1.2646e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6745e-02,  6.9107e-03,  6.1572e-03, -4.3624e-03, -8.3589e-03,\n",
            "           -2.5512e-03, -1.4547e-03, -3.7465e-03, -5.0979e-03,  1.6027e-04,\n",
            "           -1.2283e-03,  1.9270e-03],\n",
            "          [ 1.7920e-02, -5.9658e-04, -1.7653e-03,  3.5928e-03, -1.0989e-02,\n",
            "            3.0777e-03,  7.3871e-04, -3.1536e-03,  1.5582e-06, -4.2909e-03,\n",
            "           -4.1817e-03,  3.8773e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6815e-03,  1.9248e-03,  2.6267e-03, -8.4905e-04,  3.0615e-03,\n",
            "           -5.9149e-04,  3.3705e-03,  1.5123e-05,  1.8638e-03,  3.9543e-03,\n",
            "            1.0835e-03,  2.9722e-04],\n",
            "          [ 1.1505e-02, -8.0667e-03, -9.8305e-03,  2.8215e-03,  3.7158e-04,\n",
            "            4.5624e-03,  2.8503e-03,  2.2312e-03,  1.9767e-03, -2.2664e-04,\n",
            "            2.3834e-03, -2.5413e-04]]],\n",
            "\n",
            "\n",
            "        [[[-7.0811e-03, -4.8317e-03, -8.3252e-03, -3.7646e-04, -3.2065e-03,\n",
            "            1.2698e-04,  5.1009e-03,  2.1795e-03, -2.4441e-03, -3.3665e-03,\n",
            "            2.3069e-03, -1.1218e-03],\n",
            "          [-7.9655e-03, -4.6318e-03,  5.0661e-03,  1.3728e-03,  3.3230e-03,\n",
            "           -6.4417e-03, -9.3550e-04,  2.6415e-03,  6.4577e-03, -1.6910e-03,\n",
            "            1.2384e-03, -1.0540e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.7196e-04, -7.7413e-04,  4.7906e-04,  9.6966e-03,  2.5541e-03,\n",
            "            1.1131e-03,  6.7520e-03,  2.8237e-03,  5.2309e-03,  4.5382e-03,\n",
            "           -3.2797e-03, -2.8778e-04],\n",
            "          [ 1.6833e-03, -1.0136e-03,  8.9653e-03, -1.4815e-03,  8.8063e-03,\n",
            "            4.7778e-03,  7.3423e-03,  1.9926e-03,  2.4701e-03, -2.0357e-04,\n",
            "            2.6407e-03,  3.6458e-04]]],\n",
            "\n",
            "\n",
            "        [[[-2.5660e-02, -8.9299e-03,  9.9845e-03,  8.1224e-03,  5.3008e-03,\n",
            "            9.2241e-03,  3.0605e-03, -3.2993e-03,  6.4222e-03, -1.5041e-03,\n",
            "           -2.1166e-03, -1.6552e-03],\n",
            "          [-1.3368e-02, -7.5288e-04,  1.2942e-02,  1.5170e-04,  1.6981e-03,\n",
            "           -5.3542e-03, -6.5004e-03,  7.5089e-04,  2.5582e-03, -3.3620e-03,\n",
            "            2.2966e-03, -1.6035e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.3567e-03, -4.4457e-03, -6.4737e-03,  5.3499e-03, -2.7570e-03,\n",
            "            3.0498e-03, -5.8234e-04, -3.9693e-03,  5.8559e-03, -1.2565e-03,\n",
            "           -2.8012e-03, -1.4720e-03],\n",
            "          [-3.5763e-03, -1.6793e-02,  3.0492e-03,  4.9246e-03,  5.5235e-03,\n",
            "           -7.0515e-05, -2.2797e-05,  1.2610e-03,  3.9189e-03, -6.8561e-04,\n",
            "            1.7581e-03,  1.0545e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.4699e-03,  4.5877e-03,  1.5926e-03, -2.9711e-03,  8.1348e-03,\n",
            "            5.6410e-03, -2.2001e-03,  3.8192e-04,  3.0315e-04,  5.2393e-03,\n",
            "           -7.4108e-04, -2.4657e-03],\n",
            "          [-4.7224e-03, -7.4676e-03,  7.4496e-03, -1.0535e-03,  8.5711e-04,\n",
            "            1.7713e-03,  4.8223e-03,  5.7420e-03, -2.2025e-03,  1.3252e-03,\n",
            "           -4.9100e-03, -5.0313e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2961e-02, -7.0081e-03,  3.7739e-03, -6.2767e-03,  8.3229e-03,\n",
            "           -6.8078e-04,  6.2257e-03, -3.0148e-03, -3.1807e-03, -1.1557e-03,\n",
            "            6.2210e-03, -3.1839e-03],\n",
            "          [ 1.3015e-02,  5.7909e-03,  8.5093e-03,  2.0328e-03, -3.2379e-03,\n",
            "            1.3868e-03,  3.3990e-03,  4.3601e-03, -3.3072e-03,  2.7943e-03,\n",
            "           -2.0754e-03,  1.8001e-04]]]]), 'exp_avg_sq': tensor([[[[1.7662e-03, 1.8829e-03, 1.7575e-03, 1.1111e-03, 1.1915e-03,\n",
            "           4.6588e-04, 2.8793e-04, 2.7187e-04, 3.3310e-04, 1.6748e-04,\n",
            "           3.1954e-04, 1.9346e-04],\n",
            "          [7.7058e-03, 4.0487e-03, 3.3319e-03, 1.2585e-03, 9.6283e-04,\n",
            "           3.8341e-04, 5.2403e-04, 2.3842e-04, 4.2774e-04, 2.0664e-04,\n",
            "           2.0781e-04, 1.6177e-04]]],\n",
            "\n",
            "\n",
            "        [[[6.7580e-04, 2.5826e-03, 5.6737e-04, 3.8822e-04, 8.9536e-04,\n",
            "           4.1910e-04, 1.4711e-04, 1.9735e-04, 1.9388e-04, 1.2352e-04,\n",
            "           1.4480e-04, 7.9912e-05],\n",
            "          [2.0189e-03, 1.2536e-03, 8.0669e-04, 5.7857e-04, 5.8477e-04,\n",
            "           3.7352e-04, 2.4594e-04, 2.1295e-04, 2.5354e-04, 1.0909e-04,\n",
            "           1.1749e-04, 1.2257e-04]]],\n",
            "\n",
            "\n",
            "        [[[8.4295e-03, 1.1245e-03, 2.4873e-03, 1.2415e-03, 1.0340e-03,\n",
            "           5.7367e-04, 3.4251e-04, 1.8043e-04, 2.6465e-04, 2.6801e-04,\n",
            "           1.3588e-04, 3.7539e-05],\n",
            "          [1.3579e-03, 2.0439e-03, 1.1453e-03, 7.7191e-04, 7.2495e-04,\n",
            "           4.1735e-04, 4.3754e-04, 4.3514e-04, 3.1721e-04, 2.6679e-04,\n",
            "           2.1388e-04, 1.2584e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.3320e-04, 1.5354e-03, 3.1565e-04, 2.1454e-04, 9.0909e-04,\n",
            "           1.6863e-04, 1.7961e-04, 1.3052e-04, 1.7722e-04, 5.9035e-05,\n",
            "           1.1672e-04, 8.1660e-05],\n",
            "          [9.1482e-04, 1.3695e-03, 1.5879e-03, 8.6692e-04, 7.1586e-04,\n",
            "           5.9915e-04, 2.1652e-04, 1.8199e-04, 1.7253e-04, 1.5110e-04,\n",
            "           1.2664e-04, 7.8488e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.5725e-04, 1.0654e-03, 8.8581e-04, 5.7030e-04, 2.5608e-04,\n",
            "           5.6756e-04, 2.8798e-04, 2.0595e-04, 1.5124e-04, 7.7954e-05,\n",
            "           1.0985e-04, 9.2142e-05],\n",
            "          [7.1436e-04, 6.8952e-04, 7.8136e-04, 3.5555e-04, 5.0694e-04,\n",
            "           4.7289e-04, 1.2440e-04, 1.1899e-04, 1.1752e-04, 1.2957e-04,\n",
            "           8.4780e-05, 5.1202e-05]]],\n",
            "\n",
            "\n",
            "        [[[1.4397e-03, 1.1887e-03, 1.7461e-03, 7.0481e-04, 1.3558e-03,\n",
            "           6.6976e-04, 2.2686e-04, 1.5737e-04, 3.5188e-04, 2.5771e-04,\n",
            "           2.0399e-04, 1.6369e-04],\n",
            "          [5.1237e-03, 1.8665e-03, 1.4479e-03, 8.1742e-04, 9.8374e-04,\n",
            "           8.5132e-04, 1.9978e-04, 1.5103e-04, 3.7960e-04, 1.1519e-04,\n",
            "           2.1467e-04, 1.1991e-04]]],\n",
            "\n",
            "\n",
            "        [[[5.0148e-03, 6.4516e-04, 1.2339e-03, 2.8756e-04, 3.8764e-04,\n",
            "           2.2032e-04, 2.8579e-04, 1.4410e-04, 1.4610e-04, 1.4418e-04,\n",
            "           9.7352e-05, 3.8268e-05],\n",
            "          [3.3383e-03, 8.5697e-04, 1.1368e-03, 3.7490e-04, 4.5007e-04,\n",
            "           8.1400e-04, 2.5019e-04, 2.0239e-04, 1.8789e-04, 1.3745e-04,\n",
            "           1.0486e-04, 7.5352e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.1510e-03, 6.9424e-04, 5.5257e-04, 3.1040e-04, 4.7933e-04,\n",
            "           5.2067e-04, 1.9239e-04, 2.1330e-04, 1.9552e-04, 9.7083e-05,\n",
            "           1.3561e-04, 8.7327e-05],\n",
            "          [5.3461e-04, 7.1893e-04, 7.3333e-04, 4.0068e-04, 6.2138e-04,\n",
            "           6.1360e-04, 1.2986e-04, 1.3257e-04, 1.7963e-04, 1.0988e-04,\n",
            "           1.1142e-04, 5.3547e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.4885e-03, 5.1383e-04, 1.1969e-03, 7.6444e-04, 4.7982e-04,\n",
            "           5.6038e-04, 1.4126e-04, 2.0031e-04, 2.1846e-04, 1.1624e-04,\n",
            "           1.0056e-04, 6.6251e-05],\n",
            "          [2.7942e-03, 5.4639e-04, 8.8886e-04, 4.9912e-04, 4.2349e-04,\n",
            "           4.9237e-04, 2.0297e-04, 1.5065e-04, 1.7175e-04, 1.7343e-04,\n",
            "           1.1844e-04, 5.6610e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.3128e-03, 7.1795e-04, 1.1237e-03, 7.9352e-04, 6.5409e-04,\n",
            "           2.4286e-04, 1.1549e-04, 1.6759e-04, 2.0077e-04, 1.4060e-04,\n",
            "           9.2041e-05, 7.2785e-05],\n",
            "          [4.6188e-03, 4.5111e-04, 1.5910e-03, 5.8389e-04, 3.6291e-04,\n",
            "           8.4252e-04, 2.0008e-04, 2.0227e-04, 1.7820e-04, 1.4783e-04,\n",
            "           7.8520e-05, 6.9836e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.9415e-04, 1.7818e-03, 1.0763e-03, 7.9478e-04, 7.9634e-04,\n",
            "           2.3576e-04, 2.9206e-04, 2.5846e-04, 2.6717e-04, 1.7957e-04,\n",
            "           1.5413e-04, 1.1490e-04],\n",
            "          [1.4992e-03, 1.0837e-03, 4.2234e-04, 2.8936e-04, 5.7381e-04,\n",
            "           8.8115e-04, 2.2843e-04, 2.0190e-04, 2.5875e-04, 8.5915e-05,\n",
            "           1.4449e-04, 1.1182e-04]]],\n",
            "\n",
            "\n",
            "        [[[4.8902e-04, 1.4849e-03, 5.9997e-04, 3.8822e-04, 7.5176e-04,\n",
            "           4.6988e-04, 2.3701e-04, 1.9255e-04, 2.7416e-04, 1.5199e-04,\n",
            "           2.1301e-04, 1.2113e-04],\n",
            "          [7.6205e-04, 7.3458e-04, 1.0899e-03, 8.3158e-04, 8.9348e-04,\n",
            "           6.8092e-04, 2.0336e-04, 2.5698e-04, 2.2460e-04, 1.9862e-04,\n",
            "           1.3429e-04, 6.5751e-05]]],\n",
            "\n",
            "\n",
            "        [[[4.0185e-03, 5.5566e-04, 1.4508e-03, 3.1798e-04, 6.2596e-04,\n",
            "           6.2336e-04, 1.0101e-04, 9.9429e-05, 2.9543e-04, 1.4401e-04,\n",
            "           1.2903e-04, 8.2626e-05],\n",
            "          [1.8133e-03, 7.9248e-04, 1.2565e-03, 7.3512e-04, 5.0582e-04,\n",
            "           1.8939e-04, 2.1007e-04, 1.3670e-04, 1.2897e-04, 1.9695e-04,\n",
            "           1.3945e-04, 5.2872e-05]]],\n",
            "\n",
            "\n",
            "        [[[7.6054e-04, 1.9280e-03, 9.5544e-04, 5.9761e-04, 7.1133e-04,\n",
            "           2.3218e-04, 2.1200e-04, 2.1068e-04, 2.3131e-04, 8.9632e-05,\n",
            "           1.9762e-04, 9.3348e-05],\n",
            "          [2.8855e-03, 1.5578e-03, 8.9801e-04, 4.8123e-04, 5.8882e-04,\n",
            "           1.0336e-04, 2.1992e-04, 1.7277e-04, 1.2601e-04, 1.0235e-04,\n",
            "           1.1521e-04, 5.2492e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.1055e-03, 8.0407e-04, 9.3349e-04, 4.1118e-04, 8.6398e-04,\n",
            "           2.1440e-04, 1.8133e-04, 1.5133e-04, 1.8404e-04, 1.1902e-04,\n",
            "           1.5271e-04, 7.7924e-05],\n",
            "          [1.7293e-03, 1.1885e-03, 1.0691e-03, 8.6495e-04, 5.6286e-04,\n",
            "           1.3500e-04, 2.4271e-04, 2.3475e-04, 2.0531e-04, 1.0478e-04,\n",
            "           1.4549e-04, 9.1983e-05]]],\n",
            "\n",
            "\n",
            "        [[[3.2495e-03, 1.5480e-03, 8.0143e-04, 5.1253e-04, 7.3898e-04,\n",
            "           1.6937e-04, 1.3540e-04, 1.4560e-04, 1.2531e-04, 1.0585e-04,\n",
            "           1.2368e-04, 4.1274e-05],\n",
            "          [2.5772e-03, 7.9937e-04, 1.2810e-03, 7.9058e-04, 6.7082e-04,\n",
            "           1.8499e-04, 2.0705e-04, 2.4414e-04, 2.5553e-04, 1.4456e-04,\n",
            "           1.5036e-04, 8.6861e-05]]]])}, 1: {'step': 8100, 'exp_avg': tensor([ 7.3661e-05,  6.7321e-04, -9.0732e-04, -6.4390e-04, -3.5986e-04,\n",
            "         1.4049e-04, -3.1418e-04,  1.5226e-04, -1.1359e-04,  6.6075e-04,\n",
            "        -1.6878e-04, -1.1054e-04,  5.6808e-04,  5.9078e-05,  4.1521e-05,\n",
            "         1.1493e-06]), 'exp_avg_sq': tensor([9.5003e-06, 8.9789e-06, 8.4271e-06, 5.9715e-06, 4.3786e-06, 9.9352e-06,\n",
            "        4.6439e-06, 4.2301e-06, 5.4228e-06, 7.3018e-06, 4.2551e-06, 6.0582e-06,\n",
            "        5.0303e-06, 4.1750e-06, 6.0763e-06, 5.9480e-06])}, 4: {'step': 8100, 'exp_avg': tensor([[[[ 1.5056e-03],\n",
            "          [ 4.2566e-03]],\n",
            "\n",
            "         [[ 1.7075e-03],\n",
            "          [ 4.5824e-03]],\n",
            "\n",
            "         [[ 2.2916e-03],\n",
            "          [-2.3931e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9490e-03],\n",
            "          [-7.4913e-04]],\n",
            "\n",
            "         [[-4.1290e-03],\n",
            "          [ 5.6194e-04]],\n",
            "\n",
            "         [[ 1.4070e-03],\n",
            "          [-6.3318e-04]]],\n",
            "\n",
            "\n",
            "        [[[-4.2355e-03],\n",
            "          [-1.1688e-03]],\n",
            "\n",
            "         [[-5.3576e-04],\n",
            "          [-1.6070e-03]],\n",
            "\n",
            "         [[ 9.2387e-04],\n",
            "          [ 3.1559e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4351e-03],\n",
            "          [-1.0103e-03]],\n",
            "\n",
            "         [[ 2.0871e-03],\n",
            "          [ 5.1843e-03]],\n",
            "\n",
            "         [[-2.6960e-04],\n",
            "          [-1.3920e-03]]],\n",
            "\n",
            "\n",
            "        [[[-9.0123e-04],\n",
            "          [-2.7421e-03]],\n",
            "\n",
            "         [[-1.6193e-03],\n",
            "          [-1.2192e-03]],\n",
            "\n",
            "         [[ 1.6981e-03],\n",
            "          [ 3.9446e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2186e-04],\n",
            "          [ 1.8807e-04]],\n",
            "\n",
            "         [[ 8.4420e-04],\n",
            "          [-2.6199e-04]],\n",
            "\n",
            "         [[ 4.1728e-04],\n",
            "          [-5.8501e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.7068e-04],\n",
            "          [-1.2221e-03]],\n",
            "\n",
            "         [[-7.0638e-05],\n",
            "          [ 3.4510e-05]],\n",
            "\n",
            "         [[ 1.4956e-03],\n",
            "          [ 3.3000e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2320e-03],\n",
            "          [-4.1212e-03]],\n",
            "\n",
            "         [[-3.4216e-03],\n",
            "          [-3.6033e-03]],\n",
            "\n",
            "         [[ 1.4943e-03],\n",
            "          [-1.4667e-03]]],\n",
            "\n",
            "\n",
            "        [[[-5.3084e-03],\n",
            "          [-1.7484e-03]],\n",
            "\n",
            "         [[ 1.1199e-03],\n",
            "          [-1.6654e-03]],\n",
            "\n",
            "         [[-2.5173e-03],\n",
            "          [-2.2734e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.3429e-03],\n",
            "          [-1.9482e-03]],\n",
            "\n",
            "         [[ 7.9294e-04],\n",
            "          [ 3.9398e-03]],\n",
            "\n",
            "         [[-1.1844e-03],\n",
            "          [-3.0628e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.3829e-03],\n",
            "          [-4.5038e-04]],\n",
            "\n",
            "         [[ 3.9811e-03],\n",
            "          [ 1.2167e-03]],\n",
            "\n",
            "         [[-4.2502e-03],\n",
            "          [-4.7939e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4869e-04],\n",
            "          [ 3.2590e-03]],\n",
            "\n",
            "         [[ 3.8920e-03],\n",
            "          [-6.4319e-04]],\n",
            "\n",
            "         [[-2.2292e-03],\n",
            "          [-1.3597e-03]]]]), 'exp_avg_sq': tensor([[[[8.4840e-05],\n",
            "          [6.9654e-05]],\n",
            "\n",
            "         [[1.0465e-04],\n",
            "          [1.1918e-04]],\n",
            "\n",
            "         [[2.6338e-05],\n",
            "          [5.8632e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.2557e-05],\n",
            "          [5.4929e-05]],\n",
            "\n",
            "         [[1.0518e-04],\n",
            "          [4.8864e-05]],\n",
            "\n",
            "         [[6.0888e-05],\n",
            "          [7.5983e-05]]],\n",
            "\n",
            "\n",
            "        [[[5.2832e-05],\n",
            "          [1.9289e-04]],\n",
            "\n",
            "         [[1.0230e-04],\n",
            "          [1.6821e-04]],\n",
            "\n",
            "         [[1.2843e-04],\n",
            "          [8.7438e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.3795e-05],\n",
            "          [7.7764e-05]],\n",
            "\n",
            "         [[9.2687e-05],\n",
            "          [1.4143e-04]],\n",
            "\n",
            "         [[7.3175e-05],\n",
            "          [1.0898e-04]]],\n",
            "\n",
            "\n",
            "        [[[2.2995e-05],\n",
            "          [3.5124e-05]],\n",
            "\n",
            "         [[3.4429e-05],\n",
            "          [1.7640e-05]],\n",
            "\n",
            "         [[3.5981e-05],\n",
            "          [3.1822e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.2792e-05],\n",
            "          [1.3272e-05]],\n",
            "\n",
            "         [[3.1623e-05],\n",
            "          [2.6147e-05]],\n",
            "\n",
            "         [[1.7963e-05],\n",
            "          [1.3974e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[7.3535e-05],\n",
            "          [1.6408e-04]],\n",
            "\n",
            "         [[1.1124e-04],\n",
            "          [1.1191e-04]],\n",
            "\n",
            "         [[2.0598e-04],\n",
            "          [8.6475e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[3.8551e-05],\n",
            "          [8.7016e-05]],\n",
            "\n",
            "         [[1.1459e-04],\n",
            "          [3.6129e-04]],\n",
            "\n",
            "         [[8.0145e-05],\n",
            "          [1.4643e-04]]],\n",
            "\n",
            "\n",
            "        [[[1.0008e-04],\n",
            "          [2.7431e-05]],\n",
            "\n",
            "         [[7.4501e-05],\n",
            "          [4.8335e-05]],\n",
            "\n",
            "         [[2.7811e-05],\n",
            "          [1.1231e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[5.4357e-05],\n",
            "          [2.2907e-05]],\n",
            "\n",
            "         [[7.9084e-05],\n",
            "          [1.0711e-04]],\n",
            "\n",
            "         [[6.0524e-05],\n",
            "          [4.1642e-05]]],\n",
            "\n",
            "\n",
            "        [[[7.2179e-05],\n",
            "          [1.2426e-04]],\n",
            "\n",
            "         [[1.5293e-04],\n",
            "          [1.2358e-04]],\n",
            "\n",
            "         [[3.4709e-04],\n",
            "          [3.9061e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[7.2505e-05],\n",
            "          [1.7204e-04]],\n",
            "\n",
            "         [[3.8153e-04],\n",
            "          [1.0455e-04]],\n",
            "\n",
            "         [[1.9373e-04],\n",
            "          [9.0004e-05]]]])}, 5: {'step': 8100, 'exp_avg': tensor([-2.6412e-10,  1.0789e-10,  9.2020e-11, -2.4736e-11, -3.5665e-11,\n",
            "        -9.5871e-12, -6.4779e-11, -3.2436e-11, -5.6961e-11, -2.5265e-10,\n",
            "         1.8988e-10,  4.5592e-10,  3.7254e-10,  7.0206e-11,  3.4323e-10,\n",
            "        -4.2053e-12, -1.3948e-10, -1.3663e-10, -1.3992e-10, -1.4765e-10,\n",
            "         2.6462e-12,  4.4000e-10, -3.6995e-11, -5.8513e-14, -4.3331e-10,\n",
            "         2.3212e-10, -5.9220e-11, -4.0272e-11, -6.8259e-10, -3.5059e-11,\n",
            "         7.7650e-11, -6.3729e-11]), 'exp_avg_sq': tensor([7.2351e-19, 5.2779e-19, 1.7032e-19, 1.8730e-19, 1.1573e-19, 6.2602e-19,\n",
            "        5.8623e-20, 1.2803e-19, 4.3482e-20, 3.2457e-18, 2.5442e-19, 3.1797e-18,\n",
            "        8.5709e-19, 2.0113e-19, 5.6154e-19, 7.7817e-19, 1.4762e-19, 6.9415e-19,\n",
            "        6.9749e-19, 3.0722e-19, 2.0582e-19, 4.0803e-18, 5.4942e-19, 2.6790e-18,\n",
            "        3.7204e-18, 2.2545e-18, 2.4566e-19, 3.3781e-19, 4.9899e-18, 1.0827e-18,\n",
            "        7.4849e-19, 1.0877e-18])}, 6: {'step': 8100, 'exp_avg': tensor([-1.8936e-03,  1.5655e-03,  9.5310e-04,  5.6611e-04,  3.8996e-04,\n",
            "         2.1640e-03, -3.0354e-04,  4.8895e-04, -4.6363e-04, -2.9637e-03,\n",
            "        -1.4715e-03, -6.1124e-03, -2.1493e-03,  6.4254e-04,  1.5721e-03,\n",
            "         2.0255e-03, -5.2563e-04, -1.5114e-03, -3.0514e-03, -6.4148e-04,\n",
            "        -8.3187e-04,  4.6529e-03, -2.3928e-03,  4.9878e-04, -6.8221e-03,\n",
            "         9.9785e-05, -1.3862e-03,  1.6542e-03,  5.0566e-03, -5.4222e-03,\n",
            "         3.9272e-03,  9.3374e-04]), 'exp_avg_sq': tensor([4.1573e-05, 2.0421e-05, 7.7825e-06, 1.6413e-05, 1.2646e-05, 5.5912e-05,\n",
            "        8.3829e-06, 6.2456e-06, 2.8639e-06, 7.2542e-05, 1.5075e-05, 1.0659e-04,\n",
            "        6.1085e-05, 7.8066e-06, 1.1891e-05, 5.0786e-05, 9.2045e-06, 2.4156e-05,\n",
            "        4.7356e-05, 1.5373e-05, 1.0938e-05, 1.0406e-04, 2.5394e-05, 8.1802e-05,\n",
            "        1.8937e-04, 4.7086e-05, 3.0585e-05, 1.2061e-05, 9.7243e-05, 6.2377e-05,\n",
            "        4.0126e-05, 4.5503e-05])}, 7: {'step': 8100, 'exp_avg': tensor([-3.4043e-03,  2.1459e-03,  1.4581e-03,  1.9398e-03,  1.2778e-03,\n",
            "         3.1748e-03,  1.1415e-03,  7.9678e-04,  1.9845e-04, -3.5453e-03,\n",
            "        -3.8656e-04, -6.4625e-03, -1.2065e-03,  1.2851e-03,  1.5438e-03,\n",
            "         4.9997e-03,  6.5924e-05, -2.2681e-03, -2.1919e-03, -5.2105e-04,\n",
            "        -1.5483e-04,  6.6795e-03, -2.9707e-03,  2.2153e-03, -6.9697e-03,\n",
            "         2.2958e-03, -1.2090e-03,  2.6941e-03,  7.2056e-03, -5.5653e-03,\n",
            "         4.0680e-03,  3.3452e-03]), 'exp_avg_sq': tensor([5.1951e-05, 1.5785e-05, 7.5587e-06, 1.2443e-05, 7.0650e-06, 5.3181e-05,\n",
            "        6.2203e-06, 6.6516e-06, 3.0312e-06, 1.0012e-04, 7.6299e-06, 1.1228e-04,\n",
            "        2.0272e-05, 9.4925e-06, 1.3176e-05, 5.6543e-05, 5.7243e-06, 2.0580e-05,\n",
            "        4.1493e-05, 5.5422e-06, 7.2407e-06, 8.8276e-05, 1.9857e-05, 6.0870e-05,\n",
            "        1.8599e-04, 3.8582e-05, 1.6774e-05, 1.5733e-05, 1.5787e-04, 4.9919e-05,\n",
            "        4.8358e-05, 4.8807e-05])}, 8: {'step': 8100, 'exp_avg': tensor([[[[ 8.9538e-04,  6.5182e-04,  1.0616e-03,  ..., -9.2285e-05,\n",
            "           -1.6458e-04, -1.7160e-03],\n",
            "          [ 1.6063e-03, -2.7350e-03,  2.6373e-03,  ...,  7.8245e-04,\n",
            "            5.2547e-04, -1.0227e-04],\n",
            "          [-1.0460e-03, -4.0834e-03, -1.0865e-03,  ...,  5.8211e-04,\n",
            "           -6.3087e-04,  1.5256e-04],\n",
            "          ...,\n",
            "          [ 9.8473e-04, -3.3943e-06,  2.3131e-03,  ...,  1.2788e-03,\n",
            "           -3.5422e-04,  4.6598e-04],\n",
            "          [ 4.3210e-04,  1.0314e-03,  3.8036e-03,  ...,  8.3062e-04,\n",
            "           -1.2962e-04, -1.0129e-03],\n",
            "          [-3.3587e-04,  6.4338e-04, -1.0233e-04,  ...,  4.3434e-05,\n",
            "            1.2158e-04, -2.6663e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 9.0406e-04,  9.3316e-04,  4.8632e-04,  ...,  5.4924e-04,\n",
            "            8.4873e-04, -4.2211e-04],\n",
            "          [-9.9737e-05, -8.1662e-04, -1.8228e-03,  ..., -8.1484e-05,\n",
            "            4.4311e-04,  2.4829e-04],\n",
            "          [ 1.8635e-03,  3.4545e-04,  5.0446e-04,  ..., -4.7573e-04,\n",
            "           -2.7759e-04,  3.8690e-04],\n",
            "          ...,\n",
            "          [ 1.2343e-03,  4.3102e-04,  1.2404e-03,  ..., -3.2800e-04,\n",
            "           -7.2052e-04,  1.4849e-04],\n",
            "          [ 1.0740e-03,  2.3392e-03,  5.2951e-05,  ...,  4.2034e-04,\n",
            "           -6.8953e-04,  1.2630e-04],\n",
            "          [ 1.4531e-03, -2.1368e-03,  5.2157e-03,  ...,  9.6890e-05,\n",
            "            1.7148e-03, -2.7599e-04]]],\n",
            "\n",
            "\n",
            "        [[[-3.4187e-04, -6.4263e-05, -9.2280e-04,  ...,  2.8139e-04,\n",
            "           -2.4218e-04,  8.0395e-04],\n",
            "          [-1.0222e-03,  3.7895e-03, -2.7767e-03,  ..., -2.0228e-04,\n",
            "           -1.1648e-03,  1.4024e-03],\n",
            "          [ 3.7821e-04,  2.1096e-03,  7.1350e-04,  ...,  3.9832e-04,\n",
            "           -6.4206e-04,  1.6499e-03],\n",
            "          ...,\n",
            "          [-1.2070e-03,  4.2944e-04, -1.5790e-03,  ...,  5.0108e-04,\n",
            "            1.4694e-04, -7.7547e-05],\n",
            "          [-1.4277e-03, -3.6799e-03, -8.3572e-05,  ...,  6.8640e-05,\n",
            "           -3.2308e-04, -7.4488e-05],\n",
            "          [-1.8868e-03, -4.1752e-03, -4.0839e-04,  ...,  2.2434e-04,\n",
            "            3.5516e-04,  2.0173e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.5417e-05,  1.5683e-04,  3.0237e-05,  ...,  5.8822e-05,\n",
            "            1.7874e-04,  4.2749e-05],\n",
            "          [-6.3820e-04, -8.8946e-04, -4.8034e-04,  ..., -2.3638e-04,\n",
            "            1.2446e-04, -2.2951e-04],\n",
            "          [-2.6448e-04, -5.4808e-04,  1.1237e-05,  ...,  1.4642e-04,\n",
            "            1.4201e-05,  4.1649e-04],\n",
            "          ...,\n",
            "          [-2.4292e-04, -3.8368e-04, -1.5161e-04,  ..., -1.2652e-04,\n",
            "           -3.4619e-04,  3.0332e-04],\n",
            "          [-6.5636e-04, -7.2491e-04, -8.3545e-04,  ...,  2.5489e-04,\n",
            "           -4.9881e-04, -5.2950e-06],\n",
            "          [-7.5971e-04, -2.0333e-04, -9.4029e-04,  ..., -7.9237e-05,\n",
            "           -9.3897e-04,  8.0101e-05]]],\n",
            "\n",
            "\n",
            "        [[[-8.0924e-04,  2.2864e-04, -1.2447e-03,  ..., -3.1787e-04,\n",
            "           -3.2733e-04,  7.3297e-04],\n",
            "          [ 7.3022e-05,  1.5516e-04, -4.4735e-04,  ..., -1.1860e-05,\n",
            "            9.9284e-05,  7.5190e-04],\n",
            "          [ 1.0641e-03,  1.9580e-03,  1.1527e-03,  ..., -3.4068e-04,\n",
            "           -6.0162e-04,  3.8094e-04],\n",
            "          ...,\n",
            "          [ 6.7935e-04, -9.3461e-04,  1.1412e-03,  ..., -1.0415e-03,\n",
            "           -4.2676e-04,  2.1193e-04],\n",
            "          [ 1.5156e-04,  9.5118e-04, -2.6664e-04,  ...,  7.4156e-04,\n",
            "            1.7855e-04, -6.1526e-04],\n",
            "          [-2.5067e-04, -5.1064e-04, -5.0832e-04,  ...,  1.4993e-04,\n",
            "           -3.2824e-04,  1.3466e-05]]],\n",
            "\n",
            "\n",
            "        [[[-3.6551e-04, -3.8398e-03, -1.0980e-03,  ..., -2.4594e-04,\n",
            "           -2.1488e-03,  5.9543e-04],\n",
            "          [ 4.4112e-04,  1.8200e-03, -1.1356e-03,  ...,  3.7047e-06,\n",
            "           -6.1760e-04,  4.4197e-05],\n",
            "          [ 1.8299e-03,  3.2430e-03,  1.0897e-03,  ..., -3.1743e-04,\n",
            "            5.9695e-05, -8.7016e-05],\n",
            "          ...,\n",
            "          [ 2.0568e-03,  5.1938e-03,  7.0898e-04,  ..., -9.2397e-05,\n",
            "            6.8042e-04,  1.5326e-04],\n",
            "          [ 1.6090e-03, -2.9014e-03,  2.2523e-03,  ..., -3.3646e-04,\n",
            "            2.1453e-04,  1.7534e-04],\n",
            "          [-3.8104e-04,  9.8626e-04, -7.7150e-04,  ..., -1.0705e-04,\n",
            "            1.6678e-04, -1.6262e-04]]]]), 'exp_avg_sq': tensor([[[[8.9514e-06, 2.6181e-05, 1.3122e-05,  ..., 4.6461e-06,\n",
            "           9.7105e-06, 1.2448e-05],\n",
            "          [1.7430e-05, 9.9309e-05, 4.7937e-05,  ..., 5.6565e-06,\n",
            "           1.0400e-05, 4.7820e-06],\n",
            "          [1.5488e-05, 7.6979e-05, 2.2627e-05,  ..., 4.8194e-06,\n",
            "           9.9633e-06, 1.4350e-05],\n",
            "          ...,\n",
            "          [1.7857e-05, 1.5901e-04, 4.7043e-05,  ..., 9.2057e-06,\n",
            "           3.2220e-06, 4.8314e-06],\n",
            "          [2.3559e-05, 1.0456e-04, 6.4258e-05,  ..., 7.6275e-06,\n",
            "           5.0639e-06, 8.2134e-06],\n",
            "          [1.2016e-05, 4.2860e-05, 1.8639e-05,  ..., 1.5605e-05,\n",
            "           5.9723e-06, 6.4979e-06]]],\n",
            "\n",
            "\n",
            "        [[[1.0896e-05, 2.0766e-05, 2.4928e-05,  ..., 5.1298e-06,\n",
            "           7.2011e-06, 5.1484e-06],\n",
            "          [1.3212e-05, 5.0650e-05, 2.5154e-05,  ..., 4.0239e-06,\n",
            "           9.0652e-06, 6.0224e-06],\n",
            "          [7.7649e-06, 6.0144e-05, 3.2658e-05,  ..., 8.7253e-06,\n",
            "           7.3368e-06, 3.5170e-06],\n",
            "          ...,\n",
            "          [1.0610e-05, 9.8625e-05, 3.7252e-05,  ..., 5.4225e-06,\n",
            "           6.8672e-06, 6.4567e-06],\n",
            "          [7.3715e-06, 2.0885e-05, 1.0510e-05,  ..., 7.3090e-06,\n",
            "           5.6579e-06, 5.1494e-06],\n",
            "          [2.0621e-05, 1.3624e-04, 6.8484e-05,  ..., 5.4395e-06,\n",
            "           8.9299e-06, 4.4464e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.6585e-06, 1.5223e-04, 3.0383e-05,  ..., 3.2227e-06,\n",
            "           7.7672e-06, 8.1742e-06],\n",
            "          [1.3754e-05, 7.2496e-05, 4.1775e-05,  ..., 4.6119e-06,\n",
            "           1.2934e-05, 1.1666e-05],\n",
            "          [1.0089e-05, 2.7372e-05, 2.4429e-05,  ..., 7.5589e-06,\n",
            "           1.1776e-05, 1.1233e-05],\n",
            "          ...,\n",
            "          [2.7020e-05, 3.0687e-05, 4.9072e-05,  ..., 1.0108e-05,\n",
            "           5.6774e-06, 6.3155e-06],\n",
            "          [2.1067e-05, 7.8699e-05, 3.0979e-05,  ..., 7.4078e-06,\n",
            "           7.3167e-06, 8.5106e-06],\n",
            "          [4.9219e-05, 1.3023e-04, 7.1411e-05,  ..., 8.3846e-06,\n",
            "           2.1823e-05, 6.9158e-06]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[3.7340e-06, 9.9591e-06, 4.9916e-06,  ..., 1.3254e-06,\n",
            "           1.4075e-06, 1.3336e-06],\n",
            "          [3.8054e-06, 6.2356e-06, 1.3144e-05,  ..., 1.4127e-06,\n",
            "           2.8213e-06, 2.2555e-06],\n",
            "          [4.2680e-06, 2.4094e-05, 8.9918e-06,  ..., 1.1788e-06,\n",
            "           2.2036e-06, 2.3013e-06],\n",
            "          ...,\n",
            "          [1.0617e-05, 4.3611e-05, 2.0283e-05,  ..., 3.5161e-06,\n",
            "           2.7492e-06, 2.1546e-06],\n",
            "          [4.0704e-06, 7.2490e-06, 5.9703e-06,  ..., 2.3373e-06,\n",
            "           1.0100e-06, 1.0761e-06],\n",
            "          [3.0972e-06, 2.0812e-05, 6.7701e-06,  ..., 1.0818e-06,\n",
            "           2.4021e-06, 2.7368e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.1903e-06, 4.1060e-05, 2.2791e-05,  ..., 3.2646e-06,\n",
            "           5.4089e-06, 4.8495e-06],\n",
            "          [6.9855e-06, 1.3931e-05, 1.8848e-05,  ..., 2.4269e-06,\n",
            "           2.8603e-06, 4.5939e-06],\n",
            "          [1.1954e-05, 4.0525e-05, 1.9083e-05,  ..., 6.5616e-06,\n",
            "           5.0465e-06, 4.2234e-06],\n",
            "          ...,\n",
            "          [2.1777e-05, 8.4084e-05, 5.5408e-05,  ..., 8.0151e-06,\n",
            "           4.4740e-06, 4.0197e-06],\n",
            "          [1.2434e-05, 2.0821e-05, 2.0376e-05,  ..., 5.6585e-06,\n",
            "           2.9772e-06, 4.0726e-06],\n",
            "          [1.0060e-05, 1.9546e-05, 1.7385e-05,  ..., 2.4228e-06,\n",
            "           4.4158e-06, 4.6292e-06]]],\n",
            "\n",
            "\n",
            "        [[[1.1433e-05, 7.1974e-05, 1.7793e-05,  ..., 4.0719e-06,\n",
            "           8.9254e-06, 4.9746e-06],\n",
            "          [7.4039e-06, 2.2327e-05, 1.3628e-05,  ..., 2.9936e-06,\n",
            "           6.1604e-06, 6.8948e-06],\n",
            "          [9.2197e-06, 4.4741e-05, 2.1785e-05,  ..., 4.0221e-06,\n",
            "           5.2751e-06, 4.8203e-06],\n",
            "          ...,\n",
            "          [1.0118e-05, 1.4069e-04, 4.5375e-05,  ..., 4.6942e-06,\n",
            "           6.6353e-06, 5.6445e-06],\n",
            "          [9.2437e-06, 1.5568e-04, 3.9428e-05,  ..., 8.8580e-06,\n",
            "           4.3043e-06, 4.7663e-06],\n",
            "          [1.0464e-05, 2.0555e-05, 1.9573e-05,  ..., 9.9760e-06,\n",
            "           3.5440e-06, 3.2181e-06]]]])}, 9: {'step': 8100, 'exp_avg': tensor([-3.5472e-05, -3.9919e-06,  4.8319e-05, -7.8228e-05,  3.8669e-06,\n",
            "        -1.5824e-05, -7.4206e-05,  9.9378e-05, -9.1940e-05, -2.1466e-04,\n",
            "         9.4662e-07, -1.8380e-05,  4.8411e-05,  1.2732e-04,  6.8456e-05,\n",
            "         4.1808e-05]), 'exp_avg_sq': tensor([4.8360e-08, 4.4067e-08, 8.6008e-08, 2.1023e-07, 1.8090e-07, 3.1408e-07,\n",
            "        9.4342e-08, 1.1375e-07, 2.3297e-07, 3.2058e-07, 1.6303e-07, 3.4847e-08,\n",
            "        1.7774e-07, 1.0935e-07, 3.0807e-07, 6.2492e-08])}, 12: {'step': 8100, 'exp_avg': tensor([[[[-5.4819e-06],\n",
            "          [ 1.5077e-04],\n",
            "          [-9.6951e-04],\n",
            "          ...,\n",
            "          [ 4.5117e-04],\n",
            "          [ 3.5953e-04],\n",
            "          [-3.0983e-04]],\n",
            "\n",
            "         [[ 1.7811e-04],\n",
            "          [-3.1471e-04],\n",
            "          [-4.9041e-04],\n",
            "          ...,\n",
            "          [ 2.9137e-04],\n",
            "          [-3.3155e-04],\n",
            "          [-6.6466e-05]],\n",
            "\n",
            "         [[-2.7872e-04],\n",
            "          [-2.7352e-04],\n",
            "          [ 2.4304e-05],\n",
            "          ...,\n",
            "          [-1.0261e-04],\n",
            "          [-3.9405e-04],\n",
            "          [ 2.7600e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2741e-05],\n",
            "          [ 3.7062e-04],\n",
            "          [ 3.9161e-04],\n",
            "          ...,\n",
            "          [ 3.0546e-04],\n",
            "          [ 3.6787e-05],\n",
            "          [ 6.5303e-05]],\n",
            "\n",
            "         [[ 5.1320e-04],\n",
            "          [-8.4866e-05],\n",
            "          [ 1.6590e-05],\n",
            "          ...,\n",
            "          [-1.3791e-04],\n",
            "          [ 1.8534e-04],\n",
            "          [ 5.5338e-04]],\n",
            "\n",
            "         [[ 5.9166e-04],\n",
            "          [ 3.0262e-04],\n",
            "          [-5.6939e-04],\n",
            "          ...,\n",
            "          [-9.2490e-04],\n",
            "          [-6.6926e-05],\n",
            "          [ 3.2965e-04]]],\n",
            "\n",
            "\n",
            "        [[[-3.8924e-04],\n",
            "          [ 1.2230e-03],\n",
            "          [-2.0020e-04],\n",
            "          ...,\n",
            "          [-2.8780e-04],\n",
            "          [-6.0154e-04],\n",
            "          [-2.3987e-05]],\n",
            "\n",
            "         [[ 3.4886e-04],\n",
            "          [-5.3449e-04],\n",
            "          [-5.2376e-04],\n",
            "          ...,\n",
            "          [-2.7274e-04],\n",
            "          [ 2.0429e-04],\n",
            "          [ 7.3326e-04]],\n",
            "\n",
            "         [[-1.0965e-03],\n",
            "          [-3.7163e-05],\n",
            "          [ 9.2625e-05],\n",
            "          ...,\n",
            "          [-8.6951e-05],\n",
            "          [-8.0484e-04],\n",
            "          [ 5.0035e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.3365e-05],\n",
            "          [ 5.6469e-04],\n",
            "          [ 5.3569e-04],\n",
            "          ...,\n",
            "          [ 3.1241e-04],\n",
            "          [-1.4016e-05],\n",
            "          [-6.5251e-04]],\n",
            "\n",
            "         [[ 4.2586e-04],\n",
            "          [ 1.9988e-04],\n",
            "          [-2.6233e-05],\n",
            "          ...,\n",
            "          [-7.1282e-05],\n",
            "          [-6.8596e-05],\n",
            "          [-1.1661e-03]],\n",
            "\n",
            "         [[ 7.4846e-06],\n",
            "          [ 4.2781e-04],\n",
            "          [ 4.4368e-04],\n",
            "          ...,\n",
            "          [-3.8304e-04],\n",
            "          [ 8.2943e-04],\n",
            "          [ 4.7598e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4424e-04],\n",
            "          [-5.0360e-04],\n",
            "          [ 8.4099e-04],\n",
            "          ...,\n",
            "          [ 5.6444e-04],\n",
            "          [-5.8803e-04],\n",
            "          [ 1.8504e-03]],\n",
            "\n",
            "         [[-9.4659e-05],\n",
            "          [-2.0611e-04],\n",
            "          [-1.6612e-04],\n",
            "          ...,\n",
            "          [ 2.2289e-04],\n",
            "          [-6.5530e-04],\n",
            "          [-3.4046e-04]],\n",
            "\n",
            "         [[ 1.2034e-03],\n",
            "          [-3.3251e-04],\n",
            "          [ 6.0455e-04],\n",
            "          ...,\n",
            "          [-6.3371e-04],\n",
            "          [ 6.1700e-07],\n",
            "          [ 2.8879e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.4496e-04],\n",
            "          [-3.6428e-04],\n",
            "          [-2.5795e-04],\n",
            "          ...,\n",
            "          [ 5.9749e-04],\n",
            "          [ 1.6281e-04],\n",
            "          [-5.3728e-04]],\n",
            "\n",
            "         [[-3.8555e-05],\n",
            "          [ 1.1380e-04],\n",
            "          [ 1.2125e-03],\n",
            "          ...,\n",
            "          [ 2.2825e-04],\n",
            "          [ 7.7381e-04],\n",
            "          [-1.0541e-03]],\n",
            "\n",
            "         [[ 3.9070e-04],\n",
            "          [ 2.1295e-04],\n",
            "          [-4.2879e-05],\n",
            "          ...,\n",
            "          [ 9.3227e-04],\n",
            "          [ 6.3686e-05],\n",
            "          [-3.3971e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.4009e-04],\n",
            "          [-8.4599e-04],\n",
            "          [-3.3726e-05],\n",
            "          ...,\n",
            "          [ 1.7543e-04],\n",
            "          [-6.6760e-04],\n",
            "          [-3.7251e-05]],\n",
            "\n",
            "         [[ 1.7911e-04],\n",
            "          [-7.4115e-04],\n",
            "          [ 6.6424e-04],\n",
            "          ...,\n",
            "          [-5.2400e-04],\n",
            "          [ 2.8183e-05],\n",
            "          [ 4.4821e-04]],\n",
            "\n",
            "         [[-3.3975e-04],\n",
            "          [ 4.2915e-05],\n",
            "          [-2.3976e-04],\n",
            "          ...,\n",
            "          [-2.7436e-04],\n",
            "          [ 5.5472e-04],\n",
            "          [ 6.6844e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.6062e-06],\n",
            "          [ 8.0495e-04],\n",
            "          [ 1.1265e-05],\n",
            "          ...,\n",
            "          [ 4.8325e-05],\n",
            "          [ 8.2351e-05],\n",
            "          [ 1.7395e-04]],\n",
            "\n",
            "         [[ 3.5251e-04],\n",
            "          [ 5.2138e-05],\n",
            "          [-4.8099e-04],\n",
            "          ...,\n",
            "          [-2.7913e-05],\n",
            "          [-7.4951e-04],\n",
            "          [-2.6217e-04]],\n",
            "\n",
            "         [[-2.7609e-04],\n",
            "          [-1.6377e-04],\n",
            "          [-3.8417e-04],\n",
            "          ...,\n",
            "          [-5.5368e-04],\n",
            "          [-7.7618e-04],\n",
            "          [ 5.4002e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2109e-04],\n",
            "          [ 4.1466e-04],\n",
            "          [-1.4635e-03],\n",
            "          ...,\n",
            "          [ 2.8668e-04],\n",
            "          [ 1.7176e-03],\n",
            "          [-1.1607e-03]],\n",
            "\n",
            "         [[-9.5557e-04],\n",
            "          [-2.0250e-04],\n",
            "          [-9.3569e-04],\n",
            "          ...,\n",
            "          [-1.8885e-04],\n",
            "          [ 4.0963e-04],\n",
            "          [-2.7853e-04]],\n",
            "\n",
            "         [[-7.0272e-04],\n",
            "          [ 9.8901e-04],\n",
            "          [-7.7036e-04],\n",
            "          ...,\n",
            "          [ 1.5496e-04],\n",
            "          [-5.0214e-04],\n",
            "          [-4.4909e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.6925e-04],\n",
            "          [ 8.9230e-04],\n",
            "          [ 1.5222e-04],\n",
            "          ...,\n",
            "          [-1.2139e-04],\n",
            "          [ 1.7315e-04],\n",
            "          [-6.4583e-04]],\n",
            "\n",
            "         [[ 5.6907e-04],\n",
            "          [-1.2944e-04],\n",
            "          [-4.8975e-04],\n",
            "          ...,\n",
            "          [ 3.2427e-04],\n",
            "          [-4.0539e-04],\n",
            "          [ 2.3316e-04]],\n",
            "\n",
            "         [[ 8.5147e-04],\n",
            "          [ 2.5219e-04],\n",
            "          [-1.1186e-03],\n",
            "          ...,\n",
            "          [-6.5602e-04],\n",
            "          [ 1.7470e-03],\n",
            "          [ 5.3386e-05]]],\n",
            "\n",
            "\n",
            "        [[[-6.9431e-04],\n",
            "          [ 7.7861e-04],\n",
            "          [ 2.9709e-04],\n",
            "          ...,\n",
            "          [-8.2856e-04],\n",
            "          [ 9.2735e-04],\n",
            "          [ 6.0560e-04]],\n",
            "\n",
            "         [[-4.0976e-05],\n",
            "          [ 4.3979e-04],\n",
            "          [ 3.0791e-05],\n",
            "          ...,\n",
            "          [-3.3949e-04],\n",
            "          [-4.0518e-05],\n",
            "          [-7.8793e-05]],\n",
            "\n",
            "         [[-1.9311e-05],\n",
            "          [-1.7680e-04],\n",
            "          [-2.5333e-04],\n",
            "          ...,\n",
            "          [-8.5041e-04],\n",
            "          [ 7.5565e-04],\n",
            "          [-1.8034e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5718e-04],\n",
            "          [-5.1634e-04],\n",
            "          [-4.3258e-05],\n",
            "          ...,\n",
            "          [ 1.7651e-04],\n",
            "          [ 1.8765e-04],\n",
            "          [ 1.5530e-04]],\n",
            "\n",
            "         [[ 2.1152e-04],\n",
            "          [ 5.5683e-04],\n",
            "          [ 1.6315e-04],\n",
            "          ...,\n",
            "          [ 6.8687e-04],\n",
            "          [-2.1526e-04],\n",
            "          [ 2.0857e-04]],\n",
            "\n",
            "         [[-5.6194e-04],\n",
            "          [ 7.6163e-04],\n",
            "          [ 7.2578e-04],\n",
            "          ...,\n",
            "          [-4.3170e-04],\n",
            "          [-7.2728e-04],\n",
            "          [ 4.9535e-04]]]]), 'exp_avg_sq': tensor([[[[3.2123e-06],\n",
            "          [2.7450e-06],\n",
            "          [5.8564e-06],\n",
            "          ...,\n",
            "          [4.2303e-06],\n",
            "          [2.7422e-06],\n",
            "          [6.5835e-06]],\n",
            "\n",
            "         [[1.9566e-06],\n",
            "          [2.2903e-06],\n",
            "          [1.8654e-06],\n",
            "          ...,\n",
            "          [3.6384e-06],\n",
            "          [3.0870e-06],\n",
            "          [1.1974e-06]],\n",
            "\n",
            "         [[2.4485e-06],\n",
            "          [3.6370e-06],\n",
            "          [2.2721e-06],\n",
            "          ...,\n",
            "          [3.3807e-06],\n",
            "          [6.0475e-06],\n",
            "          [2.4047e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[8.6586e-07],\n",
            "          [1.4753e-06],\n",
            "          [1.0661e-06],\n",
            "          ...,\n",
            "          [1.3031e-06],\n",
            "          [1.4763e-06],\n",
            "          [1.5371e-06]],\n",
            "\n",
            "         [[1.1106e-06],\n",
            "          [1.4623e-06],\n",
            "          [8.8370e-07],\n",
            "          ...,\n",
            "          [1.7234e-06],\n",
            "          [2.2201e-06],\n",
            "          [5.4494e-06]],\n",
            "\n",
            "         [[1.6747e-06],\n",
            "          [2.0973e-06],\n",
            "          [2.1973e-06],\n",
            "          ...,\n",
            "          [3.6785e-06],\n",
            "          [3.0626e-06],\n",
            "          [7.4793e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.7755e-07],\n",
            "          [7.5882e-06],\n",
            "          [1.7335e-06],\n",
            "          ...,\n",
            "          [1.1058e-06],\n",
            "          [3.4736e-06],\n",
            "          [1.2628e-06]],\n",
            "\n",
            "         [[7.7460e-07],\n",
            "          [4.5431e-06],\n",
            "          [3.0960e-06],\n",
            "          ...,\n",
            "          [2.5620e-06],\n",
            "          [8.3327e-07],\n",
            "          [2.9689e-06]],\n",
            "\n",
            "         [[5.5274e-06],\n",
            "          [1.1556e-06],\n",
            "          [1.6862e-06],\n",
            "          ...,\n",
            "          [3.0225e-06],\n",
            "          [7.0212e-06],\n",
            "          [2.5778e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.6404e-06],\n",
            "          [1.4949e-06],\n",
            "          [2.4103e-06],\n",
            "          ...,\n",
            "          [1.6141e-06],\n",
            "          [1.5437e-06],\n",
            "          [1.8051e-06]],\n",
            "\n",
            "         [[9.4046e-07],\n",
            "          [3.3375e-07],\n",
            "          [5.4191e-07],\n",
            "          ...,\n",
            "          [6.6488e-07],\n",
            "          [2.1216e-06],\n",
            "          [5.6812e-06]],\n",
            "\n",
            "         [[4.1871e-07],\n",
            "          [1.1518e-06],\n",
            "          [2.2340e-06],\n",
            "          ...,\n",
            "          [2.1534e-06],\n",
            "          [5.3505e-06],\n",
            "          [2.4057e-06]]],\n",
            "\n",
            "\n",
            "        [[[8.7237e-06],\n",
            "          [5.1477e-06],\n",
            "          [6.8760e-06],\n",
            "          ...,\n",
            "          [6.3617e-06],\n",
            "          [9.2253e-06],\n",
            "          [1.3071e-05]],\n",
            "\n",
            "         [[4.2554e-06],\n",
            "          [3.3677e-06],\n",
            "          [4.2443e-06],\n",
            "          ...,\n",
            "          [4.8353e-06],\n",
            "          [9.0654e-06],\n",
            "          [3.4363e-06]],\n",
            "\n",
            "         [[8.6938e-06],\n",
            "          [7.9315e-06],\n",
            "          [6.6245e-06],\n",
            "          ...,\n",
            "          [8.0432e-06],\n",
            "          [7.8980e-06],\n",
            "          [6.1812e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[6.6566e-06],\n",
            "          [2.4287e-06],\n",
            "          [2.5142e-06],\n",
            "          ...,\n",
            "          [1.7393e-05],\n",
            "          [2.1561e-06],\n",
            "          [4.3683e-06]],\n",
            "\n",
            "         [[1.5338e-06],\n",
            "          [2.5674e-06],\n",
            "          [8.6712e-06],\n",
            "          ...,\n",
            "          [3.1345e-06],\n",
            "          [6.2503e-06],\n",
            "          [5.3464e-06]],\n",
            "\n",
            "         [[4.2497e-06],\n",
            "          [3.9348e-06],\n",
            "          [3.2960e-06],\n",
            "          ...,\n",
            "          [9.2169e-06],\n",
            "          [8.0406e-06],\n",
            "          [1.0411e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[4.1198e-06],\n",
            "          [6.6711e-06],\n",
            "          [2.1400e-06],\n",
            "          ...,\n",
            "          [4.5769e-06],\n",
            "          [5.1406e-06],\n",
            "          [1.8987e-06]],\n",
            "\n",
            "         [[2.8416e-06],\n",
            "          [6.2454e-06],\n",
            "          [4.9703e-06],\n",
            "          ...,\n",
            "          [7.6557e-06],\n",
            "          [1.1057e-06],\n",
            "          [3.5567e-06]],\n",
            "\n",
            "         [[4.1928e-06],\n",
            "          [1.3505e-06],\n",
            "          [4.2498e-06],\n",
            "          ...,\n",
            "          [1.6303e-05],\n",
            "          [3.3001e-06],\n",
            "          [4.9202e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.1005e-06],\n",
            "          [3.0460e-06],\n",
            "          [3.1098e-06],\n",
            "          ...,\n",
            "          [3.8747e-06],\n",
            "          [7.0625e-07],\n",
            "          [2.6259e-06]],\n",
            "\n",
            "         [[1.8645e-06],\n",
            "          [8.3968e-07],\n",
            "          [1.1395e-06],\n",
            "          ...,\n",
            "          [1.1798e-06],\n",
            "          [3.7829e-06],\n",
            "          [2.5971e-06]],\n",
            "\n",
            "         [[1.4270e-06],\n",
            "          [2.2330e-06],\n",
            "          [1.6842e-06],\n",
            "          ...,\n",
            "          [3.4985e-06],\n",
            "          [8.8660e-06],\n",
            "          [3.8602e-06]]],\n",
            "\n",
            "\n",
            "        [[[2.6080e-06],\n",
            "          [4.1796e-06],\n",
            "          [1.0995e-05],\n",
            "          ...,\n",
            "          [5.4587e-06],\n",
            "          [8.6775e-06],\n",
            "          [1.7161e-05]],\n",
            "\n",
            "         [[5.6615e-06],\n",
            "          [3.7075e-06],\n",
            "          [6.0222e-06],\n",
            "          ...,\n",
            "          [6.5382e-06],\n",
            "          [3.8460e-06],\n",
            "          [3.3992e-06]],\n",
            "\n",
            "         [[5.6605e-06],\n",
            "          [7.6464e-06],\n",
            "          [3.2051e-06],\n",
            "          ...,\n",
            "          [5.3449e-06],\n",
            "          [5.3462e-06],\n",
            "          [7.5302e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[2.7089e-06],\n",
            "          [3.2214e-06],\n",
            "          [1.9572e-06],\n",
            "          ...,\n",
            "          [2.0395e-06],\n",
            "          [2.0094e-06],\n",
            "          [3.9898e-06]],\n",
            "\n",
            "         [[2.4471e-06],\n",
            "          [2.8965e-06],\n",
            "          [2.3261e-06],\n",
            "          ...,\n",
            "          [1.8316e-06],\n",
            "          [3.3141e-06],\n",
            "          [3.9143e-06]],\n",
            "\n",
            "         [[4.3079e-06],\n",
            "          [2.5431e-06],\n",
            "          [6.6379e-06],\n",
            "          ...,\n",
            "          [3.9061e-06],\n",
            "          [8.1914e-06],\n",
            "          [6.3552e-06]]],\n",
            "\n",
            "\n",
            "        [[[5.0007e-06],\n",
            "          [2.9641e-06],\n",
            "          [3.5847e-06],\n",
            "          ...,\n",
            "          [3.5562e-06],\n",
            "          [5.3491e-06],\n",
            "          [5.4579e-06]],\n",
            "\n",
            "         [[2.6079e-06],\n",
            "          [2.2458e-06],\n",
            "          [1.8805e-06],\n",
            "          ...,\n",
            "          [1.2936e-06],\n",
            "          [1.5294e-06],\n",
            "          [2.1665e-06]],\n",
            "\n",
            "         [[2.5647e-06],\n",
            "          [3.7707e-06],\n",
            "          [2.0648e-06],\n",
            "          ...,\n",
            "          [2.0270e-06],\n",
            "          [5.1172e-06],\n",
            "          [6.5231e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.2610e-06],\n",
            "          [1.5841e-06],\n",
            "          [1.5510e-06],\n",
            "          ...,\n",
            "          [2.4083e-06],\n",
            "          [1.1567e-06],\n",
            "          [6.4278e-07]],\n",
            "\n",
            "         [[9.0264e-07],\n",
            "          [1.7480e-06],\n",
            "          [6.5717e-07],\n",
            "          ...,\n",
            "          [2.7449e-06],\n",
            "          [3.6347e-06],\n",
            "          [3.9592e-06]],\n",
            "\n",
            "         [[3.0472e-06],\n",
            "          [1.9351e-06],\n",
            "          [4.3509e-06],\n",
            "          ...,\n",
            "          [2.8559e-06],\n",
            "          [5.4718e-06],\n",
            "          [2.6461e-06]]]])}, 13: {'step': 8100, 'exp_avg': tensor([-2.9755e-11,  1.0616e-11,  5.6853e-11, -2.9304e-11, -9.6308e-11,\n",
            "         2.6700e-11, -1.5475e-10, -2.9917e-11,  2.7840e-11, -5.6429e-11,\n",
            "         1.5356e-11,  2.7952e-11, -6.7596e-11, -3.7354e-11,  2.8381e-11,\n",
            "         2.0129e-10, -3.9265e-13,  4.9659e-11,  3.8814e-11,  1.4366e-12,\n",
            "         5.5876e-11, -8.3454e-12,  2.5442e-11,  7.8127e-12, -2.5962e-11,\n",
            "        -1.0790e-12,  5.2754e-11,  2.8973e-11,  1.2904e-11, -5.8411e-11,\n",
            "        -2.9050e-11,  2.6298e-11]), 'exp_avg_sq': tensor([2.1708e-20, 7.0271e-20, 5.4154e-20, 2.0215e-20, 6.1007e-20, 9.6900e-21,\n",
            "        6.0713e-20, 1.7602e-20, 1.1565e-20, 2.4557e-20, 6.1481e-21, 1.2347e-20,\n",
            "        1.4177e-19, 1.5109e-20, 6.8975e-21, 5.6963e-19, 1.9431e-20, 1.9452e-20,\n",
            "        6.4852e-20, 1.3140e-19, 1.5765e-19, 1.5605e-20, 2.4271e-20, 2.3197e-20,\n",
            "        1.1281e-20, 1.1943e-19, 2.9574e-20, 1.6171e-20, 2.7185e-20, 6.6863e-20,\n",
            "        2.3416e-20, 3.1777e-20])}, 14: {'step': 8100, 'exp_avg': tensor([-6.7221e-05, -1.9659e-03,  4.3067e-04, -7.9770e-04, -6.6724e-05,\n",
            "         4.8924e-07, -7.7328e-04,  1.9368e-04,  9.4431e-05,  4.6645e-04,\n",
            "         3.9259e-05, -1.1548e-04,  1.0629e-03, -3.1677e-04,  6.9094e-06,\n",
            "         5.8289e-04,  7.5167e-05, -3.8752e-04, -5.7731e-06, -7.8681e-05,\n",
            "        -1.6890e-03, -1.1281e-04, -1.2384e-04,  2.3835e-04, -4.2526e-04,\n",
            "         7.2648e-04,  4.9529e-04, -2.4786e-04,  1.2740e-04, -1.4077e-03,\n",
            "        -2.1292e-05, -1.9059e-05]), 'exp_avg_sq': tensor([1.0023e-06, 1.2828e-05, 1.3723e-06, 4.6621e-06, 4.8678e-06, 7.6857e-07,\n",
            "        5.8384e-06, 3.5449e-06, 3.2402e-07, 7.6184e-07, 3.3023e-07, 5.4046e-07,\n",
            "        4.3199e-06, 5.3649e-07, 3.2610e-07, 1.3727e-05, 1.2946e-06, 1.0763e-06,\n",
            "        1.0715e-06, 2.3302e-06, 1.8646e-05, 1.1348e-06, 2.5762e-06, 9.9206e-07,\n",
            "        8.1631e-07, 3.7102e-06, 1.9937e-06, 6.4505e-07, 7.9662e-07, 9.1385e-06,\n",
            "        6.9070e-07, 1.6271e-06])}, 15: {'step': 8100, 'exp_avg': tensor([-2.2682e-05, -1.3953e-03,  4.2320e-04, -3.1788e-04,  1.4818e-04,\n",
            "        -8.9054e-05, -1.0737e-03,  2.5154e-04,  2.2450e-04,  4.0076e-04,\n",
            "         1.7470e-04, -1.5386e-04,  8.3557e-04, -1.0485e-04,  1.7056e-04,\n",
            "         7.3569e-04,  4.4053e-04, -6.9064e-05,  2.7781e-04,  1.0613e-04,\n",
            "        -1.3178e-03, -2.1543e-04, -8.3154e-05,  4.6812e-04, -3.3468e-04,\n",
            "         7.9652e-04,  4.2591e-04, -3.2435e-04, -5.7951e-06, -1.1207e-03,\n",
            "         3.6924e-05,  1.1718e-04]), 'exp_avg_sq': tensor([9.3474e-07, 6.6418e-06, 1.7549e-06, 1.2768e-06, 9.1421e-07, 6.6116e-07,\n",
            "        5.4174e-06, 8.4195e-07, 7.0300e-07, 8.4524e-07, 3.0914e-07, 8.3216e-07,\n",
            "        2.4884e-06, 7.5677e-07, 3.6831e-07, 7.4859e-06, 8.0618e-07, 9.7354e-07,\n",
            "        2.9902e-06, 1.4770e-06, 1.2817e-05, 1.7602e-06, 1.5981e-06, 8.0383e-07,\n",
            "        6.8786e-07, 5.4276e-06, 1.2887e-06, 1.0183e-06, 1.1649e-06, 6.6560e-06,\n",
            "        4.9433e-07, 1.5571e-06])}, 16: {'step': 8100, 'exp_avg': tensor([[ 1.3093e-04, -7.4676e-04, -1.2488e-03,  ..., -2.5546e-04,\n",
            "         -1.0503e-04, -2.4033e-04],\n",
            "        [-1.0651e-04,  5.1511e-04,  5.3354e-04,  ...,  6.1711e-05,\n",
            "          1.9304e-04,  7.1226e-05],\n",
            "        [-1.3785e-04, -1.0225e-03, -1.8771e-03,  ..., -2.9358e-04,\n",
            "         -6.3273e-04, -2.7118e-04],\n",
            "        ...,\n",
            "        [ 3.8345e-04,  2.3662e-06,  6.6228e-05,  ..., -1.8946e-06,\n",
            "          4.5187e-12, -1.6235e-07],\n",
            "        [ 2.9805e-05, -4.4260e-05, -9.9853e-05,  ...,  1.1830e-04,\n",
            "         -2.9680e-04,  2.5570e-04],\n",
            "        [-4.1777e-04, -1.5370e-04, -5.5178e-05,  ..., -1.6225e-04,\n",
            "         -7.1201e-06, -4.0276e-04]]), 'exp_avg_sq': tensor([[2.7326e-06, 3.2547e-06, 2.2837e-06,  ..., 4.5791e-07, 2.4969e-07,\n",
            "         1.7255e-06],\n",
            "        [7.9287e-07, 8.4903e-07, 8.0003e-07,  ..., 1.5618e-07, 2.5759e-07,\n",
            "         4.5322e-07],\n",
            "        [5.5563e-06, 7.7062e-06, 5.3184e-06,  ..., 9.2246e-07, 1.4328e-06,\n",
            "         1.8001e-06],\n",
            "        ...,\n",
            "        [5.7567e-07, 3.9496e-07, 2.5649e-07,  ..., 1.1341e-08, 1.7525e-10,\n",
            "         1.7730e-08],\n",
            "        [2.4015e-06, 2.7439e-06, 2.1505e-06,  ..., 5.6164e-07, 1.1423e-06,\n",
            "         1.8565e-06],\n",
            "        [2.8211e-06, 1.7736e-06, 2.0173e-06,  ..., 5.2122e-07, 1.9425e-07,\n",
            "         1.3854e-06]])}, 17: {'step': 8100, 'exp_avg': tensor([-1.0385e-03,  5.9058e-04, -1.2449e-03,  7.9391e-04, -1.8822e-04,\n",
            "        -9.6873e-04,  8.5146e-04, -5.6909e-04, -9.6366e-04, -3.8340e-04,\n",
            "        -1.0602e-03,  2.2224e-04,  9.5568e-04, -2.1381e-04,  5.1112e-05,\n",
            "        -8.6201e-04, -1.1500e-03, -1.0971e-03,  4.1176e-04,  4.9359e-06,\n",
            "        -9.8189e-05,  9.0425e-04,  1.0382e-03, -5.3765e-04, -5.1987e-04,\n",
            "         6.3761e-04, -2.9972e-04, -5.3947e-04, -5.0801e-04, -3.9737e-05,\n",
            "         6.3455e-04, -1.3861e-04, -1.4172e-03,  5.3897e-05, -2.0060e-04,\n",
            "         6.3135e-04,  5.1727e-04, -8.0025e-04, -9.7672e-04,  4.7123e-04,\n",
            "        -4.7079e-04, -8.9871e-04, -1.2011e-03,  6.1531e-04,  9.0685e-04,\n",
            "        -9.9990e-04,  1.5009e-05, -6.3972e-04,  4.4559e-04,  2.4884e-05,\n",
            "        -4.0781e-04,  5.7689e-04,  1.0099e-03, -1.0911e-03,  1.6302e-04,\n",
            "         5.6411e-04,  8.7330e-04,  5.4903e-04, -9.7333e-04, -3.3704e-04,\n",
            "         8.3308e-04,  9.2035e-05,  3.0969e-04,  1.1642e-04,  4.1872e-04,\n",
            "        -1.2226e-03, -8.5237e-04, -7.7500e-04,  7.1258e-04, -1.4873e-03,\n",
            "        -5.8027e-04,  1.0057e-04, -3.4590e-04,  8.1212e-04,  7.6969e-04,\n",
            "         2.7738e-04, -4.8183e-04, -9.4223e-04, -1.1548e-03, -6.3784e-04,\n",
            "        -9.8104e-04,  3.3830e-04, -6.8493e-04,  3.3873e-04, -2.6141e-04,\n",
            "        -3.2759e-04, -5.9599e-04,  6.0822e-07,  3.9330e-04, -5.0285e-04,\n",
            "        -7.9543e-04, -9.0069e-04, -5.0953e-04, -4.8141e-04,  8.8944e-04,\n",
            "        -2.2231e-04, -5.2838e-04, -2.1481e-04, -1.1865e-04, -6.5829e-04,\n",
            "        -1.3045e-03,  6.3730e-04, -2.3718e-04,  2.4496e-04,  5.1636e-04,\n",
            "         8.4147e-04,  9.8981e-04,  9.9363e-05,  4.5797e-04,  5.6258e-04,\n",
            "        -3.3737e-04,  1.0458e-03,  5.9135e-04, -6.1956e-04,  7.0424e-04,\n",
            "         8.0617e-04,  5.7152e-04,  2.9440e-04, -6.3746e-04, -5.7061e-04,\n",
            "         1.7331e-05, -8.7777e-04, -1.0061e-03,  6.1675e-04,  1.1205e-04,\n",
            "         2.1818e-04, -4.6200e-04,  3.5239e-04]), 'exp_avg_sq': tensor([3.2541e-06, 1.1474e-06, 6.6700e-06, 5.0523e-06, 8.8998e-08, 5.4796e-06,\n",
            "        4.3032e-06, 5.6292e-07, 5.2913e-06, 1.1324e-06, 3.4827e-06, 1.8449e-07,\n",
            "        3.7671e-06, 6.9242e-07, 1.4696e-06, 3.3252e-06, 2.2145e-06, 4.8383e-06,\n",
            "        2.4914e-06, 2.9188e-10, 6.3204e-07, 2.3671e-06, 3.3016e-06, 3.1707e-06,\n",
            "        2.7733e-06, 2.5276e-06, 6.2907e-07, 2.2254e-06, 5.5025e-06, 1.2211e-06,\n",
            "        3.1417e-06, 2.2604e-06, 6.7986e-06, 3.1722e-06, 2.1773e-06, 1.6821e-06,\n",
            "        5.7360e-06, 4.5623e-06, 2.2058e-06, 2.0033e-06, 5.0031e-06, 4.8074e-06,\n",
            "        3.4338e-06, 3.7425e-06, 4.4128e-06, 5.6214e-06, 2.5740e-09, 2.2884e-06,\n",
            "        4.9690e-07, 2.5650e-09, 3.3317e-06, 2.5095e-06, 3.3033e-06, 3.7236e-06,\n",
            "        2.2893e-06, 5.9710e-06, 1.8160e-06, 2.2231e-06, 2.2595e-06, 2.2875e-06,\n",
            "        3.3657e-06, 2.6045e-08, 6.7677e-06, 3.6499e-08, 2.6745e-06, 5.4704e-06,\n",
            "        2.2991e-06, 4.1017e-06, 3.5921e-06, 4.7968e-06, 2.3606e-06, 1.4455e-07,\n",
            "        1.1846e-06, 2.1563e-06, 2.0480e-06, 3.2672e-07, 1.6892e-06, 3.3526e-06,\n",
            "        4.8215e-06, 2.3013e-06, 3.6803e-06, 9.4193e-07, 3.3408e-06, 1.0498e-06,\n",
            "        1.6107e-06, 1.6855e-06, 2.2665e-06, 4.8066e-10, 1.3504e-06, 3.3915e-06,\n",
            "        2.3951e-06, 1.7706e-06, 3.8043e-06, 2.4844e-06, 2.3730e-06, 3.9315e-06,\n",
            "        2.2744e-06, 1.7066e-07, 6.2128e-06, 3.3923e-06, 5.3623e-06, 2.0539e-06,\n",
            "        3.2325e-07, 3.9114e-07, 1.2058e-06, 3.3823e-06, 3.6924e-06, 4.3828e-08,\n",
            "        1.2354e-06, 3.1653e-06, 4.4877e-06, 4.1031e-06, 3.1131e-06, 3.5886e-06,\n",
            "        1.6619e-06, 2.8877e-06, 3.1893e-06, 5.0526e-07, 2.2316e-06, 1.6141e-06,\n",
            "        3.8095e-09, 4.1335e-06, 2.1960e-06, 5.8297e-06, 5.6879e-08, 7.9505e-07,\n",
            "        2.4450e-06, 3.5984e-06])}, 18: {'step': 8100, 'exp_avg': tensor([[-2.6823e-03, -2.4121e-03, -1.4114e-03, -1.0691e-03, -2.5303e-03,\n",
            "         -1.3390e-03,  4.6507e-05, -1.6703e-03, -1.6330e-03, -1.5865e-03,\n",
            "         -2.7369e-03, -8.0121e-03, -4.9646e-04, -9.6986e-04,  9.9521e-05,\n",
            "         -1.6795e-03, -1.6850e-03, -9.7342e-04,  6.7509e-04, -1.5616e-02,\n",
            "         -3.3137e-04,  1.6717e-04, -1.7348e-03, -1.1321e-03, -1.7117e-03,\n",
            "         -3.7760e-04, -2.4827e-03, -1.1998e-03, -5.4646e-04, -5.0429e-04,\n",
            "         -9.4860e-04, -5.5326e-04, -2.0236e-03,  7.2669e-04, -9.6741e-04,\n",
            "         -1.8463e-03, -1.6113e-04, -1.6901e-03, -1.4938e-03, -8.0686e-04,\n",
            "         -1.5790e-03, -1.0700e-03, -2.0010e-03, -7.7624e-04, -1.3426e-03,\n",
            "         -1.1836e-03, -1.5189e-02, -1.3662e-03, -5.2331e-03, -2.9648e-02,\n",
            "         -7.3007e-04, -5.4134e-04, -2.8670e-04, -1.1007e-03,  3.0347e-04,\n",
            "         -1.3223e-03, -1.2206e-03, -3.5954e-04, -1.7661e-03, -6.8607e-04,\n",
            "         -5.4232e-04, -1.7566e-02,  7.7508e-05, -1.2322e-02,  3.6332e-04,\n",
            "         -2.0541e-03, -2.4850e-03, -8.9597e-04, -1.3583e-04, -2.3248e-03,\n",
            "         -9.6214e-04, -6.0536e-03, -1.5173e-03, -7.9898e-04, -2.4928e-03,\n",
            "         -5.0733e-03, -1.4773e-03, -2.1652e-03, -1.7377e-03, -1.2200e-03,\n",
            "         -1.4677e-03, -1.6851e-03, -1.3853e-03, -1.6179e-03, -4.7724e-04,\n",
            "         -1.4639e-03, -1.5981e-03, -2.5231e-02, -1.9356e-03, -7.6004e-04,\n",
            "         -3.0523e-03, -2.7970e-03, -5.3427e-04, -1.0677e-03, -4.6136e-04,\n",
            "         -5.7257e-04, -1.4846e-03, -3.0343e-03,  5.5803e-04, -1.9778e-03,\n",
            "         -1.2911e-03, -8.1967e-04, -2.5150e-03, -1.7415e-03, -2.1235e-03,\n",
            "         -1.6461e-05, -1.7538e-03, -1.1165e-02, -2.2228e-03, -6.8320e-04,\n",
            "         -9.0914e-05, -1.0013e-03, -4.8394e-04, -1.4673e-03, -1.4719e-03,\n",
            "         -1.8257e-03, -1.9570e-04, -3.4091e-03, -1.3842e-03, -1.0558e-03,\n",
            "         -1.1741e-02, -6.3933e-04, -2.0566e-03, -6.1255e-04, -9.0009e-03,\n",
            "          8.7064e-05, -7.0846e-04,  1.6472e-04],\n",
            "        [ 2.6971e-03,  2.3151e-03,  1.4058e-03,  1.0525e-03,  2.5234e-03,\n",
            "          1.3043e-03, -5.6491e-05,  1.6397e-03,  1.6120e-03,  1.5915e-03,\n",
            "          2.7291e-03,  7.9146e-03,  4.9348e-04,  9.6556e-04, -1.0913e-04,\n",
            "          1.6806e-03,  1.6797e-03,  9.7040e-04, -6.9866e-04,  1.5364e-02,\n",
            "          3.2911e-04, -1.9176e-04,  1.6866e-03,  1.1192e-03,  1.6969e-03,\n",
            "          2.7403e-04,  2.4704e-03,  1.1957e-03,  5.0156e-04,  5.0140e-04,\n",
            "          8.7128e-04,  5.1987e-04,  2.0186e-03, -7.6329e-04,  9.6512e-04,\n",
            "          1.7974e-03,  1.6108e-04,  1.6921e-03,  1.4904e-03,  7.5749e-04,\n",
            "          1.5765e-03,  1.0422e-03,  2.0074e-03,  6.8952e-04,  1.2611e-03,\n",
            "          1.1545e-03,  1.4816e-02,  1.3578e-03,  5.1327e-03,  2.9351e-02,\n",
            "          6.9965e-04,  5.0841e-04,  1.9932e-04,  1.0988e-03, -3.9605e-04,\n",
            "          1.2883e-03,  1.2217e-03,  2.9097e-04,  1.7414e-03,  6.8403e-04,\n",
            "          4.8692e-04,  1.7236e-02, -1.4146e-04,  1.2050e-02, -3.7468e-04,\n",
            "          2.0583e-03,  2.4790e-03,  8.9210e-04,  1.2387e-04,  2.3332e-03,\n",
            "          9.5758e-04,  5.8430e-03,  1.4830e-03,  7.9892e-04,  2.4211e-03,\n",
            "          4.9182e-03,  1.4662e-03,  2.0928e-03,  1.7492e-03,  1.2204e-03,\n",
            "          1.4611e-03,  1.6292e-03,  1.3796e-03,  1.5437e-03,  4.7600e-04,\n",
            "          1.4452e-03,  1.5927e-03,  2.4904e-02,  1.8284e-03,  7.6529e-04,\n",
            "          3.0252e-03,  2.7590e-03,  5.1088e-04,  1.0398e-03,  4.1492e-04,\n",
            "          5.4713e-04,  1.4748e-03,  2.9823e-03, -5.7714e-04,  1.9521e-03,\n",
            "          1.2547e-03,  7.4721e-04,  2.4930e-03,  1.6170e-03,  2.0522e-03,\n",
            "          1.1194e-07,  1.7158e-03,  1.1065e-02,  2.1460e-03,  6.3838e-04,\n",
            "          6.9150e-05,  9.6235e-04,  4.2638e-04,  1.4408e-03,  1.3824e-03,\n",
            "          1.7369e-03,  1.3058e-04,  3.3549e-03,  1.3709e-03,  1.0504e-03,\n",
            "          1.1546e-02,  6.4174e-04,  2.0322e-03,  5.3462e-04,  8.7852e-03,\n",
            "         -9.2258e-05,  7.0181e-04, -2.4492e-04]]), 'exp_avg_sq': tensor([[1.5948e-05, 3.3769e-05, 9.1775e-06, 8.5041e-06, 2.7240e-05, 9.9103e-06,\n",
            "         5.3198e-06, 1.9614e-05, 8.0078e-06, 8.0528e-06, 1.4881e-05, 3.0284e-04,\n",
            "         8.9341e-06, 7.3738e-06, 9.0455e-06, 9.3923e-06, 9.2126e-06, 9.4271e-06,\n",
            "         1.0920e-05, 1.0145e-03, 5.8808e-06, 5.6300e-06, 1.5314e-05, 6.9732e-06,\n",
            "         8.1901e-06, 1.4434e-05, 2.2695e-05, 7.8387e-06, 1.6880e-05, 5.7512e-06,\n",
            "         2.4821e-05, 9.7843e-06, 9.7923e-06, 5.7209e-06, 1.0507e-05, 3.5543e-05,\n",
            "         6.5783e-06, 8.6845e-06, 7.5027e-06, 2.7447e-05, 5.1457e-06, 8.3742e-06,\n",
            "         1.1879e-05, 1.9494e-05, 1.5830e-05, 1.1779e-05, 2.7039e-03, 5.3292e-06,\n",
            "         8.9612e-05, 2.1266e-03, 1.5577e-05, 1.3181e-05, 1.2043e-05, 6.0594e-06,\n",
            "         2.9467e-05, 2.5703e-05, 9.6849e-06, 1.2563e-05, 7.0011e-06, 5.2366e-06,\n",
            "         1.4054e-05, 1.1003e-03, 1.3593e-05, 5.6246e-04, 1.0476e-05, 8.7140e-06,\n",
            "         1.3574e-05, 1.2492e-05, 6.8776e-06, 1.1251e-05, 9.1345e-06, 3.9939e-04,\n",
            "         1.3319e-05, 1.0034e-05, 3.8066e-05, 1.5709e-04, 1.3093e-05, 1.1036e-05,\n",
            "         1.8960e-05, 5.6157e-06, 1.9572e-05, 2.8164e-05, 6.9423e-06, 5.9544e-05,\n",
            "         1.0137e-05, 1.4021e-05, 7.4428e-06, 2.0214e-03, 3.2252e-05, 1.0696e-05,\n",
            "         2.2672e-05, 2.2589e-05, 6.5162e-06, 9.5391e-06, 1.4165e-05, 1.2085e-05,\n",
            "         1.5150e-05, 5.7448e-05, 8.2884e-06, 1.3357e-05, 1.8331e-05, 1.6833e-05,\n",
            "         4.4488e-05, 8.9294e-05, 5.3082e-05, 4.0231e-06, 2.4562e-05, 6.4222e-04,\n",
            "         4.3254e-05, 8.1840e-06, 1.3680e-05, 1.3969e-05, 2.0465e-05, 1.0711e-05,\n",
            "         3.0911e-05, 2.3564e-05, 1.5621e-05, 7.9889e-05, 1.2350e-05, 9.1510e-06,\n",
            "         1.7979e-03, 1.2059e-05, 1.3603e-05, 1.5056e-05, 4.7486e-04, 2.4269e-06,\n",
            "         9.8628e-06, 1.7256e-05],\n",
            "        [1.5943e-05, 3.3878e-05, 9.1275e-06, 8.5537e-06, 2.7219e-05, 9.8564e-06,\n",
            "         5.3414e-06, 1.9577e-05, 7.9694e-06, 8.0439e-06, 1.4858e-05, 3.0334e-04,\n",
            "         8.9586e-06, 7.3660e-06, 9.0423e-06, 9.3684e-06, 9.1793e-06, 9.4290e-06,\n",
            "         1.0986e-05, 1.0156e-03, 5.8818e-06, 5.6700e-06, 1.5346e-05, 6.9496e-06,\n",
            "         8.1639e-06, 1.4531e-05, 2.2643e-05, 7.8114e-06, 1.6849e-05, 5.7461e-06,\n",
            "         2.4912e-05, 9.7858e-06, 9.7517e-06, 5.7422e-06, 1.0495e-05, 3.5623e-05,\n",
            "         6.5939e-06, 8.6429e-06, 7.4775e-06, 2.7575e-05, 5.1384e-06, 8.3640e-06,\n",
            "         1.1870e-05, 1.9542e-05, 1.5886e-05, 1.1737e-05, 2.7060e-03, 5.3244e-06,\n",
            "         8.9821e-05, 2.1277e-03, 1.5550e-05, 1.3226e-05, 1.2123e-05, 6.0421e-06,\n",
            "         2.9632e-05, 2.5788e-05, 9.6985e-06, 1.2644e-05, 6.9800e-06, 5.2129e-06,\n",
            "         1.4120e-05, 1.1015e-03, 1.3685e-05, 5.6317e-04, 1.0489e-05, 8.6882e-06,\n",
            "         1.3550e-05, 1.2473e-05, 6.9035e-06, 1.1213e-05, 9.1273e-06, 3.9980e-04,\n",
            "         1.3293e-05, 1.0051e-05, 3.8284e-05, 1.5734e-04, 1.3092e-05, 1.0994e-05,\n",
            "         1.8949e-05, 5.6052e-06, 1.9567e-05, 2.8226e-05, 6.9292e-06, 5.9651e-05,\n",
            "         1.0141e-05, 1.4013e-05, 7.4102e-06, 2.0221e-03, 3.2349e-05, 1.0655e-05,\n",
            "         2.2641e-05, 2.2535e-05, 6.4840e-06, 9.5147e-06, 1.4216e-05, 1.2033e-05,\n",
            "         1.5135e-05, 5.7461e-05, 8.2618e-06, 1.3339e-05, 1.8306e-05, 1.6900e-05,\n",
            "         4.4541e-05, 8.9452e-05, 5.3264e-05, 4.0445e-06, 2.4589e-05, 6.4271e-04,\n",
            "         4.3406e-05, 8.2187e-06, 1.3662e-05, 1.4034e-05, 2.0564e-05, 1.0689e-05,\n",
            "         3.1058e-05, 2.3637e-05, 1.5697e-05, 8.0124e-05, 1.2321e-05, 9.1366e-06,\n",
            "         1.7984e-03, 1.2015e-05, 1.3551e-05, 1.5167e-05, 4.7520e-04, 2.4271e-06,\n",
            "         9.8537e-06, 1.7353e-05]])}}\n",
            "param_groups \t [{'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]}]\n"
          ]
        }
      ],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in net.state_dict():\n",
        "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating data independence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def calculate_identity(query,database):\n",
        "#     if len(query) == len(database):\n",
        "#         return np.sum([1 for i in range(len(query)) if query[i] == database[i]])/len(query)\n",
        "    \n",
        "#     elif len(query) > len(database):\n",
        "#         max_score = 0\n",
        "#         diff_len = len(query) - len(database)\n",
        "#         for i in range(diff_len + 1):\n",
        "#             query_shortened = query[i:i+len(database)]\n",
        "#             score = np.sum([1 for i in range(len(query_shortened)) if query_shortened[i] == database[i]])/len(query_shortened)\n",
        "#             if score > max_score:\n",
        "#                 max_score = score\n",
        "\n",
        "#         return max_score\n",
        "    \n",
        "#     elif len(query) < len(database):\n",
        "#         max_score = 0\n",
        "#         diff_len = len(database) - len(query)\n",
        "#         for i in range(diff_len + 1):\n",
        "#             database_shortened = database[i:i+len(query)]\n",
        "#             score = np.sum([1 for i in range(len(query)) if query[i] == database_shortened[i]])/len(database_shortened)\n",
        "#             if score > max_score:\n",
        "#                 max_score = score\n",
        "\n",
        "#         return max_score\n",
        "        \n",
        "# identity = [[calculate_identity(pep1,pep2) for pep1 in training_df[\"peptide\"]] for pep2 in validation_df[\"peptide\"]]\n",
        "# peptides = [[(pep1,pep2) for pep1 in training_df[\"peptide\"]] for pep2 in validation_df[\"peptide\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identity = np.array(identity)\n",
        "# peptides = np.array(peptides)\n",
        "# max_identity = np.max(identity)\n",
        "# max_row_index,max_column_index = np.where(identity == np.amax(identity))\n",
        "# print(peptides[max_row_index[0],max_column_index[0]])\n",
        "# print(calculate_identity(\"FLPSDFFPSV\",\"FLPPDFFPSV\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_peptides_train = []\n",
        "# all_HLA_train = []\n",
        "\n",
        "# for i in range(len((peptide_train_loader))):\n",
        "#     train_peptides = peptide_train_loader[i].reshape(len(peptide_train_loader[i]),-1).tolist()\n",
        "#     all_peptides_train += train_peptides\n",
        "#     train_HLA = HLA_train_loader[i].reshape(len(HLA_train_loader[i]),-1).tolist()\n",
        "#     all_HLA_train += train_HLA\n",
        "\n",
        "\n",
        "# all_peptides_val = []\n",
        "# all_HLA_val = []\n",
        "# for j in range(len((peptide_val_loader))):\n",
        "#     val_peptides = peptide_val_loader[j].reshape(len(peptide_val_loader[j]),-1).tolist()\n",
        "#     all_peptides_val += val_peptides\n",
        "#     val_HLA = HLA_val_loader[j].reshape(len(HLA_val_loader[j]),-1).tolist()\n",
        "#     all_HLA_val += val_HLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(all_peptides_train[0] + all_HLA_train[0])\n",
        "# a = np.array(all_peptides_train[0] + all_HLA_train[0])\n",
        "# b =  np.array(all_peptides_train[1] + all_HLA_train[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix_HLA_pept_dist = [[0 for _ in range(len(all_HLA_val))] for _ in range(len(all_peptides_train))]\n",
        "# matrix_HLA_pept_dist = np.array(matrix_HLA_pept_dist)\n",
        "\n",
        "# for i in range(len(all_peptides_train)):\n",
        "#     if i % 500 == 0:\n",
        "#         print(i)\n",
        "#     for j in range(len(all_peptides_val)):\n",
        "#         train_obs = np.array(all_peptides_train[i] + all_HLA_train[i])\n",
        "#         val_obs = np.array(all_peptides_val[j] + all_HLA_val[j])\n",
        "#         dist = np.linalg.norm(train_obs-val_obs)\n",
        "#         matrix_HLA_pept_dist[i,j] = dist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.min(matrix_HLA_pept_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# matrix_pept_dist = [[0 for _ in range(len(all_HLA_val))] for _ in range(len(all_peptides_train))]\n",
        "# matrix_pept_dist = np.array(matrix_pept_dist)\n",
        "\n",
        "# for i in range(len(all_peptides_train)):\n",
        "#     if i % 500 == 0:\n",
        "#         print(i)\n",
        "#     for j in range(len(all_peptides_val)):\n",
        "#         train_obs = np.array(all_peptides_train[i])\n",
        "#         val_obs = np.array(all_peptides_val[j])\n",
        "#         dist = np.linalg.norm(train_obs-val_obs)\n",
        "#         matrix_pept_dist[i,j] = dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.min(matrix_pept_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "reproduce_deep_immuno_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
