{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    \"\"\" Initializes weights for network\n",
    "\n",
    "    Args:\n",
    "        m (pytorch object): A layer in the network\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model structures from the \"model_structures.py\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 392801\n",
      "best_RNN(\n",
      "  (peptide_encoding): LSTM(12, 10, batch_first=True, bidirectional=True)\n",
      "  (hla_encoding): LSTM(12, 10, batch_first=True, bidirectional=True)\n",
      "  (drop_out): Dropout(p=0.4, inplace=False)\n",
      "  (L_in): Linear(in_features=880, out_features=440, bias=True)\n",
      "  (batchnorm1): BatchNorm1d(440, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (L_2): Linear(in_features=440, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[0.5096],\n",
      "        [0.3726],\n",
      "        [0.5181],\n",
      "        [0.3676],\n",
      "        [0.4467],\n",
      "        [0.5800],\n",
      "        [0.3737],\n",
      "        [0.6598],\n",
      "        [0.3949],\n",
      "        [0.5577]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model_structures import *\n",
    "net = best_RNN()\n",
    "print(\"Number of parameters in model:\", get_n_params(net))\n",
    "print(net)\n",
    "\n",
    "# Testing flow-through of information within the netwowkr\n",
    "peptide_random = np.random.normal(0,1, (10, 10, 12)).astype('float32')\n",
    "peptide_random = Variable(torch.from_numpy(peptide_random))\n",
    "HLA_random = np.random.normal(0,1, (10, 34, 12)).astype('float32')\n",
    "HLA_random = Variable(torch.from_numpy(HLA_random))\n",
    "binding_random = np.random.normal(0,1, (10, 1)).astype('float32')\n",
    "binding_random = Variable(torch.from_numpy(binding_random))\n",
    "output = net(peptide_random,HLA_random)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading and encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_peptide_onehot(aa_seq):\n",
    "    \"\"\"Enocding an aa-seqquence using the One-hot scheme\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): Peptide sequence to encode\n",
    "\n",
    "    Returns:\n",
    "        np.array: Encoded peptide\n",
    "    \"\"\"\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F','P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    one_hot_matrix = pd.DataFrame(np.identity(len(amino_acids)).astype(\"float32\"))\n",
    "    one_hot_matrix.index = amino_acids\n",
    "    encoded_aa_seq = []\n",
    "\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for _ in range(len(amino_acids))]))\n",
    "        else:\n",
    "            try:    \n",
    "                encoded_aa_seq.append(one_hot_matrix.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    \n",
    "\n",
    "    encoded_aa_seq = np.array(encoded_aa_seq)\n",
    "    return encoded_aa_seq\n",
    "\n",
    "\n",
    "def load_blossum62_matrix():\n",
    "    \"\"\"Loads the blossum62 substitution matrix using the BioPython library\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas dataframe which holds the blossum62 sub-matrix\n",
    "    \"\"\"\n",
    "    from Bio.Align import substitution_matrices\n",
    "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "    blossum_aas = list(\"ARNDCQEGHILKMFPSTWYVBZX*\")\n",
    "    blosum62 = pd.DataFrame(blosum62,columns=blossum_aas,index=blossum_aas)\n",
    "    return blosum62\n",
    "\n",
    "\n",
    "def encode_peptide_blossum65(aa_seq,blussom_matrix):\n",
    "    \"\"\"Enocding an aa-seqquence using the Blossum62 encoding scheme\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        blussom_matrix (pd.DataFrame): A pandas dataframe which holds the blossum62 sub-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    AAs = blussom_matrix.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for _ in range(AAs)]))\n",
    "        else:\n",
    "            try:\n",
    "                encoded_aa_seq.append(blussom_matrix.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    \n",
    "    encoded_aa_seq = np.array(encoded_aa_seq)\n",
    "    return encoded_aa_seq\n",
    "\n",
    "\n",
    "def encode_peptide_aaindex(aa_seq,aaindex_PCA):\n",
    "    \"\"\"Enocding an aa-seqquence using the AAindex encoding scheme.\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        aaindex_PCA (pd.DataFrame):  A pandas dataframe which holds the AAindex encoding scheme\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    PCs = aaindex_PCA.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for x in range(PCs)]))\n",
    "        else:\n",
    "            try:\n",
    "                encoded_aa_seq.append(aaindex_PCA.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    return np.array(encoded_aa_seq)\n",
    "\n",
    "def encode_multiple(aa_seq,aaindex_PCA,blussom_matrix):\n",
    "    \"\"\"Enocding an aa-seqquence using the the Combined encoding schemes of AAindex, Blossum62 and One-hot.\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        aaindex_PCA (pd.DataFrame):  A pandas dataframe which holds the AAindex encoding scheme\n",
    "        blussom_matrix (pd.DataFrame):  A pandas dataframe which holds the blossum62 sub-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F','P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    one_hot_matrix = pd.DataFrame(np.identity(len(amino_acids)).astype(\"float32\"))\n",
    "    one_hot_matrix.index = amino_acids\n",
    "    encoded_aa_seq = []\n",
    "\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    PCs = aaindex_PCA.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for x in range(56)]))\n",
    "        else:\n",
    "            try:\n",
    "                aa_index_encoding = aaindex_PCA.loc[aa].to_numpy()\n",
    "                blossum_encoding = blussom_matrix.loc[aa].to_numpy()\n",
    "                \n",
    "                onehot_encoding = one_hot_matrix.loc[aa].to_numpy()\n",
    "                encoding = np.concatenate((aa_index_encoding,blossum_encoding,onehot_encoding))\n",
    "                encoded_aa_seq.append(encoding)\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    return np.array(encoded_aa_seq)\n",
    "\n",
    "def encode_dataset(df,aa_index_matrix,blosum62_matrix,HLA_dict,peptide_len,padding=\"right\"):\n",
    "    \"\"\"Encodes the filtered, balanced and partioned dataset. This is done in three major steps. \n",
    "    1) The MHCI-allele code is converted to the paratope sequence. \n",
    "    2) The MHCI is encoded using one of four different schemes.\n",
    "    3) The peptide sequence is encoded using one of four different schemes.\n",
    "    4) The peptide sequence is padded to a specified length.\n",
    "    5) Encoding the binding score retrived from netMHCpan\n",
    "    6) The output label y is created based on the number of positive subjects from the raw data\n",
    "        1 ~ If number of positive subjects is greater than 0\n",
    "        0 ~ If number of positive subjects is equal to 0\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): The dataframe containing the dataset which is to be encoded\n",
    "        aa_index_matrix (pd.DataFrame): A pandas dataframe which holds the AAindex encoding scheme\n",
    "        blussom_matrix (pd.DataFrame):  A pandas dataframe which holds the blossum62 sub-matrix\n",
    "        HLA_dict (dict): A dictionary used to translate the MHCI-allele code is converted to the paratope sequence. \n",
    "        peptide_len (int): The length we want the encoded sequence to have.\n",
    "        padding (str, optional): The strategy used to pad the peptide sequences. Defaults to \"right\".\n",
    "\n",
    "    Returns:\n",
    "        np.array: Encoded peptide\n",
    "        np.array: Encoded HLA\n",
    "        np.array: Encoded Binding-score\n",
    "        np.array: Encoded Labels\n",
    "\n",
    "    \"\"\"\n",
    "    encoded_peptides = []\n",
    "    encoded_labels = []\n",
    "    encoded_hlas = []\n",
    "    encoded_binding_scores = []\n",
    "    for i,row in df.iterrows():\n",
    "        \n",
    "        HLA = HLA_dict[row[\"HLA_allele\"].replace(\":\",\"\")]\n",
    "        encoded_HLA = encode_peptide_aaindex(HLA,aa_index_matrix)\n",
    "        # encoded_HLA = encode_peptide_onehot(HLA)\n",
    "        # encoded_HLA = encode_peptide_blossum65(HLA,blosum62_matrix)\n",
    "        # encoded_HLA = encode_multiple(HLA,aaindex_PCA,blosum62_matrix)\n",
    "\n",
    "        peptide = row[\"peptide\"]\n",
    "        encoded_peptide = encode_peptide_aaindex(peptide,aa_index_matrix)\n",
    "        # encoded_peptide = encode_peptide_onehot(peptide)\n",
    "        # encoded_peptide = encode_peptide_blossum65(peptide,blosum62_matrix)\n",
    "        # encoded_peptide = encode_multiple(peptide,aaindex_PCA,blosum62_matrix)\n",
    "        \n",
    "\n",
    "        binding_score = row['binding_score']\n",
    "\n",
    "        # Adding padding\n",
    "        if len(encoded_peptide) < peptide_len:\n",
    "            n_added = peptide_len-len(encoded_peptide)\n",
    "            if padding == \"right\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((0, 1), (0, 0)), 'constant')\n",
    "            elif padding == \"left\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((1, 0), (0, 0)), 'constant')\n",
    "            elif padding == \"random\":\n",
    "                top_pad = random.choice([0,1])\n",
    "                bot_pad = 1-top_pad\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((top_pad, bot_pad), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "        encoded_label = min(1,row[\"positive_subjects\"])\n",
    "        encoded_peptides.append(encoded_peptide)\n",
    "        encoded_hlas.append(encoded_HLA)\n",
    "        encoded_labels.append(encoded_label)\n",
    "        encoded_binding_scores.append(binding_score)\n",
    "    \n",
    "    encoded_peptides = np.array(encoded_peptides).astype('float32')\n",
    "    encoded_hlas = np.array(encoded_hlas).astype('float32')\n",
    "    encoded_labels = np.array(encoded_labels).astype('float32').reshape(-1,1)\n",
    "    encoded_binding_scores = np.array(encoded_binding_scores).astype('float32').reshape(-1,1)\n",
    "\n",
    "    return encoded_peptides, encoded_hlas, encoded_binding_scores, encoded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(K, lst_train_acc, lst_val_acc): \n",
    "    \"\"\"For each fold, plot the accuracy on\n",
    "    train and validation data for each epoch\n",
    "\n",
    "    Args:\n",
    "        K (int): Number of CV folds\n",
    "        lst_train_acc (list of lists): list of lists containing training accuracy for each fold pr epoch\n",
    "        lst_val_acc (list of lists): list of lists containing validation accuracy for each fold pr epoch\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    for i in range(K):\n",
    "        epoch = np.arange(len(lst_train_acc[i]))\n",
    "        plt.plot(epoch, lst_train_acc[i], 'r', epoch, lst_val_acc[i], 'b')\n",
    "    plt.title(\"Performance of {} fold CV\".format(K))\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('epochs'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_avg_val_performance(K, N, lst_val_acc, lst_val_lab, lst_val_pred):\n",
    "    \"\"\"Calculate the generalization error\n",
    "\n",
    "    Args:\n",
    "        lst_val_acc (list of lists): list of lists containing validation accuracies for each fold\n",
    "        lst_val_lab (list of lists): list of lists containing validation labels for each fold\n",
    "        lst_val_pred (list of lists): list of lists containing validation predictions for each fold\n",
    "        N (int): Total number of observations in data set\n",
    "    \"\"\"\n",
    "    avg_recall = 0\n",
    "    avg_accuracy = 0 \n",
    "    avg_f1 = 0\n",
    "\n",
    "    res = np.zeros((K,5))\n",
    "    for i in range(K):\n",
    "        best_epoch_model = np.argmax(lst_val_acc[i])\n",
    "        n = len(lst_val_lab[i][best_epoch_model])\n",
    "        accuracy = accuracy_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "        recall = recall_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "        f1 = f1_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "\n",
    "        res[i][0] = best_epoch_model\n",
    "        res[i][1] = n\n",
    "        res[i][2] = accuracy\n",
    "        res[i][3] = recall\n",
    "        res[i][4] = f1\n",
    "\n",
    "        avg_recall += (n/N) * recall\n",
    "        avg_accuracy += (n/N) * accuracy\n",
    "        avg_f1 += (n/N) * f1\n",
    "\n",
    "    print(f\"Best average results - Recall: {avg_recall} accuracy: {avg_accuracy} f1-score: {avg_f1}\")\n",
    "    return res\n",
    "\n",
    "def plot_roc_curve_best_epoch(valid_losses, predictions, targets):\n",
    "    \"\"\"Plots the ROC curve for the best epoch\n",
    "\n",
    "    Args:\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    best_epoch_model = np.argmin(valid_losses)\n",
    "    print(\"Best Epoch\",best_epoch_model)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(targets[best_epoch_model],predictions[best_epoch_model])\n",
    "    roc_auc = metrics.auc(fpr,tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b',label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "def plot_PR_curve_best_epoch(valid_losses, predictions, targets):\n",
    "    \"\"\"Plots the PR curve for the best epoch\n",
    "\n",
    "    Args:\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    best_epoch_model = np.argmin(valid_losses)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(targets[best_epoch_model], predictions[best_epoch_model])\n",
    "    roc_auc = metrics.auc(recall, precision)\n",
    "    plt.title('Precission-Recall curve')\n",
    "    plt.plot(recall, precision, 'b', label = 'AUCpr = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Precession')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pick_optimal_threshold_auc(fpr, tpr, threshold):\n",
    "    \"\"\"Picks the optimal threshold for determining \n",
    "    immunogenic/non-immunogenic targets on unlabelled data\n",
    "\n",
    "    Args:\n",
    "        fpr (float): false positive rate  \n",
    "        tpr (float): true positive rate\n",
    "        threshold (float): The current threshold to update\n",
    "\n",
    "    Returns:\n",
    "        int: position of highest geometric mean\n",
    "    \"\"\"\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    return ix\n",
    "\n",
    "def plot_all_roc_curves(K,valid_losses, predictions, targets):\n",
    "    \"\"\"Plot of ROC curve for all K CV folds\n",
    "\n",
    "    Args:\n",
    "        K (int): number of CV folds\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    for k in range(K):\n",
    "        best_epoch_model = np.argmin(valid_losses[k])\n",
    "        \n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(targets[k][best_epoch_model],predictions[k][best_epoch_model])\n",
    "        roc_auc = round(metrics.auc(fpr,tpr),3)\n",
    "        best_threshold = pick_optimal_threshold_auc(fpr, tpr, threshold)\n",
    "        print(f\"Best Epoch in K {k}\",best_epoch_model,\"best threshold:\",threshold[best_threshold])\n",
    "        plt.plot(fpr, tpr,label = f'CV {k+1} AUC {roc_auc}')\n",
    "        plt.plot(fpr[best_threshold], tpr[best_threshold],color=\"black\",marker=\"d\")\n",
    "        plt.plot()\n",
    "\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_all_PR_curves(K,valid_losses, predictions, targets):\n",
    "    \"\"\"Plot of PR curve for all K CV folds\n",
    "\n",
    "    Args:\n",
    "        K (int): number of CV folds\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    for k in range(K):\n",
    "        best_epoch_model = np.argmin(valid_losses[k])\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(targets[k][best_epoch_model], predictions[k][best_epoch_model])\n",
    "        roc_auc = round(metrics.auc(recall, precision),3)\n",
    "        plt.plot(recall, precision,label = f'CV {k+1} AUC {roc_auc}')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device state: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_learning_curve(train_accuracies,val_accuracies):\n",
    "    \"\"\"Plot accuracy during training\n",
    "\n",
    "    Args:\n",
    "        train_accuracies (list): list of training accuracies on target values in current CV fold\n",
    "        val_accuracies (list): list of validation accuracies on target values in current CV fold\n",
    "    \"\"\"\n",
    "    epoch = np.arange(len(train_accuracies))\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_accuracies, 'r', epoch, val_accuracies, 'b')\n",
    "    plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "    plt.xlabel('epochs'), plt.ylabel('Acc')\n",
    "\n",
    "\n",
    "def validation(model,device,valid_loaders,train_loaders):\n",
    "    \"\"\"Generates model predictions on train and validation data\n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): trained neural network \n",
    "        device (state): either cuda enabled or not\n",
    "        valid_loaders (Data loader): Data loader for validation data\n",
    "        train_loaders (Data loader): Data loader for training data\n",
    "\n",
    "    Returns:\n",
    "        lists: lists of predictions, targets and validation loss\n",
    "    \"\"\"\n",
    "    peptide_val_loader,HLA_val_loader,label_val_loader,binding_score_val_loader = valid_loaders\n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_train_predictions = []\n",
    "        all_train_targets = []\n",
    "        for i in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[i].to(device)\n",
    "            train_HLA = HLA_train_loader[i].to(device)\n",
    "            train_labels = label_train_loader[i].to(device)\n",
    "            train_binding_scores = binding_score_train_loader[i].to(device)\n",
    "            outputs = model(train_peptides,train_HLA)\n",
    "            all_train_predictions += outputs.cpu().numpy().tolist()\n",
    "            all_train_targets += train_labels.cpu().numpy().tolist()\n",
    "        \n",
    "        all_val_targets = []\n",
    "        all_val_predictions = []\n",
    "        for j in range(len((peptide_val_loader))):\n",
    "            val_peptides = peptide_val_loader[j].to(device)\n",
    "            val_HLA = HLA_val_loader[j].to(device)\n",
    "            val_labels = label_val_loader[j].to(device)\n",
    "            val_binding_scores = binding_score_val_loader[j].to(device)\n",
    "            outputs = model(val_peptides,val_HLA)\n",
    "            all_val_predictions += outputs.cpu().numpy().tolist()\n",
    "            all_val_targets += val_labels.cpu().numpy().tolist()\n",
    "\n",
    "        validation_loss = mean_squared_error(all_val_targets,all_val_predictions)\n",
    "        # loss_function = nn.BCELoss()\n",
    "        # targets = torch.tensor(all_val_targets)\n",
    "        # predictions = torch.tensor(all_val_predictions)\n",
    "        # validation_loss = loss_function(targets,predictions)/len(targets)\n",
    "        \n",
    "\n",
    "    return all_train_targets,all_train_predictions,all_val_targets,all_val_predictions,validation_loss\n",
    "\n",
    "\n",
    "def testing(model,device,test_loader):\n",
    "    \"\"\"Testing model every epoch on testing data\n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): Trained neural network\n",
    "        device (state): enable cuda or not\n",
    "        test_loader (Data loader): Data loader for test data \n",
    "\n",
    "    Returns:\n",
    "        lists: Target and prediction values \n",
    "    \"\"\"\n",
    "    peptide_test_loader,HLA_test_loader,label_test_loader,binding_score_test_loader = test_loader\n",
    "    model.eval()\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "    with torch.no_grad():\n",
    "        test_peptides = peptide_test_loader[0].to(device)\n",
    "        test_HLA = HLA_test_loader[0].to(device)\n",
    "        test_labels = label_test_loader[0].to(device)\n",
    "        test_binding_scores = binding_score_test_loader[0].to(device)\n",
    "        outputs = model(test_peptides,test_HLA)\n",
    "        all_test_predictions.append(outputs.cpu().numpy().tolist())\n",
    "        all_test_targets.append(test_labels.cpu().numpy().tolist())\n",
    "    return all_test_targets,all_test_predictions\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train(model, device, epochs, train_loaders, valid_loaders, test_loaders, learning_rate, weight_decay):\n",
    "    \"\"\"train the model \n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): untrained neural network\n",
    "        device (state): cuda enabled or not\n",
    "        epochs (int): number of epoch to do\n",
    "        train_loaders (Data loaders): Data loaders for training data\n",
    "        valid_loaders (Data loaders): Data loaders for validation data\n",
    "        test_loaders (Data loaders): Data loaders for test data\n",
    "        learning_rate (float) : learning rate value\n",
    "        weight_decay (float) : weight decay value\n",
    "\n",
    "    Returns:\n",
    "        pytorch model: trained neural network\n",
    "        lists: train and validation losses\n",
    "    \"\"\"\n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Early stopping\n",
    "    the_last_loss = 100\n",
    "    patience = 4\n",
    "    trigger_times = 0\n",
    "    \n",
    "    all_val_targets_pr_epoch = []\n",
    "    all_val_predictions_pr_epoch = []\n",
    "\n",
    "    all_test_targets_pr_epoch = []\n",
    "    all_test_predictions_pr_epoch = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0\n",
    "        \n",
    "        for train_batch_index in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[train_batch_index].to(device)\n",
    "            train_HLA = HLA_train_loader[train_batch_index].to(device)\n",
    "            train_labels = label_train_loader[train_batch_index].to(device)\n",
    "            train_binding_scores = binding_score_train_loader[train_batch_index].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_peptides,train_HLA)\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "        train_losses.append(current_loss/len((peptide_train_loader)))\n",
    "\n",
    "        all_train_targets,all_train_predictions,all_val_targets,all_val_predictions,validation_loss = validation(model,device,valid_loaders,train_loaders)\n",
    "        \n",
    "\n",
    "        val_losses.append(validation_loss)\n",
    "        all_val_targets_pr_epoch.append(all_val_targets)\n",
    "        all_val_predictions_pr_epoch.append(all_val_predictions)\n",
    "\n",
    "        all_test_targets,all_test_predictions = testing(model,device,test_loaders)\n",
    "        all_test_targets_pr_epoch.append(all_test_targets)\n",
    "        all_test_predictions_pr_epoch.append(all_test_predictions)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch %2i : Train Loss %f , Validation loss %f\" % (epoch+1, train_losses[-1], val_losses[-1]))\n",
    "        \n",
    "\n",
    "        # Early stopping\n",
    "        the_current_val_loss = val_losses[-1]\n",
    "        the_last_val_loss = 0 if len(val_losses) < 2 else val_losses[-2]\n",
    "\n",
    "\n",
    "        if the_current_val_loss > the_last_val_loss:\n",
    "            trigger_times += 1\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping at epoch',epoch,\" with patience\",patience)\n",
    "                return model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch\n",
    "\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "    return model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device state:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Encoding dataset\n",
      "Shape of peptides (3228, 10, 12)\n",
      "Shape of hla (3228, 34, 12)\n",
      "Shape of binding_scores (3228, 1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the encoding schemes\n",
    "aaindex_PCA = pd.read_csv('../data/PCA_repr_aa.csv',index_col=0)\n",
    "blosum62 = load_blossum62_matrix()\n",
    "blosum62 = (blosum62 - blosum62.mean()) / blosum62.std()\n",
    "\n",
    "# HLA\n",
    "hla_database = pd.read_csv('../data/formatted_hla2paratope_MHC_pseudo.dat', sep=' ',index_col=0)\n",
    "# hla_database = pd.read_csv('../data/MHC_full.dat', sep=' ',index_col=0)\n",
    "hla_dic = hla_database.to_dict(\"dict\")[\"pseudo\"]\n",
    "\n",
    "# Reading the data \n",
    "all_data = pd.read_csv(\"../data/ifng_true_balanced_w_parts_w_binding_scores_w_iedb.csv\")\n",
    "all_data = all_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "print(\"## Encoding dataset\")\n",
    "\n",
    "all_peptides_encoded,all_HLA_encoded,all_binding_scores_encoded,all_label_encoded = encode_dataset(all_data,aaindex_PCA,blosum62,hla_dic,peptide_len=10,padding=\"right\")\n",
    "\n",
    "\n",
    "print(\"Shape of peptides\",all_peptides_encoded.shape)\n",
    "print(\"Shape of hla\",all_HLA_encoded.shape)\n",
    "print(\"Shape of binding_scores\",all_binding_scores_encoded.shape)\n",
    "\n",
    "# print(\"## Encoding semi-supervised\")\n",
    "# semisup_data = pd.read_csv(\"../data/semi_supervised_data_w_binding_no_overlap_astrid.csv\")\n",
    "# semisup_data = semisup_data.sample(frac=0.1, random_state=1).reset_index(drop=True)\n",
    "# semisup_peptides_encoded, semisup_HLA_encoded, semisup_binding_scores_encoded,_ = encode_dataset(semisup_data,aaindex_PCA,hla_dic,peptide_len=10,padding=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the val and train parts\n",
      "[1] [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Epoch  1 : Train Loss 0.278900 , Validation loss 0.238688\n",
      "Epoch  6 : Train Loss 0.203640 , Validation loss 0.216487\n",
      "Epoch 11 : Train Loss 0.178590 , Validation loss 0.218540\n",
      "Epoch 16 : Train Loss 0.156706 , Validation loss 0.216072\n",
      "Epoch 21 : Train Loss 0.141678 , Validation loss 0.213014\n",
      "Epoch 26 : Train Loss 0.126271 , Validation loss 0.211860\n",
      "Epoch 31 : Train Loss 0.114081 , Validation loss 0.213217\n",
      "Epoch 36 : Train Loss 0.098197 , Validation loss 0.210396\n",
      "Epoch 41 : Train Loss 0.091323 , Validation loss 0.209297\n",
      "Epoch 46 : Train Loss 0.081653 , Validation loss 0.216466\n",
      "Epoch 51 : Train Loss 0.077664 , Validation loss 0.218269\n",
      "Epoch 56 : Train Loss 0.067605 , Validation loss 0.211884\n",
      "Epoch 61 : Train Loss 0.060953 , Validation loss 0.215076\n"
     ]
    }
   ],
   "source": [
    "N = len(all_data)\n",
    "no_epoch = 64\n",
    "lst_train_accuracies = []\n",
    "\n",
    "lst_val_losses = []\n",
    "lst_val_predictions = []\n",
    "lst_val_labels = []\n",
    "\n",
    "lst_test_targets = []\n",
    "lst_test_predictions = []\n",
    "\n",
    "lst_roc_auc = []\n",
    "lst_pr_auc = []\n",
    "\n",
    "best_roc_auc_indx = 0\n",
    "best_roc_auc = 0\n",
    "\n",
    "# Chose parameters \n",
    "# Best current parameters RNN\n",
    "batch_size = 40\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "dropout = 0.4\n",
    "RNN_encodings = 10\n",
    "\n",
    "\n",
    "## The partitions to use for training, validation ##\n",
    "test_parts = [0]\n",
    "validation_parts = [1]\n",
    "training_parts = [j for j in range(2,10)]\n",
    "print(\"the val and train parts\")\n",
    "print(validation_parts, training_parts)\n",
    "\n",
    "train_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_label_encoded = all_label_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "\n",
    "val_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_label_encoded = all_label_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "\n",
    "test_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_label_encoded = all_label_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "\n",
    "## Batches for training the model ##\n",
    "peptide_train_loader = list(DataLoader(train_peptides_encoded,batch_size=batch_size))\n",
    "HLA_train_loader = list(DataLoader(train_HLA_encoded,batch_size=batch_size))\n",
    "binding_score_train_loader = list(DataLoader(train_binding_scores_encoded,batch_size=batch_size))\n",
    "label_train_loader = list(DataLoader(train_label_encoded,batch_size=batch_size))\n",
    "\n",
    "peptide_val_loader = list(DataLoader(val_peptides_encoded,batch_size=batch_size))\n",
    "HLA_val_loader = list(DataLoader(val_HLA_encoded,batch_size=batch_size))\n",
    "binding_score_val_loader = list(DataLoader(val_binding_scores_encoded,batch_size=batch_size))\n",
    "label_val_loader = list(DataLoader(val_label_encoded,batch_size=batch_size))\n",
    "\n",
    "peptide_test_loader = list(DataLoader(test_peptides_encoded,batch_size=len(test_label_encoded)))\n",
    "HLA_test_loader = list(DataLoader(test_HLA_encoded,batch_size=len(test_label_encoded)))\n",
    "binding_score_test_loader = list(DataLoader(test_binding_scores_encoded,batch_size=len(test_label_encoded)))\n",
    "label_test_loader = list(DataLoader(test_label_encoded,batch_size=len(test_label_encoded)))\n",
    "\n",
    "train_loaders = (peptide_train_loader, HLA_train_loader, label_train_loader, binding_score_train_loader)\n",
    "val_loaders = (peptide_val_loader, HLA_val_loader, label_val_loader, binding_score_val_loader)\n",
    "test_loaders = (peptide_test_loader, HLA_test_loader, label_test_loader, binding_score_test_loader)\n",
    "torch.manual_seed(0)\n",
    "net = best_RNN() # Pick another model here :) \n",
    "net.apply(initialize_weights)\n",
    "trained_model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch = train(net,device,no_epoch,train_loaders,val_loaders,test_loaders, learning_rate, weight_decay)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "The test performance of the best model selected based on the Valdiation data:\n",
      "roc-AUC: 0.778 \n",
      "auPR: 0.79\n"
     ]
    }
   ],
   "source": [
    "def calculate_roc_auc(targets, predictions):\n",
    "    \"\"\"calculate the roc auc\n",
    "\n",
    "    Args:\n",
    "        targets (list): list of target values for given epoch\n",
    "        predictions (list): list of model predictions for given epoch\n",
    "\n",
    "    Returns:\n",
    "        float: the roc auc value\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = metrics.roc_curve(targets,predictions)\n",
    "    roc_auc = round(metrics.auc(fpr,tpr),3)\n",
    "    return roc_auc\n",
    "\n",
    "def calculate_pr_auc(targets, predictions):\n",
    "    \"\"\"calculate the pr auc\n",
    "\n",
    "    Args:\n",
    "        targets (list): list of target values for given epoch\n",
    "        predictions (list): list of model predictions for given epoch\n",
    "\n",
    "    Returns:\n",
    "        float: the pr auc value\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(targets,predictions)\n",
    "    pr_auc = round(metrics.auc(recall, precision),3)\n",
    "    return pr_auc\n",
    "\n",
    "def get_testing_performance(all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch,all_test_predictions_pr_epoch):\n",
    "    \"\"\"get the roc auc and pr auc on testing data \n",
    "\n",
    "    Args:\n",
    "        all_val_targets_pr_epoch (list): list of targets pr epoch\n",
    "        all_val_predictions_pr_epoch (list): list of model predictions pr epoch\n",
    "        all_test_targets_pr_epoch (list): list of targets pr epoch\n",
    "        all_test_predictions_pr_epoch (list): list of model predictions pr epoch\n",
    "\n",
    "    Returns:\n",
    "        roc_test: roc auc value on test data\n",
    "        auPR_test: pr auc value on test data \n",
    "    \"\"\" \n",
    "    # Finding the best epoch\n",
    "    roc_scores = []\n",
    "    for targets,predictions in zip(all_val_targets_pr_epoch,all_val_predictions_pr_epoch):\n",
    "        roc_scores.append(calculate_roc_auc(targets,predictions))\n",
    "    \n",
    "    best_validation_epoch = np.argmax(roc_scores)\n",
    "    print(best_validation_epoch)\n",
    "    roc_test = calculate_roc_auc(all_test_targets_pr_epoch[best_validation_epoch][0],all_test_predictions_pr_epoch[best_validation_epoch][0])\n",
    "    auPR_test = calculate_pr_auc(all_test_targets_pr_epoch[best_validation_epoch][0],all_test_predictions_pr_epoch[best_validation_epoch][0])\n",
    "    \n",
    "\n",
    "    return roc_test, auPR_test\n",
    "\n",
    "roc_test, auPR_test = get_testing_performance(all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch,all_test_predictions_pr_epoch)\n",
    "print(f\"The test performance of the best model selected based on the Valdiation data:\\nroc-AUC: {roc_test} \\nauPR: {auPR_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 5, 4, 6, 8, 9, 7, 3]\n",
      "[0.0885094159507073, 0.08645810510266767, 0.07586633042525351, 0.06908534848461646, 0.06700936405109648, 0.06550938021654903, 0.05991443200555415, 0.05221457170909677, 0.04413099461775514, 0.04335760533493969]\n",
      "[0.019335307788169113, 0.022580097449498834, 0.02521879805320211, 0.02837133573267345, 0.028786454505087475, 0.030843019297128047, 0.027739764776061382, 0.02664423480742423, 0.025777109290309973, 0.027591747110032136]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def occlude_peptide_position(input_data,occlusion_positions):\n",
    "    \"\"\"Masks row of input data with 0-mask\n",
    "\n",
    "    Args:\n",
    "        input_data (tensor): Tensor containg the peptide encoding scheme\n",
    "        occlusion_positions (tuple): Positions to mask\n",
    "\n",
    "    Returns:\n",
    "        tensor: Data with masked position\n",
    "    \"\"\"\n",
    "    input_data = input_data.copy()\n",
    "    input_data[:,occlusion_positions,:] = 0\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def occlusion_sensitivity_analysis(model, valid_loaders,N_positions,baseline_score):\n",
    "    \"\"\"Do occlusion sensitivity analysis \n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): trained neural network\n",
    "        valid_loaders (Data loader): Data loader for validation data\n",
    "        N_positions (int): position(s) to occlude\n",
    "        baseline_score (float): AUC of model on non-occluded peptides\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    peptide_val_loader,HLA_val_loader,label_val_loader,binding_score_val_loader = valid_loaders\n",
    "    model.eval()\n",
    "    positions = range(N_positions)\n",
    "    occlussions = [list(itertools.combinations(positions,1)),list(itertools.combinations(positions,2)),list(itertools.combinations(positions,3))]\n",
    "    base_line_auc = baseline_score\n",
    "    position_occlusion_dict = {pos:[] for pos in range(N_positions)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for length, occlusion_combinations in enumerate(occlussions):\n",
    "            for occlusion in occlusion_combinations:\n",
    "                all_val_targets = []\n",
    "                all_val_predictions = []\n",
    "                for j in range(len((peptide_val_loader))):\n",
    "                    val_peptides = peptide_val_loader[j].to(device)\n",
    "                    occluded_peptides = torch.clone(val_peptides)\n",
    "                    occluded_peptides[:,occlusion,:] = 0\n",
    "                    val_HLA = HLA_val_loader[j].to(device)\n",
    "                    val_labels = label_val_loader[j].to(device)\n",
    "                    val_binding_scores = binding_score_val_loader[j].to(device)\n",
    "                    outputs = model(occluded_peptides,val_HLA)\n",
    "                    all_val_predictions += outputs.cpu().numpy().tolist()\n",
    "                    all_val_targets += val_labels.cpu().numpy().tolist()\n",
    "                fpr, tpr, threshold = metrics.roc_curve(all_val_targets,all_val_predictions)\n",
    "                roc_auc = metrics.auc(fpr,tpr)\n",
    "                difference_from_baseline = base_line_auc-roc_auc\n",
    "                for occluded_position in occlusion:\n",
    "                    position_occlusion_dict[occluded_position].append(difference_from_baseline)\n",
    "\n",
    "    \n",
    "    average_decrease_in_performance = {pos:np.mean(position_occlusion_dict[pos]) for pos in position_occlusion_dict.keys()}\n",
    "    std_decrease_in_performance = {pos:np.std(position_occlusion_dict[pos]) for pos in position_occlusion_dict.keys()}\n",
    "\n",
    "    sorted_occlusions = sorted(average_decrease_in_performance.keys(), key= lambda x: average_decrease_in_performance[x],reverse=True)\n",
    "    sorted_decreases = [average_decrease_in_performance[occ] for occ in sorted_occlusions]\n",
    "    sorted_std =  [std_decrease_in_performance[occ] for occ in sorted_occlusions]\n",
    "\n",
    "    print(sorted_occlusions)\n",
    "    print(sorted_decreases)\n",
    "    print(sorted_std)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.bar(np.arange(len(sorted_decreases)), sorted_decreases)\n",
    "    # plt.errorbar(np.arange(len(sorted_decreases)), sorted_decreases,yerr=sorted_std, fmt=\"\", color=\"k\", ls=\"\",capsize=10)\n",
    "    plt.xticks(np.arange(len(sorted_decreases)),sorted_occlusions,rotation=90, size=8)\n",
    "    plt.xlabel(\"Occludded positions\")\n",
    "    plt.ylabel(\"Decrease in performance\")\n",
    "    plt.show()\n",
    "\n",
    "occlusion_sensitivity_analysis(trained_model, test_loaders, 10, 0.778)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating pan-specific model on HLA-restricted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling only the HLA-A*02:01 and 9-mers\n",
    "all_data[\"peptide_len\"] = all_data[\"peptide\"].apply(len)\n",
    "hla_0201_9mer = all_data[(all_data[\"HLA_allele\"] == \"HLA-A*02:01\") & (all_data[\"peptide_len\"] == 9)]\n",
    "hla_0201_9mer = hla_0201_9mer[hla_0201_9mer[\"parts\"] == 0.0]\n",
    "all_peptides_encoded_test_HLA,all_HLA_encoded_test_HLA,all_binding_scores_encoded_test_HLA,all_label_encoded_test_HLA = encode_dataset(hla_0201_9mer,aaindex_PCA,blosum62,hla_dic,peptide_len=10,padding=\"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "The test performance of the best model selected based on the Valdiation data on the HLA-A*02:01:\n",
      "roc-AUC: 0.769 \n",
      "auPR: 0.838\n"
     ]
    }
   ],
   "source": [
    "test_peptides_encoded = all_peptides_encoded_test_HLA\n",
    "test_HLA_encoded = all_HLA_encoded_test_HLA\n",
    "test_binding_scores_encoded = all_binding_scores_encoded_test_HLA\n",
    "test_label_encoded = all_label_encoded_test_HLA\n",
    "peptide_test_loader = list(DataLoader(test_peptides_encoded,batch_size=len(test_label_encoded)))\n",
    "HLA_test_loader = list(DataLoader(test_HLA_encoded,batch_size=len(test_label_encoded)))\n",
    "binding_score_test_loader = list(DataLoader(test_binding_scores_encoded,batch_size=len(test_label_encoded)))\n",
    "label_test_loader = list(DataLoader(test_label_encoded,batch_size=len(test_label_encoded)))\n",
    "\n",
    "trained_model.eval()\n",
    "lst_test_predictions = []\n",
    "lst_test_targets = []\n",
    "with torch.no_grad():\n",
    "    print(len(peptide_test_loader[0]))\n",
    "    test_peptides = peptide_test_loader[0].to(device)\n",
    "    test_HLA = HLA_test_loader[0].to(device)\n",
    "    test_labels = label_test_loader[0].to(device)\n",
    "    test_binding_scores = binding_score_test_loader[0].to(device)\n",
    "    outputs = trained_model(test_peptides,test_HLA)\n",
    "    lst_test_predictions.append(outputs.cpu().numpy().tolist())\n",
    "    lst_test_targets.append(test_labels.cpu().numpy().tolist())\n",
    "\n",
    "roc_test = calculate_roc_auc(lst_test_targets[0],lst_test_predictions[0])\n",
    "auPR_test = calculate_pr_auc(lst_test_targets[0],lst_test_predictions[0])\n",
    "\n",
    "\n",
    "print(f\"The test performance of the best model selected based on the Valdiation data on the HLA-A*02:01:\\nroc-AUC: {roc_test} \\nauPR: {auPR_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
