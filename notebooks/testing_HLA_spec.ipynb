{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform(m.weight.data, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model structures from the \"model_structures.py\" script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in model: 375361\n",
      "best_RNN(\n",
      "  (peptide_encoding): LSTM(12, 10, batch_first=True, bidirectional=True)\n",
      "  (hla_encoding): LSTM(12, 10, batch_first=True, bidirectional=True)\n",
      "  (drop_out): Dropout(p=0.4, inplace=False)\n",
      "  (L_in): Linear(in_features=860, out_features=430, bias=True)\n",
      "  (batchnorm1): BatchNorm1d(430, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (L_2): Linear(in_features=430, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[0.5778],\n",
      "        [0.5094],\n",
      "        [0.6287],\n",
      "        [0.6141],\n",
      "        [0.5521],\n",
      "        [0.6250],\n",
      "        [0.5142],\n",
      "        [0.6862],\n",
      "        [0.5793],\n",
      "        [0.5204]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model_structures import *\n",
    "net = best_RNN()\n",
    "print(\"Number of parameters in model:\", get_n_params(net))\n",
    "print(net)\n",
    "\n",
    "# Testing flow-through of information within the netwowkr\n",
    "peptide_random = np.random.normal(0,1, (10, 9, 12)).astype('float32')\n",
    "peptide_random = Variable(torch.from_numpy(peptide_random))\n",
    "HLA_random = np.random.normal(0,1, (10, 34, 12)).astype('float32')\n",
    "HLA_random = Variable(torch.from_numpy(HLA_random))\n",
    "binding_random = np.random.normal(0,1, (10, 1)).astype('float32')\n",
    "binding_random = Variable(torch.from_numpy(binding_random))\n",
    "output = net(peptide_random,HLA_random)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading and encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_peptide_onehot(aa_seq):\n",
    "    \"\"\"Enocding an aa-seqquence using the One-hot scheme\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): Peptide sequence to encode\n",
    "\n",
    "    Returns:\n",
    "        np.array: Encoded peptide\n",
    "    \"\"\"\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F','P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    one_hot_matrix = pd.DataFrame(np.identity(len(amino_acids)).astype(\"float32\"))\n",
    "    one_hot_matrix.index = amino_acids\n",
    "    encoded_aa_seq = []\n",
    "\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for _ in range(len(amino_acids))]))\n",
    "        try:    \n",
    "            encoded_aa_seq.append(one_hot_matrix.loc[aa].to_numpy())\n",
    "        except KeyError:\n",
    "            print(\"Encoding error\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "\n",
    "    encoded_aa_seq = np.array(encoded_aa_seq)\n",
    "    return encoded_aa_seq\n",
    "\n",
    "\n",
    "def load_blossum62_matrix():\n",
    "    \"\"\"Loads the blossum62 substitution matrix using the BioPython library\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas dataframe which holds the blossum62 sub-matrix\n",
    "    \"\"\"\n",
    "    from Bio.Align import substitution_matrices\n",
    "    blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
    "    blossum_aas = list(\"ARNDCQEGHILKMFPSTWYVBZX*\")\n",
    "    blosum62 = pd.DataFrame(blosum62,columns=blossum_aas,index=blossum_aas)\n",
    "    return blosum62\n",
    "\n",
    "\n",
    "def encode_peptide_blossum65(aa_seq,blussom_matrix):\n",
    "    \"\"\"Enocding an aa-seqquence using the Blossum62 encoding scheme\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        blussom_matrix (pd.DataFrame): A pandas dataframe which holds the blossum62 sub-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    AAs = blussom_matrix.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for _ in range(AAs)]))\n",
    "        else:\n",
    "            try:\n",
    "                encoded_aa_seq.append(blussom_matrix.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    \n",
    "    encoded_aa_seq = np.array(encoded_aa_seq)\n",
    "    return encoded_aa_seq\n",
    "\n",
    "\n",
    "def encode_peptide_aaindex(aa_seq,aaindex_PCA):\n",
    "    \"\"\"Enocding an aa-seqquence using the AAindex encoding scheme.\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        aaindex_PCA (pd.DataFrame):  A pandas dataframe which holds the AAindex encoding scheme\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    PCs = aaindex_PCA.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for x in range(PCs)]))\n",
    "        else:\n",
    "            try:\n",
    "                encoded_aa_seq.append(aaindex_PCA.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    return np.array(encoded_aa_seq)\n",
    "\n",
    "def encode_multiple(aa_seq,aaindex_PCA,blussom_matrix):\n",
    "    \"\"\"Enocding an aa-seqquence using the the Combined encoding schemes of AAindex, Blossum62 and One-hot.\n",
    "\n",
    "    Args:\n",
    "        aa_seq (str): The aa-sequence we want to encode\n",
    "        aaindex_PCA (pd.DataFrame):  A pandas dataframe which holds the AAindex encoding scheme\n",
    "        blussom_matrix (pd.DataFrame):  A pandas dataframe which holds the blossum62 sub-matrix\n",
    "\n",
    "    Returns:\n",
    "        np.array: The encoded peptide sequence\n",
    "    \"\"\"\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F','P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    one_hot_matrix = pd.DataFrame(np.identity(len(amino_acids)).astype(\"float32\"))\n",
    "    one_hot_matrix.index = amino_acids\n",
    "    encoded_aa_seq = []\n",
    "\n",
    "\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    PCs = aaindex_PCA.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for x in range(56)]))\n",
    "        else:\n",
    "            try:\n",
    "                aa_index_encoding = aaindex_PCA.loc[aa].to_numpy()\n",
    "                blossum_encoding = blussom_matrix.loc[aa].to_numpy()\n",
    "                \n",
    "                onehot_encoding = one_hot_matrix.loc[aa].to_numpy()\n",
    "                encoding = np.concatenate((aa_index_encoding,blossum_encoding,onehot_encoding))\n",
    "                encoded_aa_seq.append(encoding)\n",
    "\n",
    "            except KeyError:\n",
    "                print(\"Encoding error\")\n",
    "                sys.exit(1)\n",
    "    return np.array(encoded_aa_seq)\n",
    "\n",
    "def encode_dataset(df,aa_index_matrix,blosum62_matrix,HLA_dict,peptide_len,padding=\"right\"):\n",
    "    \"\"\"Encodes the filtered, balanced and partioned dataset. This is done in three major steps. \n",
    "    1) The MHCI-allele code is converted to the paratope sequence. \n",
    "    2) The MHCI is encoded using one of four different schemes.\n",
    "    3) The peptide sequence is encoded using one of four different schemes.\n",
    "    4) The peptide sequence is padded to a specified length.\n",
    "    5) Encoding the binding score retrived from netMHCpan\n",
    "    6) The output label y is created based on the number of positive subjects from the raw data\n",
    "        1 ~ If number of positive subjects is greater than 0\n",
    "        0 ~ If number of positive subjects is equal to 0\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): The dataframe containing the dataset which is to be encoded\n",
    "        aa_index_matrix (pd.DataFrame): A pandas dataframe which holds the AAindex encoding scheme\n",
    "        blussom_matrix (pd.DataFrame):  A pandas dataframe which holds the blossum62 sub-matrix\n",
    "        HLA_dict (dict): A dictionary used to translate the MHCI-allele code is converted to the paratope sequence. \n",
    "        peptide_len (int): The length we want the encoded sequence to have.\n",
    "        padding (str, optional): The strategy used to pad the peptide sequences. Defaults to \"right\".\n",
    "\n",
    "    Returns:\n",
    "        np.array: Encoded peptide\n",
    "        np.array: Encoded HLA\n",
    "        np.array: Encoded Binding-score\n",
    "        np.array: Encoded Labels\n",
    "\n",
    "    \"\"\"\n",
    "    encoded_peptides = []\n",
    "    encoded_labels = []\n",
    "    encoded_hlas = []\n",
    "    encoded_binding_scores = []\n",
    "    for i,row in df.iterrows():\n",
    "        \n",
    "        HLA = HLA_dict[row[\"HLA_allele\"].replace(\":\",\"\")]\n",
    "        #encoded_HLA = encode_peptide_aaindex(HLA,aa_index_matrix)\n",
    "        encoded_HLA = encode_peptide_onehot(HLA)\n",
    "        # encoded_HLA = encode_peptide_blossum65(HLA,blosum62_matrix)\n",
    "        # encoded_HLA = encode_multiple(HLA,aaindex_PCA,blosum62_matrix)\n",
    "\n",
    "        peptide = row[\"peptide\"]\n",
    "        #encoded_peptide = encode_peptide_aaindex(peptide,aa_index_matrix)\n",
    "        encoded_peptide = encode_peptide_onehot(peptide)\n",
    "        # encoded_peptide = encode_peptide_blossum65(peptide,blosum62_matrix)\n",
    "        # encoded_peptide = encode_multiple(peptide,aaindex_PCA,blosum62_matrix)\n",
    "        \n",
    "\n",
    "        binding_score = row['binding_score']\n",
    "\n",
    "        # Adding padding\n",
    "        if len(encoded_peptide) < peptide_len:\n",
    "            n_added = peptide_len-len(encoded_peptide)\n",
    "            if padding == \"right\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((0, 1), (0, 0)), 'constant')\n",
    "            elif padding == \"left\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((1, 0), (0, 0)), 'constant')\n",
    "            elif padding == \"random\":\n",
    "                top_pad = random.choice([0,1])\n",
    "                bot_pad = 1-top_pad\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((top_pad, bot_pad), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "        encoded_label = min(1,row[\"positive_subjects\"])\n",
    "        encoded_peptides.append(encoded_peptide)\n",
    "        encoded_hlas.append(encoded_HLA)\n",
    "        encoded_labels.append(encoded_label)\n",
    "        encoded_binding_scores.append(binding_score)\n",
    "    \n",
    "    encoded_peptides = np.array(encoded_peptides).astype('float32')\n",
    "    encoded_hlas = np.array(encoded_hlas).astype('float32')\n",
    "    encoded_labels = np.array(encoded_labels).astype('float32').reshape(-1,1)\n",
    "    encoded_binding_scores = np.array(encoded_binding_scores).astype('float32').reshape(-1,1)\n",
    "\n",
    "    return encoded_peptides, encoded_hlas, encoded_binding_scores, encoded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for plotting model statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(K, lst_train_acc, lst_val_acc): \n",
    "    \"\"\"For each fold, plot the accuracy on\n",
    "    train and validation data for each epoch\n",
    "\n",
    "    Args:\n",
    "        K (int): Number of CV folds\n",
    "        lst_train_acc (list of lists): list of lists containing training accuracy for each fold pr epoch\n",
    "        lst_val_acc (list of lists): list of lists containing validation accuracy for each fold pr epoch\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    for i in range(K):\n",
    "        epoch = np.arange(len(lst_train_acc[i]))\n",
    "        plt.plot(epoch, lst_train_acc[i], 'r', epoch, lst_val_acc[i], 'b')\n",
    "    plt.title(\"Performance of {} fold CV\".format(K))\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('epochs'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_avg_val_performance(K, N, lst_val_acc, lst_val_lab, lst_val_pred):\n",
    "    \"\"\"Calculate the generalization error\n",
    "\n",
    "    Args:\n",
    "        lst_val_acc (list of lists): list of lists containing validation accuracies for each fold\n",
    "        lst_val_lab (list of lists): list of lists containing validation labels for each fold\n",
    "        lst_val_pred (list of lists): list of lists containing validation predictions for each fold\n",
    "        N (int): Total number of observations in data set\n",
    "    \"\"\"\n",
    "    avg_recall = 0\n",
    "    avg_accuracy = 0 \n",
    "    avg_f1 = 0\n",
    "\n",
    "    res = np.zeros((K,5))\n",
    "    for i in range(K):\n",
    "        best_epoch_model = np.argmax(lst_val_acc[i])\n",
    "        n = len(lst_val_lab[i][best_epoch_model])\n",
    "        accuracy = accuracy_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "        recall = recall_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "        f1 = f1_score(lst_val_lab[i][best_epoch_model],lst_val_pred[i][best_epoch_model])\n",
    "\n",
    "        res[i][0] = best_epoch_model\n",
    "        res[i][1] = n\n",
    "        res[i][2] = accuracy\n",
    "        res[i][3] = recall\n",
    "        res[i][4] = f1\n",
    "\n",
    "        avg_recall += (n/N) * recall\n",
    "        avg_accuracy += (n/N) * accuracy\n",
    "        avg_f1 += (n/N) * f1\n",
    "\n",
    "    print(f\"Best average results - Recall: {avg_recall} accuracy: {avg_accuracy} f1-score: {avg_f1}\")\n",
    "    return res\n",
    "\n",
    "def plot_roc_curve_best_epoch(valid_losses, predictions, targets):\n",
    "    \"\"\"Plots the ROC curve for the best epoch\n",
    "\n",
    "    Args:\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    best_epoch_model = np.argmin(valid_losses)\n",
    "    print(\"Best Epoch\",best_epoch_model)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(targets[best_epoch_model],predictions[best_epoch_model])\n",
    "    roc_auc = metrics.auc(fpr,tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b',label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "def plot_PR_curve_best_epoch(valid_losses, predictions, targets):\n",
    "    \"\"\"Plots the PR curve for the best epoch\n",
    "\n",
    "    Args:\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    best_epoch_model = np.argmin(valid_losses)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(targets[best_epoch_model], predictions[best_epoch_model])\n",
    "    roc_auc = metrics.auc(recall, precision)\n",
    "    plt.title('Precission-Recall curve')\n",
    "    plt.plot(recall, precision, 'b', label = 'AUCpr = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Precession')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pick_optimal_threshold_auc(fpr, tpr, threshold):\n",
    "    \"\"\"Picks the optimal threshold for determining \n",
    "    immunogenic/non-immunogenic targets on unlabelled data\n",
    "\n",
    "    Args:\n",
    "        fpr (float): false positive rate  \n",
    "        tpr (float): true positive rate\n",
    "        threshold (float): The current threshold to update\n",
    "\n",
    "    Returns:\n",
    "        int: position of highest geometric mean\n",
    "    \"\"\"\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    return ix\n",
    "\n",
    "def plot_all_roc_curves(K,valid_losses, predictions, targets):\n",
    "    \"\"\"Plot of ROC curve for all K CV folds\n",
    "\n",
    "    Args:\n",
    "        K (int): number of CV folds\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    for k in range(K):\n",
    "        best_epoch_model = np.argmin(valid_losses[k])\n",
    "        \n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(targets[k][best_epoch_model],predictions[k][best_epoch_model])\n",
    "        roc_auc = round(metrics.auc(fpr,tpr),3)\n",
    "        best_threshold = pick_optimal_threshold_auc(fpr, tpr, threshold)\n",
    "        print(f\"Best Epoch in K {k}\",best_epoch_model,\"best threshold:\",threshold[best_threshold])\n",
    "        plt.plot(fpr, tpr,label = f'CV {k+1} AUC {roc_auc}')\n",
    "        plt.plot(fpr[best_threshold], tpr[best_threshold],color=\"black\",marker=\"d\")\n",
    "        plt.plot()\n",
    "\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_all_PR_curves(K,valid_losses, predictions, targets):\n",
    "    \"\"\"Plot of PR curve for all K CV folds\n",
    "\n",
    "    Args:\n",
    "        K (int): number of CV folds\n",
    "        valid_losses (list): list of validation loss\n",
    "        predictions (list of lists): model predictions for all epochs per CV fold\n",
    "        targets (list of lists): target values for all epochs per CV fold\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    for k in range(K):\n",
    "        best_epoch_model = np.argmin(valid_losses[k])\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(targets[k][best_epoch_model], predictions[k][best_epoch_model])\n",
    "        roc_auc = round(metrics.auc(recall, precision),3)\n",
    "        plt.plot(recall, precision,label = f'CV {k+1} AUC {roc_auc}')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device state: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_learning_curve(train_accuracies,val_accuracies):\n",
    "    \"\"\"Plot accuracy during training\n",
    "\n",
    "    Args:\n",
    "        train_accuracies (list): list of training accuracies on target values in current CV fold\n",
    "        val_accuracies (list): list of validation accuracies on target values in current CV fold\n",
    "    \"\"\"\n",
    "    epoch = np.arange(len(train_accuracies))\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_accuracies, 'r', epoch, val_accuracies, 'b')\n",
    "    plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "    plt.xlabel('epochs'), plt.ylabel('Acc')\n",
    "\n",
    "\n",
    "def validation(model,device,valid_loaders,train_loaders):\n",
    "    \"\"\"Generates model predictions on train and validation data\n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): trained neural network \n",
    "        device (state): either cuda enabled or not\n",
    "        valid_loaders (Data loader): Data loader for validation data\n",
    "        train_loaders (Data loader): Data loader for training data\n",
    "\n",
    "    Returns:\n",
    "        lists: lists of predictions, targets and validation loss\n",
    "    \"\"\"\n",
    "    peptide_val_loader,HLA_val_loader,label_val_loader,binding_score_val_loader = valid_loaders\n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_train_predictions = []\n",
    "        all_train_targets = []\n",
    "        for i in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[i].to(device)\n",
    "            train_HLA = HLA_train_loader[i].to(device)\n",
    "            train_labels = label_train_loader[i].to(device)\n",
    "            train_binding_scores = binding_score_train_loader[i].to(device)\n",
    "            outputs = model(train_peptides,train_HLA)\n",
    "            all_train_predictions += outputs.cpu().numpy().tolist()\n",
    "            all_train_targets += train_labels.cpu().numpy().tolist()\n",
    "        \n",
    "        all_val_targets = []\n",
    "        all_val_predictions = []\n",
    "        for j in range(len((peptide_val_loader))):\n",
    "            val_peptides = peptide_val_loader[j].to(device)\n",
    "            val_HLA = HLA_val_loader[j].to(device)\n",
    "            val_labels = label_val_loader[j].to(device)\n",
    "            val_binding_scores = binding_score_val_loader[j].to(device)\n",
    "            outputs = model(val_peptides,val_HLA)\n",
    "            all_val_predictions += outputs.cpu().numpy().tolist()\n",
    "            all_val_targets += val_labels.cpu().numpy().tolist()\n",
    "\n",
    "        validation_loss = mean_squared_error(all_val_targets,all_val_predictions)\n",
    "        # loss_function = nn.BCELoss()\n",
    "        # targets = torch.tensor(all_val_targets)\n",
    "        # predictions = torch.tensor(all_val_predictions)\n",
    "        # validation_loss = loss_function(targets,predictions)/len(targets)\n",
    "        \n",
    "\n",
    "    return all_train_targets,all_train_predictions,all_val_targets,all_val_predictions,validation_loss\n",
    "\n",
    "\n",
    "def testing(model,device,test_loader):\n",
    "    \"\"\"Testing model every epoch on testing data\n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): Trained neural network\n",
    "        device (state): enable cuda or not\n",
    "        test_loader (Data loader): Data loader for test data \n",
    "\n",
    "    Returns:\n",
    "        lists: Target and prediction values \n",
    "    \"\"\"\n",
    "    peptide_test_loader,HLA_test_loader,label_test_loader,binding_score_test_loader = test_loader\n",
    "    model.eval()\n",
    "    all_test_predictions = []\n",
    "    all_test_targets = []\n",
    "    with torch.no_grad():\n",
    "        test_peptides = peptide_test_loader[0].to(device)\n",
    "        test_HLA = HLA_test_loader[0].to(device)\n",
    "        test_labels = label_test_loader[0].to(device)\n",
    "        test_binding_scores = binding_score_test_loader[0].to(device)\n",
    "        outputs = model(test_peptides,test_HLA)\n",
    "        all_test_predictions.append(outputs.cpu().numpy().tolist())\n",
    "        all_test_targets.append(test_labels.cpu().numpy().tolist())\n",
    "    return all_test_targets,all_test_predictions\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train(model, device, epochs, train_loaders, valid_loaders, test_loaders, learning_rate, weight_decay):\n",
    "    \"\"\"train the model \n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): untrained neural network\n",
    "        device (state): cuda enabled or not\n",
    "        epochs (int): number of epoch to do\n",
    "        train_loaders (Data loaders): Data loaders for training data\n",
    "        valid_loaders (Data loaders): Data loaders for validation data\n",
    "        test_loaders (Data loaders): Data loaders for test data\n",
    "        learning_rate (float) : learning rate value\n",
    "        weight_decay (float) : weight decay value\n",
    "\n",
    "    Returns:\n",
    "        pytorch model: trained neural network\n",
    "        lists: train and validation losses\n",
    "    \"\"\"\n",
    "    \n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Early stopping\n",
    "    the_last_loss = 100\n",
    "    patience = 4\n",
    "    trigger_times = 0\n",
    "    \n",
    "    all_val_targets_pr_epoch = []\n",
    "    all_val_predictions_pr_epoch = []\n",
    "\n",
    "    all_test_targets_pr_epoch = []\n",
    "    all_test_predictions_pr_epoch = []\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0\n",
    "        \n",
    "        for train_batch_index in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[train_batch_index].to(device)\n",
    "            train_HLA = HLA_train_loader[train_batch_index].to(device)\n",
    "            train_labels = label_train_loader[train_batch_index].to(device)\n",
    "            train_binding_scores = binding_score_train_loader[train_batch_index].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_peptides,train_HLA)\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "        train_losses.append(current_loss/len((peptide_train_loader)))\n",
    "\n",
    "        all_train_targets,all_train_predictions,all_val_targets,all_val_predictions,validation_loss = validation(model,device,valid_loaders,train_loaders)\n",
    "        \n",
    "\n",
    "        val_losses.append(validation_loss)\n",
    "        all_val_targets_pr_epoch.append(all_val_targets)\n",
    "        all_val_predictions_pr_epoch.append(all_val_predictions)\n",
    "\n",
    "        all_test_targets,all_test_predictions = testing(model,device,test_loaders)\n",
    "        all_test_targets_pr_epoch.append(all_test_targets)\n",
    "        all_test_predictions_pr_epoch.append(all_test_predictions)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch %2i : Train Loss %f , Validation loss %f\" % (epoch+1, train_losses[-1], val_losses[-1]))\n",
    "        \n",
    "\n",
    "        # Early stopping\n",
    "        the_current_val_loss = val_losses[-1]\n",
    "        the_last_val_loss = 0 if len(val_losses) < 2 else val_losses[-2]\n",
    "\n",
    "\n",
    "        if the_current_val_loss > the_last_val_loss:\n",
    "            trigger_times += 1\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping at epoch',epoch,\" with patience\",patience)\n",
    "                return model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch\n",
    "\n",
    "        else:\n",
    "            trigger_times = 0\n",
    "\n",
    "    return model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device state:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Encoding dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding error\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4554, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 4562, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'X'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-e597e34ba508>\", line 19, in encode_peptide_onehot\n",
      "    encoded_aa_seq.append(one_hot_matrix.loc[aa].to_numpy())\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\", line 895, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1124, in _getitem_axis\n",
      "    return self._get_label(key, axis=axis)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\", line 1073, in _get_label\n",
      "    return self.obj.xs(label, axis=axis)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 3739, in xs\n",
      "    loc = index.get_loc(key)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'X'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-cdf98fb34046>\", line 18, in <module>\n",
      "    all_peptides_encoded,all_HLA_encoded,all_binding_scores_encoded,all_label_encoded = encode_dataset(all_data,aaindex_PCA,blosum62,hla_dic,peptide_len=9,padding=\"right\")\n",
      "  File \"<ipython-input-11-e597e34ba508>\", line 173, in encode_dataset\n",
      "    encoded_peptide = encode_peptide_onehot(peptide)\n",
      "  File \"<ipython-input-11-e597e34ba508>\", line 22, in encode_peptide_onehot\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/anaconda3/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e597e34ba508>\u001b[0m in \u001b[0;36mencode_peptide_onehot\u001b[0;34m(aa_seq)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mencoded_aa_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3739\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-cdf98fb34046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mall_peptides_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_HLA_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_binding_scores_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_label_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maaindex_PCA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblosum62\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhla_dic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpeptide_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e597e34ba508>\u001b[0m in \u001b[0;36mencode_dataset\u001b[0;34m(df, aa_index_matrix, blosum62_matrix, HLA_dict, peptide_len, padding)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m#encoded_peptide = encode_peptide_aaindex(peptide,aa_index_matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mencoded_peptide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_peptide_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# encoded_peptide = encode_peptide_blossum65(peptide,blosum62_matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e597e34ba508>\u001b[0m in \u001b[0;36mencode_peptide_onehot\u001b[0;34m(aa_seq)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoding error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2052\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2053\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2054\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2055\u001b[0m                                                                      value))\n\u001b[1;32m   2056\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Loading the encoding schemes\n",
    "aaindex_PCA = pd.read_csv('../data/PCA_repr_aa.csv',index_col=0)\n",
    "blosum62 = load_blossum62_matrix()\n",
    "blosum62 = (blosum62 - blosum62.mean()) / blosum62.std()\n",
    "\n",
    "# HLA\n",
    "hla_database = pd.read_csv('../data/formatted_hla2paratope_MHC_pseudo.dat', sep=' ',index_col=0)\n",
    "# hla_database = pd.read_csv('../data/MHC_full.dat', sep=' ',index_col=0)\n",
    "hla_dic = hla_database.to_dict(\"dict\")[\"pseudo\"]\n",
    "\n",
    "# Reading the data \n",
    "all_data = pd.read_csv(\"../data/ifng_true_balanced_w_parts_w_binding_scores_w_iedb.csv\")\n",
    "all_data = all_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "all_data[\"peptide_len\"] = all_data[\"peptide\"].apply(len)\n",
    "all_data = all_data[(all_data[\"HLA_allele\"] == \"HLA-A*02:01\") & (all_data[\"peptide_len\"] == 9)]\n",
    "print(\"## Encoding dataset\")\n",
    "\n",
    "all_peptides_encoded,all_HLA_encoded,all_binding_scores_encoded,all_label_encoded = encode_dataset(all_data,aaindex_PCA,blosum62,hla_dic,peptide_len=9,padding=\"right\")\n",
    "\n",
    "\n",
    "print(\"Shape of peptides\",all_peptides_encoded.shape)\n",
    "print(\"Shape of hla\",all_HLA_encoded.shape)\n",
    "print(\"Shape of binding_scores\",all_binding_scores_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross-validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the val and train parts\n",
      "[2] [1, 3, 4, 5, 6, 7, 8, 9]\n",
      "Epoch  1 : Train Loss 0.300934 , Validation loss 0.250306\n",
      "Epoch  6 : Train Loss 0.207523 , Validation loss 0.213970\n",
      "Epoch 11 : Train Loss 0.178974 , Validation loss 0.203536\n",
      "Epoch 16 : Train Loss 0.155040 , Validation loss 0.202423\n",
      "Epoch 21 : Train Loss 0.141293 , Validation loss 0.200977\n",
      "Epoch 26 : Train Loss 0.128324 , Validation loss 0.200931\n",
      "Early stopping at epoch 27  with patience 4\n"
     ]
    }
   ],
   "source": [
    "N = len(all_data)\n",
    "no_epoch = 100\n",
    "lst_train_accuracies = []\n",
    "\n",
    "lst_val_losses = []\n",
    "lst_val_predictions = []\n",
    "lst_val_labels = []\n",
    "\n",
    "lst_test_targets = []\n",
    "lst_test_predictions = []\n",
    "\n",
    "lst_roc_auc = []\n",
    "lst_pr_auc = []\n",
    "\n",
    "best_roc_auc_indx = 0\n",
    "best_roc_auc = 0\n",
    "\n",
    "# Chose parameters \n",
    "# Best current parameters RNN\n",
    "batch_size = 40\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "dropout = 0.4\n",
    "RNN_encodings = 10\n",
    "\n",
    "\n",
    "## The partitions to use for training, validation ##\n",
    "test_parts = [0]\n",
    "validation_parts = [2]\n",
    "training_parts = [1,3,4,5,6,7,8,9]\n",
    "print(\"the val and train parts\")\n",
    "print(validation_parts, training_parts)\n",
    "\n",
    "train_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "train_label_encoded = all_label_encoded[all_data[\"parts\"].isin(training_parts)]\n",
    "\n",
    "val_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "val_label_encoded = all_label_encoded[all_data[\"parts\"].isin(validation_parts)]\n",
    "\n",
    "test_peptides_encoded = all_peptides_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_HLA_encoded = all_HLA_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_binding_scores_encoded = all_binding_scores_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "test_label_encoded = all_label_encoded[all_data[\"parts\"].isin(test_parts)]\n",
    "\n",
    "## Batches for training the model ##\n",
    "peptide_train_loader = list(DataLoader(train_peptides_encoded,batch_size=batch_size))\n",
    "HLA_train_loader = list(DataLoader(train_HLA_encoded,batch_size=batch_size))\n",
    "binding_score_train_loader = list(DataLoader(train_binding_scores_encoded,batch_size=batch_size))\n",
    "label_train_loader = list(DataLoader(train_label_encoded,batch_size=batch_size))\n",
    "\n",
    "peptide_val_loader = list(DataLoader(val_peptides_encoded,batch_size=batch_size))\n",
    "HLA_val_loader = list(DataLoader(val_HLA_encoded,batch_size=batch_size))\n",
    "binding_score_val_loader = list(DataLoader(val_binding_scores_encoded,batch_size=batch_size))\n",
    "label_val_loader = list(DataLoader(val_label_encoded,batch_size=batch_size))\n",
    "\n",
    "peptide_test_loader = list(DataLoader(test_peptides_encoded,batch_size=len(test_label_encoded)))\n",
    "HLA_test_loader = list(DataLoader(test_HLA_encoded,batch_size=len(test_label_encoded)))\n",
    "binding_score_test_loader = list(DataLoader(test_binding_scores_encoded,batch_size=len(test_label_encoded)))\n",
    "label_test_loader = list(DataLoader(test_label_encoded,batch_size=len(test_label_encoded)))\n",
    "\n",
    "train_loaders = (peptide_train_loader, HLA_train_loader, label_train_loader, binding_score_train_loader)\n",
    "val_loaders = (peptide_val_loader, HLA_val_loader, label_val_loader, binding_score_val_loader)\n",
    "test_loaders = (peptide_test_loader, HLA_test_loader, label_test_loader, binding_score_test_loader)\n",
    "torch.manual_seed(0)\n",
    "net = best_RNN()\n",
    "net.apply(initialize_weights)\n",
    "\n",
    "trained_model,train_losses,val_losses,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch, all_test_predictions_pr_epoch = train(net,device,no_epoch,train_loaders,val_loaders,test_loaders, learning_rate, weight_decay)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "The test performance of the best model selected based on the Valdiation data:\n",
      "roc-AUC: 0.746 \n",
      "auPR: 0.817\n"
     ]
    }
   ],
   "source": [
    "def calculate_roc_auc(targets, predictions):\n",
    "    \"\"\"calculate the roc auc\n",
    "\n",
    "    Args:\n",
    "        targets (list): list of target values for given epoch\n",
    "        predictions (list): list of model predictions for given epoch\n",
    "\n",
    "    Returns:\n",
    "        float: the roc auc value\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = metrics.roc_curve(targets,predictions)\n",
    "    roc_auc = round(metrics.auc(fpr,tpr),3)\n",
    "    return roc_auc\n",
    "\n",
    "def calculate_pr_auc(targets, predictions):\n",
    "    \"\"\"calculate the pr auc\n",
    "\n",
    "    Args:\n",
    "        targets (list): list of target values for given epoch\n",
    "        predictions (list): list of model predictions for given epoch\n",
    "\n",
    "    Returns:\n",
    "        float: the pr auc value\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(targets,predictions)\n",
    "    pr_auc = round(metrics.auc(recall, precision),3)\n",
    "    return pr_auc\n",
    "\n",
    "def get_testing_performance(all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch,all_test_predictions_pr_epoch):\n",
    "    \"\"\"get the roc auc and pr auc on testing data \n",
    "\n",
    "    Args:\n",
    "        all_val_targets_pr_epoch (list): list of targets pr epoch\n",
    "        all_val_predictions_pr_epoch (list): list of model predictions pr epoch\n",
    "        all_test_targets_pr_epoch (list): list of targets pr epoch\n",
    "        all_test_predictions_pr_epoch (list): list of model predictions pr epoch\n",
    "\n",
    "    Returns:\n",
    "        roc_test: roc auc value on test data\n",
    "        auPR_test: pr auc value on test data \n",
    "    \"\"\"\n",
    "    # Finding the best epoch\n",
    "    roc_scores = []\n",
    "    for targets,predictions in zip(all_val_targets_pr_epoch,all_val_predictions_pr_epoch):\n",
    "        roc_scores.append(calculate_roc_auc(targets,predictions))\n",
    "    \n",
    "    best_validation_epoch = np.argmax(roc_scores)\n",
    "    print(best_validation_epoch)\n",
    "    roc_test = calculate_roc_auc(all_test_targets_pr_epoch[best_validation_epoch][0],all_test_predictions_pr_epoch[best_validation_epoch][0])\n",
    "    auPR_test = calculate_pr_auc(all_test_targets_pr_epoch[best_validation_epoch][0],all_test_predictions_pr_epoch[best_validation_epoch][0])\n",
    "    return roc_test, auPR_test\n",
    "\n",
    "roc_test, auPR_test = get_testing_performance(all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_test_targets_pr_epoch,all_test_predictions_pr_epoch)\n",
    "print(f\"The test performance of the best model selected based on the Valdiation data:\\nroc-AUC: {roc_test} \\nauPR: {auPR_test}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occlusion sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 2, 1, 8, 5, 4, 7, 3]\n",
      "[0.06518049775192632, 0.06388513931371072, 0.06322074579217433, 0.051717127145698545, 0.04840769526483812, 0.04781015895301607, 0.04765555165555164, 0.04579190693476406, 0.014477661334804173]\n",
      "[0.024780367542950973, 0.0244053141568648, 0.02576004052609704, 0.02681621788683254, 0.03037726868935671, 0.028272949683176778, 0.028046328663316793, 0.027578559120738075, 0.023479891896630262]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def occlude_peptide_position(input_data,occlusion_positions):\n",
    "    \"\"\"Masks row of input data with 0-mask\n",
    "\n",
    "    Args:\n",
    "        input_data (tensor): Tensor containg the peptide encoding scheme\n",
    "        occlusion_positions (tuple): Positions to mask\n",
    "\n",
    "    Returns:\n",
    "        tensor: Data with masked position\n",
    "    \"\"\"\n",
    "    input_data = input_data.copy()\n",
    "    input_data[:,occlusion_positions,:] = 0\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def occlusion_sensitivity_analysis(model, valid_loaders,N_positions,baseline_score):\n",
    "    \"\"\"Do occlusion sensitivity analysis \n",
    "\n",
    "    Args:\n",
    "        model (pytorch model): trained neural network\n",
    "        valid_loaders (Data loader): Data loader for validation data\n",
    "        N_positions (int): position(s) to occlude\n",
    "        baseline_score (float): AUC of model on non-occluded peptides\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    peptide_val_loader,HLA_val_loader,label_val_loader,binding_score_val_loader = valid_loaders\n",
    "    model.eval()\n",
    "    positions = range(N_positions)\n",
    "    occlussions = [list(itertools.combinations(positions,1)),list(itertools.combinations(positions,2)),list(itertools.combinations(positions,3))]\n",
    "    base_line_auc = baseline_score\n",
    "    position_occlusion_dict = {pos:[] for pos in range(N_positions)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for length, occlusion_combinations in enumerate(occlussions):\n",
    "            for occlusion in occlusion_combinations:\n",
    "                all_val_targets = []\n",
    "                all_val_predictions = []\n",
    "                for j in range(len((peptide_val_loader))):\n",
    "                    val_peptides = peptide_val_loader[j].to(device)\n",
    "                    occluded_peptides = torch.clone(val_peptides)\n",
    "                    occluded_peptides[:,occlusion,:] = 0\n",
    "                    val_HLA = HLA_val_loader[j].to(device)\n",
    "                    val_labels = label_val_loader[j].to(device)\n",
    "                    val_binding_scores = binding_score_val_loader[j].to(device)\n",
    "                    outputs = model(occluded_peptides,val_HLA)\n",
    "                    all_val_predictions += outputs.cpu().numpy().tolist()\n",
    "                    all_val_targets += val_labels.cpu().numpy().tolist()\n",
    "                fpr, tpr, threshold = metrics.roc_curve(all_val_targets,all_val_predictions)\n",
    "                roc_auc = metrics.auc(fpr,tpr)\n",
    "                difference_from_baseline = base_line_auc-roc_auc\n",
    "                for occluded_position in occlusion:\n",
    "                    position_occlusion_dict[occluded_position].append(difference_from_baseline)\n",
    "\n",
    "    \n",
    "    average_decrease_in_performance = {pos:np.mean(position_occlusion_dict[pos]) for pos in position_occlusion_dict.keys()}\n",
    "    std_decrease_in_performance = {pos:np.std(position_occlusion_dict[pos]) for pos in position_occlusion_dict.keys()}\n",
    "\n",
    "    sorted_occlusions = sorted(average_decrease_in_performance.keys(), key= lambda x: average_decrease_in_performance[x],reverse=True)\n",
    "    sorted_decreases = [average_decrease_in_performance[occ] for occ in sorted_occlusions]\n",
    "    sorted_std =  [std_decrease_in_performance[occ] for occ in sorted_occlusions]\n",
    "\n",
    "    print(sorted_occlusions)\n",
    "    print(sorted_decreases)\n",
    "    print(sorted_std)\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.bar(np.arange(len(sorted_decreases)), sorted_decreases)\n",
    "    # plt.errorbar(np.arange(len(sorted_decreases)), sorted_decreases,yerr=sorted_std, fmt=\"\", color=\"k\", ls=\"\",capsize=10)\n",
    "    plt.xticks(np.arange(len(sorted_decreases)),sorted_occlusions,rotation=90, size=8)\n",
    "    plt.xlabel(\"Occludded positions\")\n",
    "    plt.ylabel(\"Decrease in performance\")\n",
    "    plt.show()\n",
    "\n",
    "occlusion_sensitivity_analysis(trained_model, test_loaders, 9, 0.746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
