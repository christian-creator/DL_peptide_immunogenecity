{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc,precision_recall_curve,roc_curve,confusion_matrix\n",
    "import os,sys\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "np.random.seed(10)\n",
    "random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aaindex(peptide,after_pca):\n",
    "\n",
    "    amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    matrix = np.transpose(after_pca)   # [12,21]\n",
    "    encoded = np.empty([len(peptide), 12])  # (seq_len,12)\n",
    "    for i in range(len(peptide)):\n",
    "        query = peptide[i]\n",
    "        if query == 'X': query = '-'\n",
    "        query = query.upper()\n",
    "        encoded[i, :] = matrix[:, amino.index(query)]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def rescue_unknown_hla(hla, dic_inventory):\n",
    "    type_ = hla[4]\n",
    "    first2 = hla[6:8]\n",
    "    last2 = hla[8:]\n",
    "    big_category = dic_inventory[type_]\n",
    "    if not big_category.get(first2) == None:\n",
    "        small_category = big_category.get(first2)\n",
    "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
    "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
    "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
    "    else:\n",
    "        small_category = list(big_category.keys())\n",
    "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
    "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
    "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])\n",
    "\n",
    "def hla_df_to_dic(hla):\n",
    "    dic = {}\n",
    "    for i in range(hla.shape[0]):\n",
    "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
    "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
    "        dic[col1] = col2\n",
    "    return dic\n",
    "\n",
    "def peptide_data_aaindex(peptide,after_pca):   # return numpy array [10,12,1]\n",
    "    length = len(peptide)\n",
    "    if length == 10:\n",
    "        encode = aaindex(peptide,after_pca)\n",
    "    elif length == 9:\n",
    "        peptide = peptide[:5] + '-' + peptide[5:]\n",
    "        encode = aaindex(peptide,after_pca)\n",
    "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
    "    return encode\n",
    "\n",
    "\n",
    "def hla_data_aaindex(hla_dic,hla_type,after_pca):    # return numpy array [34,12,1]\n",
    "    try:\n",
    "        seq = hla_dic[hla_type]\n",
    "    except KeyError:\n",
    "        hla_type = rescue_unknown_hla(hla_type,dic_inventory)\n",
    "        seq = hla_dic[hla_type]\n",
    "    encode = aaindex(seq,after_pca)\n",
    "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
    "    return encode\n",
    "\n",
    "def dict_inventory(inventory):\n",
    "    dicA, dicB, dicC = {}, {}, {}\n",
    "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
    "\n",
    "    for hla in inventory:\n",
    "        type_ = hla[4]  # A,B,C\n",
    "        first2 = hla[6:8]  # 01\n",
    "        last2 = hla[8:]  # 01\n",
    "        try:\n",
    "            dic[type_][first2].append(last2)\n",
    "        except KeyError:\n",
    "            dic[type_][first2] = []\n",
    "            dic[type_][first2].append(last2)\n",
    "\n",
    "    return dic\n",
    "\n",
    "def construct_aaindex(ori,hla_dic,after_pca):\n",
    "    series = []\n",
    "    for i in range(ori.shape[0]):\n",
    "        peptide = ori['peptide'].iloc[i]\n",
    "        hla_type = ori['HLA'].iloc[i]\n",
    "        immuno = np.array(ori['immunogenicity'].iloc[i]).reshape(1,-1)   # [1,1]\n",
    "\n",
    "        encode_pep = peptide_data_aaindex(peptide,after_pca)    # [10,12]\n",
    "\n",
    "        encode_hla = hla_data_aaindex(hla_dic,hla_type,after_pca)   # [46,12]\n",
    "        series.append((encode_pep, encode_hla, immuno))\n",
    "    return series\n",
    "\n",
    "def pull_peptide_aaindex(dataset):\n",
    "    result = np.empty([len(dataset),10,12,1])\n",
    "    for i in range(len(dataset)):\n",
    "        result[i,:,:,:] = dataset[i][0]\n",
    "    return result\n",
    "\n",
    "def pull_hla_aaindex(dataset):\n",
    "    result = np.empty([len(dataset),46,12,1])\n",
    "    for i in range(len(dataset)):\n",
    "        result[i,:,:,:] = dataset[i][1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def pull_label_aaindex(dataset):\n",
    "    col = [item[2] for item in dataset]\n",
    "    result = [0 if item == 'Negative' else 1 for item in col]\n",
    "    result = np.expand_dims(np.array(result),axis=1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_and_validataion_dataset(path_to_partitions,train_splits):\n",
    "    import random\n",
    "    training_partions = random.sample(range(10),train_splits)\n",
    "    # training_partions = [9, 0, 6, 3, 4, 8, 1, 7]\n",
    "    validation_partions = [i for i in range(10) if i not in training_partions]\n",
    "\n",
    "    # path_to_partitions = \"../data/partitions\"\n",
    "    partitions = []\n",
    "    for file in os.listdir(path_to_partitions):\n",
    "        path_to_file = os.path.join(path_to_partitions,file)\n",
    "        data = pd.read_csv(path_to_file,sep=\"\\t\",names=[\"peptide\",\"label\",\"HLA\"])\n",
    "        partitions.append(data)\n",
    "    training_df = pd.concat([partitions[i] for i in training_partions])\n",
    "    validation_df = pd.concat([partitions[i] for i in validation_partions])\n",
    "    return training_df, validation_df,training_partions,validation_partions\n",
    "\n",
    "def retrieve_information_from_df(data_split,entire_df):\n",
    "    potential = []\n",
    "    immunogenicity = []\n",
    "    tested = []\n",
    "    responded = []\n",
    "    for i,row in data_split.iterrows():\n",
    "        peptide, HLA = row[\"peptide\"], row['HLA']\n",
    "        original_entry = entire_df[(entire_df['peptide']==peptide) & (entire_df['HLA'] == HLA)]\n",
    "        assert len(original_entry) == 1\n",
    "        potential.append(float(original_entry['potential']))\n",
    "        immunogenicity.append(original_entry['immunogenicity'].values[0])\n",
    "        tested.append(int(original_entry['test']))\n",
    "        responded.append(int(original_entry['respond']))\n",
    "     \n",
    "    data_split['potential'] = potential\n",
    "    data_split['immunogenicity'] = immunogenicity\n",
    "    data_split['test'] = tested\n",
    "    data_split['respond'] = responded\n",
    "\n",
    "    return data_split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_pca = np.loadtxt('../DeepImmuno/reproduce/data/after_pca.txt')\n",
    "hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
    "hla_dic = hla_df_to_dic(hla)\n",
    "inventory = list(hla_dic.keys())\n",
    "dic_inventory = dict_inventory(inventory)\n",
    "\n",
    "validation_df = pd.read_csv(\"../data/validation_deepimmuno.csv\")\n",
    "\n",
    "val_dataset_encoded = construct_aaindex(validation_df, hla_dic, after_pca)\n",
    "peptide_val = pull_peptide_aaindex(val_dataset_encoded)\n",
    "HLA_val = pull_hla_aaindex(val_dataset_encoded)\n",
    "label_val = pull_label_aaindex(val_dataset_encoded)\n",
    "\n",
    "peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
    "HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
    "label_val = label_val.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1_peptide): Conv2d(1, 16, kernel_size=(2, 12), stride=(1, 1))\n",
      "  (BatchNorm_conv1_peptides): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (conv2_peptide): Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1))\n",
      "  (BatchNorm_conv2_peptides): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (maxpool1_peptide): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1_HLA): Conv2d(1, 16, kernel_size=(15, 12), stride=(1, 1))\n",
      "  (BatchNorm_conv1_HLA): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (maxpool1_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_HLA): Conv2d(16, 32, kernel_size=(9, 1), stride=(1, 1))\n",
      "  (maxpool2_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (L_in): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (drop_out): Dropout(p=0.2, inplace=False)\n",
      "  (L_out): Linear(in_features=128, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperameters of the model\n",
    "peptide_input_channels = peptide_val.shape[1]\n",
    "peptide_input_height = peptide_val.shape[2]\n",
    "peptide_input_width = peptide_val.shape[3]\n",
    "\n",
    "hla_input_channels = HLA_val.shape[1]\n",
    "hla_input_height = HLA_val.shape[2]\n",
    "hla_input_width = HLA_val.shape[3]\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convelution of peptide\n",
    "        self.conv1_peptide = Conv2d(in_channels=peptide_input_channels,\n",
    "                            out_channels=16,\n",
    "                            kernel_size=(2,12),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        \n",
    "        self.BatchNorm_conv1_peptides = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.conv2_peptide = Conv2d(in_channels=16,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=(2,1),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool1_peptide = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "\n",
    "        # Convelution of HLA\n",
    "        self.conv1_HLA = Conv2d(in_channels=peptide_input_channels,\n",
    "                            out_channels=16,\n",
    "                            kernel_size=(15,12),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv1_HLA = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool1_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "        \n",
    "        self.conv2_HLA = Conv2d(in_channels=16,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=(9,1),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool2_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "\n",
    "        # Denselayer\n",
    "        self.L_in = Linear(in_features=256,\n",
    "                            out_features=128)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0.2)\n",
    "        self.L_out = Linear(in_features=128,\n",
    "                            out_features=2,\n",
    "                            bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, peptide, HLA): # x.size() = [batch, channel, height, width]\n",
    "\n",
    "        # Encoding the peptide\n",
    "        peptide = self.conv1_peptide(peptide)\n",
    "        # peptide = self.BatchNorm_conv1_peptides(peptide)\n",
    "        peptide = relu(peptide)\n",
    "        peptide = self.conv2_peptide(peptide)\n",
    "        peptide = self.BatchNorm_conv2_peptides(peptide)\n",
    "        peptide = relu(peptide)\n",
    "        peptide = self.maxpool1_peptide(peptide)\n",
    "        peptide = torch.flatten(peptide,start_dim=1)\n",
    "\n",
    "        # Encoding the HLA\n",
    "        HLA = self.conv1_HLA(HLA)\n",
    "        # HLA = self.BatchNorm_conv1_HLA(HLA)\n",
    "        HLA = relu(HLA)\n",
    "        HLA = self.maxpool1_HLA(HLA)\n",
    "        HLA = self.conv2_HLA(HLA)\n",
    "        HLA = self.BatchNorm_conv2_peptides(HLA)\n",
    "        HLA = relu(HLA)\n",
    "        HLA = self.maxpool2_HLA(HLA)\n",
    "        HLA = torch.flatten(HLA,start_dim=1)\n",
    "\n",
    "        # Combining the output\n",
    "        combined_input = torch.cat((peptide, HLA), 1)\n",
    "\n",
    "        x = self.L_in(combined_input)\n",
    "        x = self.drop_out(x)\n",
    "        x = relu(x)\n",
    "        x = self.L_out(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6780595369349504\n",
      "0.6567695961995249\n",
      "0.6544378698224852\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "peptide_val_loader = list(DataLoader(peptide_val,batch_size=batch_size))\n",
    "HLA_val_loader = list(DataLoader(HLA_val,batch_size=batch_size))\n",
    "label_val_loader = list(DataLoader(label_val,batch_size=batch_size))\n",
    "val_accuracies = []\n",
    "losses = []\n",
    "all_val_targets_pr_epoch = []\n",
    "all_val_predictions_pr_epoch = []\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"../models/deepimmuno_classifier_epoch_30.pt\"))\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    all_train_targets = []\n",
    "    all_predicted_train_labels = []\n",
    "    for i in range(len((peptide_val_loader))):\n",
    "        train_peptides = peptide_val_loader[i]\n",
    "        train_HLA = HLA_val_loader[i]\n",
    "        train_labels = label_val_loader[i].long().reshape(-1)\n",
    "        outputs = net(train_peptides,train_HLA)\n",
    "        _,predicted_labels =  torch.max(outputs, 1)\n",
    "\n",
    "        all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
    "        all_train_targets += train_labels.numpy().tolist()\n",
    "\n",
    "\n",
    "#Calculating the accuracies\n",
    "print(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
    "print(recall_score(all_train_targets,all_predicted_train_labels))\n",
    "print(f1_score(all_train_targets,all_predicted_train_labels))\n",
    "#Saving the predicitons for further validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
