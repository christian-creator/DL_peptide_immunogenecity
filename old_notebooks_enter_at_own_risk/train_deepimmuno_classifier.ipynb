{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING the model for DeepImmuno but NOT evaluating HERE! Fy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc,precision_recall_curve,roc_curve,confusion_matrix\n",
    "import os,sys\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import seaborn as sns\n",
    "np.random.seed(10)\n",
    "random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aaindex(peptide,after_pca):\n",
    "\n",
    "    amino = 'ARNDCQEGHILKMFPSTWYV-'\n",
    "    matrix = np.transpose(after_pca)   # [12,21]\n",
    "    encoded = np.empty([len(peptide), 12])  # (seq_len,12)\n",
    "    for i in range(len(peptide)):\n",
    "        query = peptide[i]\n",
    "        if query == 'X': query = '-'\n",
    "        query = query.upper()\n",
    "        encoded[i, :] = matrix[:, amino.index(query)]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def rescue_unknown_hla(hla, dic_inventory):\n",
    "    type_ = hla[4]\n",
    "    first2 = hla[6:8]\n",
    "    last2 = hla[8:]\n",
    "    big_category = dic_inventory[type_]\n",
    "    if not big_category.get(first2) == None:\n",
    "        small_category = big_category.get(first2)\n",
    "        distance = [abs(int(last2) - int(i)) for i in small_category]\n",
    "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
    "        return 'HLA-' + str(type_) + '*' + str(first2) + str(optimal)\n",
    "    else:\n",
    "        small_category = list(big_category.keys())\n",
    "        distance = [abs(int(first2) - int(i)) for i in small_category]\n",
    "        optimal = min(zip(small_category, distance), key=lambda x: x[1])[0]\n",
    "        return 'HLA-' + str(type_) + '*' + str(optimal) + str(big_category[optimal][0])\n",
    "\n",
    "def hla_df_to_dic(hla):\n",
    "    dic = {}\n",
    "    for i in range(hla.shape[0]):\n",
    "        col1 = hla['HLA'].iloc[i]  # HLA allele\n",
    "        col2 = hla['pseudo'].iloc[i]  # pseudo sequence\n",
    "        dic[col1] = col2\n",
    "    return dic\n",
    "\n",
    "def peptide_data_aaindex(peptide,after_pca):   # return numpy array [10,12,1]\n",
    "    length = len(peptide)\n",
    "    if length == 10:\n",
    "        encode = aaindex(peptide,after_pca)\n",
    "    elif length == 9:\n",
    "        peptide = peptide[:5] + '-' + peptide[5:]\n",
    "        encode = aaindex(peptide,after_pca)\n",
    "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
    "    return encode\n",
    "\n",
    "\n",
    "def hla_data_aaindex(hla_dic,hla_type,after_pca):    # return numpy array [34,12,1]\n",
    "    try:\n",
    "        seq = hla_dic[hla_type]\n",
    "    except KeyError:\n",
    "        hla_type = rescue_unknown_hla(hla_type,dic_inventory)\n",
    "        seq = hla_dic[hla_type]\n",
    "    encode = aaindex(seq,after_pca)\n",
    "    encode = encode.reshape(encode.shape[0], encode.shape[1], -1)\n",
    "    return encode\n",
    "\n",
    "def dict_inventory(inventory):\n",
    "    dicA, dicB, dicC = {}, {}, {}\n",
    "    dic = {'A': dicA, 'B': dicB, 'C': dicC}\n",
    "\n",
    "    for hla in inventory:\n",
    "        type_ = hla[4]  # A,B,C\n",
    "        first2 = hla[6:8]  # 01\n",
    "        last2 = hla[8:]  # 01\n",
    "        try:\n",
    "            dic[type_][first2].append(last2)\n",
    "        except KeyError:\n",
    "            dic[type_][first2] = []\n",
    "            dic[type_][first2].append(last2)\n",
    "\n",
    "    return dic\n",
    "\n",
    "def construct_aaindex(ori,hla_dic,after_pca):\n",
    "    series = []\n",
    "    for i in range(ori.shape[0]):\n",
    "        peptide = ori['peptide'].iloc[i]\n",
    "        hla_type = ori['HLA'].iloc[i]\n",
    "        immuno = np.array(ori['immunogenicity'].iloc[i]).reshape(1,-1)   # [1,1]\n",
    "\n",
    "        encode_pep = peptide_data_aaindex(peptide,after_pca)    # [10,12]\n",
    "\n",
    "        encode_hla = hla_data_aaindex(hla_dic,hla_type,after_pca)   # [46,12]\n",
    "        series.append((encode_pep, encode_hla, immuno))\n",
    "    return series\n",
    "\n",
    "def pull_peptide_aaindex(dataset):\n",
    "    result = np.empty([len(dataset),10,12,1])\n",
    "    for i in range(len(dataset)):\n",
    "        result[i,:,:,:] = dataset[i][0]\n",
    "    return result\n",
    "\n",
    "def pull_hla_aaindex(dataset):\n",
    "    result = np.empty([len(dataset),46,12,1])\n",
    "    for i in range(len(dataset)):\n",
    "        result[i,:,:,:] = dataset[i][1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def pull_label_aaindex(dataset):\n",
    "    col = [item[2] for item in dataset]\n",
    "    result = [0 if item == 'Negative' else 1 for item in col]\n",
    "    result = np.expand_dims(np.array(result),axis=1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_and_validataion_dataset(path_to_partitions,train_splits):\n",
    "    import random\n",
    "    training_partions = random.sample(range(10),train_splits)\n",
    "    # training_partions = [9, 0, 6, 3, 4, 8, 1, 7]\n",
    "    validation_partions = [i for i in range(10) if i not in training_partions]\n",
    "\n",
    "    # path_to_partitions = \"../data/partitions\"\n",
    "    partitions = []\n",
    "    for file in os.listdir(path_to_partitions):\n",
    "        path_to_file = os.path.join(path_to_partitions,file)\n",
    "        data = pd.read_csv(path_to_file,sep=\"\\t\",names=[\"peptide\",\"label\",\"HLA\"])\n",
    "        partitions.append(data)\n",
    "    training_df = pd.concat([partitions[i] for i in training_partions])\n",
    "    validation_df = pd.concat([partitions[i] for i in validation_partions])\n",
    "    return training_df, validation_df,training_partions,validation_partions\n",
    "\n",
    "def retrieve_information_from_df(data_split,entire_df):\n",
    "    potential = []\n",
    "    immunogenicity = []\n",
    "    tested = []\n",
    "    responded = []\n",
    "    for i,row in data_split.iterrows():\n",
    "        peptide, HLA = row[\"peptide\"], row['HLA']\n",
    "        original_entry = entire_df[(entire_df['peptide']==peptide) & (entire_df['HLA'] == HLA)]\n",
    "        assert len(original_entry) == 1\n",
    "        potential.append(float(original_entry['potential']))\n",
    "        immunogenicity.append(original_entry['immunogenicity'].values[0])\n",
    "        tested.append(int(original_entry['test']))\n",
    "        responded.append(int(original_entry['respond']))\n",
    "     \n",
    "    data_split['potential'] = potential\n",
    "    data_split['immunogenicity'] = immunogenicity\n",
    "    data_split['test'] = tested\n",
    "    data_split['respond'] = responded\n",
    "\n",
    "    return data_split  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "after_pca = np.loadtxt('../DeepImmuno/reproduce/data/after_pca.txt')\n",
    "hla = pd.read_csv('../DeepImmuno/reproduce/data/hla2paratopeTable_aligned.txt', sep='\\t')\n",
    "hla_dic = hla_df_to_dic(hla)\n",
    "inventory = list(hla_dic.keys())\n",
    "dic_inventory = dict_inventory(inventory)\n",
    "\n",
    "entire_df = pd.read_csv('../data/deep_immuno_2.csv')\n",
    "# Allocating the partitions of the trainign and validation data\n",
    "training_df, validation_df,training_partions,validation_partions = load_training_and_validataion_dataset(path_to_partitions=\"../data/deepimmuno_parts\",train_splits=8)\n",
    "\n",
    "# Creating the training dataframe (With correct information such as tested and positive subjects aswell as label)\n",
    "training_df_entire = retrieve_information_from_df(training_df,entire_df)\n",
    "# Shuffling the dataframe\n",
    "training_df_entire = training_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Creating the validation dataframe (With correct information such as tested and positive subjects aswell as label)\n",
    "validation_df_entire = retrieve_information_from_df(validation_df,entire_df)\n",
    "# Shuffling the dataframe\n",
    "validation_df_entire = validation_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "validation_df_entire.to_csv(\"../data/validation_deepimmuno.csv\")\n",
    "\n",
    "training_dataset_encoded = construct_aaindex(training_df, hla_dic, after_pca)\n",
    "peptide_train = pull_peptide_aaindex(training_dataset_encoded)\n",
    "HLA_train = pull_hla_aaindex(training_dataset_encoded)\n",
    "label_train = pull_label_aaindex(training_dataset_encoded)\n",
    "\n",
    "\n",
    "# val_dataset_encoded = construct_aaindex(validation_df, hla_dic, after_pca)\n",
    "# peptide_val = pull_peptide_aaindex(val_dataset_encoded)\n",
    "# HLA_val = pull_hla_aaindex(val_dataset_encoded)\n",
    "# label_val = pull_label_aaindex(val_dataset_encoded)\n",
    "\n",
    "peptide_train = peptide_train.reshape(-1,1,10,12).astype('float32')\n",
    "HLA_train = HLA_train.reshape(-1,1,46,12).astype('float32')\n",
    "label_train = label_train.astype('float32')\n",
    "\n",
    "# peptide_val = peptide_val.reshape(-1,1,10,12).astype('float32')\n",
    "# HLA_val = HLA_val.reshape(-1,1,46,12).astype('float32')\n",
    "# label_val = label_val.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1_peptide): Conv2d(1, 16, kernel_size=(2, 12), stride=(1, 1))\n",
      "  (BatchNorm_conv1_peptides): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (conv2_peptide): Conv2d(16, 32, kernel_size=(2, 1), stride=(1, 1))\n",
      "  (BatchNorm_conv2_peptides): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (maxpool1_peptide): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1_HLA): Conv2d(1, 16, kernel_size=(15, 12), stride=(1, 1))\n",
      "  (BatchNorm_conv1_HLA): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "  (maxpool1_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_HLA): Conv2d(16, 32, kernel_size=(9, 1), stride=(1, 1))\n",
      "  (maxpool2_HLA): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (L_in): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (drop_out): Dropout(p=0.2, inplace=False)\n",
      "  (L_out): Linear(in_features=128, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperameters of the model\n",
    "peptide_input_channels = peptide_train.shape[1]\n",
    "peptide_input_height = peptide_train.shape[2]\n",
    "peptide_input_width = peptide_train.shape[3]\n",
    "\n",
    "hla_input_channels = HLA_train.shape[1]\n",
    "hla_input_height = HLA_train.shape[2]\n",
    "hla_input_width = HLA_train.shape[3]\n",
    "\n",
    "# define network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convelution of peptide\n",
    "        self.conv1_peptide = Conv2d(in_channels=peptide_input_channels,\n",
    "                            out_channels=16,\n",
    "                            kernel_size=(2,12),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        \n",
    "        self.BatchNorm_conv1_peptides = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.conv2_peptide = Conv2d(in_channels=16,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=(2,1),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool1_peptide = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "\n",
    "        # Convelution of HLA\n",
    "        self.conv1_HLA = Conv2d(in_channels=peptide_input_channels,\n",
    "                            out_channels=16,\n",
    "                            kernel_size=(15,12),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv1_HLA = BatchNorm2d(16,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool1_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "        \n",
    "        self.conv2_HLA = Conv2d(in_channels=16,\n",
    "                            out_channels=32,\n",
    "                            kernel_size=(9,1),\n",
    "                            stride=1,\n",
    "                            padding=0)\n",
    "        self.BatchNorm_conv2_peptides = BatchNorm2d(32,track_running_stats=False) # Output channels from the previous layer\n",
    "        self.maxpool2_HLA = nn.MaxPool2d(kernel_size=(2,1), stride=(2,1), padding=0)\n",
    "\n",
    "        # Denselayer\n",
    "        self.L_in = Linear(in_features=256,\n",
    "                            out_features=128)\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0.2)\n",
    "        self.L_out = Linear(in_features=128,\n",
    "                            out_features=2,\n",
    "                            bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, peptide, HLA): # x.size() = [batch, channel, height, width]\n",
    "\n",
    "        # Encoding the peptide\n",
    "        peptide = self.conv1_peptide(peptide)\n",
    "        # peptide = self.BatchNorm_conv1_peptides(peptide)\n",
    "        peptide = relu(peptide)\n",
    "        peptide = self.conv2_peptide(peptide)\n",
    "        peptide = self.BatchNorm_conv2_peptides(peptide)\n",
    "        peptide = relu(peptide)\n",
    "        peptide = self.maxpool1_peptide(peptide)\n",
    "        peptide = torch.flatten(peptide,start_dim=1)\n",
    "\n",
    "        # Encoding the HLA\n",
    "        HLA = self.conv1_HLA(HLA)\n",
    "        # HLA = self.BatchNorm_conv1_HLA(HLA)\n",
    "        HLA = relu(HLA)\n",
    "        HLA = self.maxpool1_HLA(HLA)\n",
    "        HLA = self.conv2_HLA(HLA)\n",
    "        HLA = self.BatchNorm_conv2_peptides(HLA)\n",
    "        HLA = relu(HLA)\n",
    "        HLA = self.maxpool2_HLA(HLA)\n",
    "        HLA = torch.flatten(HLA,start_dim=1)\n",
    "\n",
    "        # Combining the output\n",
    "        combined_input = torch.cat((peptide, HLA), 1)\n",
    "\n",
    "        x = self.L_in(combined_input)\n",
    "        x = self.drop_out(x)\n",
    "        x = relu(x)\n",
    "        x = self.L_out(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5618, 0.4382],\n",
       "        [0.5509, 0.4491],\n",
       "        [0.5195, 0.4805],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5734, 0.4266],\n",
       "        [0.4775, 0.5225],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.4531, 0.5469],\n",
       "        [0.5244, 0.4756],\n",
       "        [0.4889, 0.5111]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptide_random = np.random.normal(0,1, (10, 1, 10, 12)).astype('float32')\n",
    "peptide_random = Variable(torch.from_numpy(peptide_random))\n",
    "HLA_random = np.random.normal(0,1, (10, 1, 46, 12)).astype('float32')\n",
    "HLA_random = Variable(torch.from_numpy(HLA_random))\n",
    "output = net(peptide_random,HLA_random)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 : Train Loss 0.681264 \n",
      "Epoch 11 : Train Loss 0.506460 \n",
      "Epoch 21 : Train Loss 0.391521 \n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "peptide_train_loader = list(DataLoader(peptide_train,batch_size=batch_size))\n",
    "HLA_train_loader = list(DataLoader(HLA_train,batch_size=batch_size))\n",
    "label_train_loader = list(DataLoader(label_train,batch_size=batch_size))\n",
    "\n",
    "# peptide_val_loader = list(DataLoader(peptide_val,batch_size=batch_size))\n",
    "# HLA_val_loader = list(DataLoader(HLA_val,batch_size=batch_size))\n",
    "# label_val_loader = list(DataLoader(label_val,batch_size=batch_size))\n",
    "\n",
    "train_accuracies = []\n",
    "#val_accuracies = []\n",
    "losses = []\n",
    "#all_val_targets_pr_epoch = []\n",
    "#all_val_predictions_pr_epoch = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    net.train()\n",
    "    current_loss = 0\n",
    "    for train_batch_index in range(len((peptide_train_loader))):\n",
    "        train_peptides = peptide_train_loader[train_batch_index]\n",
    "        train_HLA = HLA_train_loader[train_batch_index]\n",
    "        train_labels = label_train_loader[train_batch_index].long().reshape(-1)\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(train_peptides,train_HLA)\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    # print(epoch, current_loss/batch_size)\n",
    "    losses.append(current_loss/len((peptide_train_loader)))\n",
    "\n",
    "    # net.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     all_train_targets = []\n",
    "    #     all_predicted_train_labels = []\n",
    "    #     for i in range(len((peptide_train_loader))):\n",
    "    #         train_peptides = peptide_train_loader[i]\n",
    "    #         train_HLA = HLA_train_loader[i]\n",
    "    #         train_labels = label_train_loader[i].long().reshape(-1)\n",
    "    #         outputs = net(train_peptides,train_HLA)\n",
    "    #         _,predicted_labels =  torch.max(outputs, 1)\n",
    "\n",
    "    #         all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
    "    #         all_train_targets += train_labels.numpy().tolist()\n",
    "\n",
    "    # Calculating the accuracies\n",
    "    #train_accuracies.append(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
    "    # Saving the predicitons for further validation\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f \" % (epoch+1, losses[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"../models/deepimmuno_classifier_epoch_30.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
