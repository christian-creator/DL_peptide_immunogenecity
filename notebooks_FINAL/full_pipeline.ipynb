{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to finish training and evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, random, sys, torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import auc,precision_recall_curve,roc_curve,confusion_matrix,accuracy_score,recall_score,f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, Conv2d, BatchNorm2d, MaxPool2d, Dropout2d, GRU, Dropout, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(10)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_and_validataion_dataset(path_to_partitions,train_splits):\n",
    "    import random\n",
    "    ## Training\n",
    "    # training_partions = random.sample(range(10),train_splits)\n",
    "    training_partions = [9, 0, 6, 3, 4, 8, 1, 7]\n",
    "\n",
    "    ## Validation\n",
    "    validation_partions = [i for i in range(10) if i not in training_partions]\n",
    "    partitions = []\n",
    "    for file in os.listdir(path_to_partitions):\n",
    "        path_to_file = os.path.join(path_to_partitions,file)\n",
    "        data = pd.read_csv(path_to_file,sep=\"\\t\",names=[\"peptide\",\"label\",\"HLA_allele\"])\n",
    "        partitions.append(data)\n",
    "    training_df = pd.concat([partitions[i] for i in training_partions])\n",
    "    validation_df = pd.concat([partitions[i] for i in validation_partions])\n",
    "    return training_df, validation_df,training_partions,validation_partions\n",
    "\n",
    "def retrieve_information_from_df(data_split,entire_df):\n",
    "    immunogenicity = []\n",
    "    response = []\n",
    "    tested_subjects = []\n",
    "    positive_subjects = []\n",
    "    binding_scores = []\n",
    "    for i,row in data_split.iterrows():\n",
    "        peptide, HLA = row[\"peptide\"], row['HLA_allele']\n",
    "        original_entry = entire_df[(entire_df['peptide']==peptide) & (entire_df['HLA_allele'] == HLA)]\n",
    "        assert len(original_entry) == 1\n",
    "        immunogenicity.append(float(original_entry['immunogenicity']))\n",
    "        response.append(original_entry['Response'].values[0])\n",
    "        tested_subjects.append(int(original_entry['tested_subjects']))\n",
    "        positive_subjects.append(int(original_entry['positive_subjects']))\n",
    "        binding_scores.append(float(original_entry['binding_score']))\n",
    "    data_split['immunogenicity'] = immunogenicity\n",
    "    data_split['response'] = response\n",
    "    data_split['test'] = tested_subjects\n",
    "    data_split['positive_subjects'] = positive_subjects\n",
    "    data_split['binding_score'] = binding_scores\n",
    "    return data_split  \n",
    "\n",
    "\n",
    "def encode_peptide_aaindex(aa_seq,aaindex_PCA,row):\n",
    "    aa_seq = list(aa_seq.upper())\n",
    "    encoded_aa_seq = []\n",
    "    PCs = aaindex_PCA.shape[1]\n",
    "    for aa in aa_seq:\n",
    "        if aa == \"X\" or aa == \"-\":\n",
    "            encoded_aa_seq.append(np.array([0 for x in range(PCs)]))\n",
    "        else:\n",
    "            try:\n",
    "                encoded_aa_seq.append(aaindex_PCA.loc[aa].to_numpy())\n",
    "            except KeyError:\n",
    "                print(row)\n",
    "                sys.exit(1)\n",
    "    return np.array(encoded_aa_seq)\n",
    "\n",
    "def encode_dataset(df,aaindex_PCA,HLA_dict,peptide_len,padding=\"right\"):\n",
    "    encoded_peptides = []\n",
    "    encoded_labels = []\n",
    "    encoded_hlas = []\n",
    "    encoded_binding_scores = []\n",
    "    for i,row in df.iterrows():\n",
    "        peptide = row[\"peptide\"]\n",
    "        HLA = HLA_dict[row[\"HLA_allele\"].replace(\":\",\"\")]\n",
    "        encoded_peptide = encode_peptide_aaindex(peptide,aaindex_PCA,row)\n",
    "        binding_score = row['binding_score']\n",
    "\n",
    "\n",
    "        # Adding padding\n",
    "        if len(encoded_peptide) < peptide_len:\n",
    "            n_added = peptide_len-len(encoded_peptide)\n",
    "            if padding == \"right\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((0, 1), (0, 0)), 'constant')\n",
    "            elif padding == \"left\":\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((1, 0), (0, 0)), 'constant')\n",
    "            elif padding == \"random\":\n",
    "                top_pad = random.choice([0,1])\n",
    "                bot_pad = 1-top_pad\n",
    "                encoded_peptide = np.pad(encoded_peptide, ((top_pad, bot_pad), (0, 0)), 'constant')\n",
    "\n",
    "\n",
    "        encoded_HLA = encode_peptide_aaindex(HLA,aaindex_PCA,row)\n",
    "        encoded_label = min(1,row[\"positive_subjects\"])\n",
    "        encoded_peptides.append(encoded_peptide)\n",
    "        encoded_hlas.append(encoded_HLA)\n",
    "        encoded_labels.append(encoded_label)\n",
    "        encoded_binding_scores.append(binding_score)\n",
    "    \n",
    "    encoded_peptides = np.array(encoded_peptides).astype('float32')\n",
    "    encoded_hlas = np.array(encoded_hlas).astype('float32')\n",
    "    encoded_labels = np.array(encoded_labels).astype('float32')\n",
    "    encoded_binding_scores = np.array(encoded_binding_scores).astype('float32')\n",
    "    return encoded_peptides, encoded_hlas, encoded_binding_scores, encoded_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the databases\n",
    "aaindex_PCA = pd.read_csv('../data/PCA_repr_aa.csv',index_col=0)\n",
    "hla_database = pd.read_csv('../data/formatted_hla2paratope_MHC_pseudo.dat', sep=' ',index_col=0)\n",
    "hla_dic = hla_database.to_dict(\"dict\")[\"pseudo\"]\n",
    "\n",
    "# Load dataset\n",
    "# entire_df = pd.read_csv('../data/filtered_data_IEDB_4_tested_len_9_10_full_HLA_IFNg_assay.csv')\n",
    "entire_df = pd.read_csv(\"../data/filtered_data_IEDB_4_tested_len_9_10_full_HLA_Multi_assay_w_binding.csv\")\n",
    "# entire_df = pd.read_csv('../data/deep_immuno_2.csv')\n",
    "\n",
    "\n",
    "# Allocating the partitions of the trainign and validation data\n",
    "training_df, validation_df, training_partions,validation_partions = load_training_and_validataion_dataset(path_to_partitions=\"../data/multi_assay_parts\",train_splits=8)\n",
    "\n",
    "\n",
    "# Creating the training dataframe (With correct information such as tested and positive subjects aswell as label)\n",
    "training_df_entire = retrieve_information_from_df(training_df,entire_df)\n",
    "# Shuffling the dataframe\n",
    "training_df_entire = training_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Creating the validation dataframe (With correct information such as tested and positive subjects aswell as label)\n",
    "validation_df_entire = retrieve_information_from_df(validation_df,entire_df)\n",
    "# Shuffling the dataframe\n",
    "validation_df_entire = validation_df_entire.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "print(\"##Encoding Training data\")\n",
    "train_peptides_encoded,train_HLA_encoded,train_binding_scores_encoded,train_label_encoded = encode_dataset(training_df_entire,aaindex_PCA,hla_dic,peptide_len=10,padding=\"right\")\n",
    "print(\"##Encoding Validation data\")\n",
    "val_peptides_encoded,val_HLA_encoded,val_binding_scores_encoded ,val_label_encoded = encode_dataset(validation_df_entire,aaindex_PCA,hla_dic,peptide_len=10,padding=\"right\")\n",
    "\n",
    "peptide_train = train_peptides_encoded.reshape(-1,1,10,12)\n",
    "HLA_train = train_HLA_encoded.reshape(-1,1,34,12)\n",
    "binding_train = train_binding_scores_encoded\n",
    "label_train = train_label_encoded\n",
    "\n",
    "peptide_val = val_peptides_encoded.reshape(-1,1,10,12)\n",
    "HLA_val = val_HLA_encoded.reshape(-1,1,34,12) # 46 aligned representataion and 34 if not aligned\n",
    "binding_val = val_binding_scores_encoded\n",
    "label_val = val_label_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_dim(dim_size,kernel_size,padding,stride):\n",
    "    return int((dim_size - kernel_size + 2 * padding) / stride + 1)\n",
    "compute_conv_dim(34,2,0,1)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "          nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "          if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "          nn.init.constant_(m.weight.data, 1)\n",
    "          nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "          nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
    "          if m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "peptide_train_loader = list(DataLoader(peptide_train,batch_size=batch_size))\n",
    "HLA_train_loader = list(DataLoader(HLA_train,batch_size=batch_size))\n",
    "label_train_loader = list(DataLoader(label_train,batch_size=batch_size))\n",
    "binding_score_train_loader = list(DataLoader(binding_train,batch_size=batch_size))\n",
    "\n",
    "\n",
    "peptide_val_loader = list(DataLoader(peptide_val,batch_size=batch_size))\n",
    "HLA_val_loader = list(DataLoader(HLA_val,batch_size=batch_size))\n",
    "label_val_loader = list(DataLoader(label_val,batch_size=batch_size))\n",
    "binding_score_val_loader = list(DataLoader(binding_val,batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_accuracies,val_accuracies):\n",
    "    epoch = np.arange(len(train_accuracies))\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_accuracies, 'r', epoch, val_accuracies, 'b')\n",
    "    plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "    plt.xlabel('epochs'), plt.ylabel('Acc')\n",
    "\n",
    "\n",
    "\n",
    "def validation(model,device,valid_loaders,train_loaders):\n",
    "    peptide_val_loader,HLA_val_loader,label_val_loader,binding_score_val_loader = valid_loaders\n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_train_targets = []\n",
    "        all_predicted_train_labels = []\n",
    "        for i in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[i]\n",
    "            train_HLA = HLA_train_loader[i]\n",
    "            train_labels = label_train_loader[i].long().reshape(-1)\n",
    "            train_binding_scores = binding_score_train_loader[i].reshape(len(train_peptides),1)\n",
    "            outputs = model(train_peptides,train_HLA,train_binding_scores)\n",
    "            _,predicted_labels =  torch.max(outputs, 1)\n",
    "\n",
    "            all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
    "            all_train_targets += train_labels.numpy().tolist()\n",
    "        \n",
    "        all_val_targets = []\n",
    "        all_predicted_val_labels = []\n",
    "        all_probabilities_val = []\n",
    "        for j in range(len((peptide_val_loader))):\n",
    "            val_peptides = peptide_val_loader[j]\n",
    "            val_HLA = HLA_val_loader[j]\n",
    "            val_labels = label_val_loader[j].long().reshape(-1)\n",
    "            val_binding_scores = binding_score_val_loader[j].reshape(len(val_peptides),1)\n",
    "            outputs = model(val_peptides,val_HLA,val_binding_scores)\n",
    "            probability,predicted_labels =  torch.max(outputs, 1)\n",
    "            all_predicted_val_labels += predicted_labels.numpy().tolist()\n",
    "            all_val_targets += val_labels.numpy().tolist()\n",
    "            all_probabilities_val += probability.numpy().tolist()\n",
    "\n",
    "    return all_train_targets,all_predicted_train_labels,all_val_targets,all_predicted_val_labels,all_probabilities_val\n",
    "\n",
    "\n",
    "def train(model, device, epochs, train_loaders, valid_loaders):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "    # Early stopping\n",
    "    the_last_loss = 100\n",
    "    patience = 2\n",
    "    trigger_times = 0\n",
    "    \n",
    "    all_val_targets_pr_epoch = []\n",
    "    all_val_predictions_pr_epoch = []\n",
    "    all_val_probabilities_pr_epoch = []\n",
    "    losses = []\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    peptide_train_loader,HLA_train_loader,label_train_loader,binding_score_train_loader = train_loaders\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        current_loss = 0\n",
    "        for train_batch_index in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[train_batch_index]\n",
    "            train_HLA = HLA_train_loader[train_batch_index]\n",
    "            train_labels = label_train_loader[train_batch_index].long().reshape(-1)\n",
    "            train_binding_scores = binding_score_train_loader[train_batch_index].reshape(len(train_peptides),1)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_peptides,train_HLA,train_binding_scores)\n",
    "            loss = criterion(outputs, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "        losses.append(current_loss/len((peptide_train_loader)))\n",
    "\n",
    "        all_train_targets,all_predicted_train_labels,all_val_targets,all_predicted_val_labels,all_probabilities_val = validation(model,device,valid_loaders,train_loaders)\n",
    "    \n",
    "        # Calculating the accuracies\n",
    "        train_accuracies.append(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
    "        val_accuracies.append(accuracy_score(all_val_targets,all_predicted_val_labels))\n",
    "        # Saving the predicitons for further validation\n",
    "        all_val_targets_pr_epoch.append(all_val_targets)\n",
    "        all_val_predictions_pr_epoch.append(all_predicted_val_labels)\n",
    "        all_val_probabilities_pr_epoch.append(all_probabilities_val)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (epoch+1, losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
    "        \n",
    "\n",
    "        # Early stopping\n",
    "        the_current_loss = val_accuracies[-1]\n",
    "        the_last_loss = 0 if len(val_accuracies) < 2 else val_accuracies[-2]\n",
    "\n",
    "        # print('The current valdiation loss:', the_current_loss)\n",
    "\n",
    "        if the_current_loss < the_last_loss:\n",
    "            trigger_times += 1\n",
    "            # print('trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                # print('Early stopping at epoch',epoch)\n",
    "                return model,train_accuracies,val_accuracies,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_val_probabilities_pr_epoch\n",
    "\n",
    "        else:\n",
    "            # print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "    return model,train_accuracies,val_accuracies,all_val_targets_pr_epoch,all_val_predictions_pr_epoch,all_val_probabilities_pr_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001,weight_decay=1e-4)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device state:', device)\n",
    "\n",
    "train_loaders = (peptide_train_loader, HLA_train_loader, label_train_loader, binding_score_train_loader)\n",
    "val_loaders = (peptide_val_loader, HLA_val_loader, label_val_loader, binding_score_val_loader)\n",
    "\n",
    "net = Net()\n",
    "net.apply(initialize_weights)\n",
    "\n",
    "epochs = 100\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "losses = []\n",
    "all_val_targets_pr_epoch = []\n",
    "all_val_predictions_pr_epoch = []\n",
    "all_val_probabilities_pr_epoch = []\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    current_loss = 0\n",
    "    for train_batch_index in range(len((peptide_train_loader))):\n",
    "        train_peptides = peptide_train_loader[train_batch_index]\n",
    "        train_HLA = HLA_train_loader[train_batch_index]\n",
    "        train_labels = label_train_loader[train_batch_index].long().reshape(-1)\n",
    "        train_binding_scores = binding_score_train_loader[train_batch_index].reshape(len(train_peptides),1)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(train_peptides,train_HLA,train_binding_scores)\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()\n",
    "\n",
    "    # print(epoch, current_loss/batch_size)\n",
    "    losses.append(current_loss/len((peptide_train_loader)))\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        all_train_targets = []\n",
    "        all_predicted_train_labels = []\n",
    "        for i in range(len((peptide_train_loader))):\n",
    "            train_peptides = peptide_train_loader[i]\n",
    "            train_HLA = HLA_train_loader[i]\n",
    "            train_labels = label_train_loader[i].long().reshape(-1)\n",
    "            train_binding_scores = binding_score_train_loader[i].reshape(len(train_peptides),1)\n",
    "            outputs = net(train_peptides,train_HLA,train_binding_scores)\n",
    "            _,predicted_labels =  torch.max(outputs, 1)\n",
    "\n",
    "            all_predicted_train_labels += predicted_labels.numpy().tolist()\n",
    "            all_train_targets += train_labels.numpy().tolist()\n",
    "        \n",
    "        all_val_targets = []\n",
    "        all_predicted_val_labels = []\n",
    "        all_probabilities_val = []\n",
    "        for j in range(len((peptide_val_loader))):\n",
    "            val_peptides = peptide_val_loader[j]\n",
    "            val_HLA = HLA_val_loader[j]\n",
    "            val_labels = label_val_loader[j].long().reshape(-1)\n",
    "            val_binding_scores = binding_score_val_loader[j].reshape(len(val_peptides),1)\n",
    "            outputs = net(val_peptides,val_HLA,val_binding_scores)\n",
    "            probability,predicted_labels =  torch.max(outputs, 1)\n",
    "            all_predicted_val_labels += predicted_labels.numpy().tolist()\n",
    "            all_val_targets += val_labels.numpy().tolist()\n",
    "            all_probabilities_val += probability.numpy().tolist()\n",
    "\n",
    "    # Calculating the accuracies\n",
    "    train_accuracies.append(accuracy_score(all_train_targets,all_predicted_train_labels))\n",
    "    val_accuracies.append(accuracy_score(all_val_targets,all_predicted_val_labels))\n",
    "    # Saving the predicitons for further validation\n",
    "    all_val_targets_pr_epoch.append(all_val_targets)\n",
    "    all_val_predictions_pr_epoch.append(all_predicted_val_labels)\n",
    "    all_val_probabilities_pr_epoch.append(all_probabilities_val)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (epoch+1, losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
